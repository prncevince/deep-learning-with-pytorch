[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning with PyTorch",
    "section": "",
    "text": "Deep Learning with PyTorch\nBy Tomas Beuzen üöÄ\nWelcome to Deep Learning with PyTorch! With this website I aim to provide an introduction to optimization, neural networks and deep learning using PyTorch. We will progressively build up our deep learning knowledge, covering topics such as optimization algorithms like gradient descent, fully connected neural networks for regression and classification tasks, convolutional neural networks for image classification, transfer learning, and even generative adversarial networks (GANs) for synthetic image generation!"
  },
  {
    "objectID": "index.html#chapter-outline",
    "href": "index.html#chapter-outline",
    "title": "Deep Learning with PyTorch",
    "section": "Chapter Outline",
    "text": "Chapter Outline\n\nGradient Descent\nStochastic Gradient Descent\nIntroduction to Pytorch & Neural Networks\nTraining Neural Networks\nIntroduction to Convolutional Neural Networks\nAdvanced Convolutional Neural Networks\nAdvanced Deep Learning"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Deep Learning with PyTorch",
    "section": "Getting Started",
    "text": "Getting Started\nThe material on this site is written in Jupyter notebooks and rendered using Jupyter Book to make it easily accessible. However, if you wish to run these notebooks on your local machine, you can do the following:\n\nClone the GitHub repository:\ngit clone https://github.com/TomasBeuzen/deep-learning-with-pytorch.git\nInstall the conda environment by typing the following in your terminal:\nconda env create -f dlwpt.yaml\nOpen the course in JupyterLab by typing the following in your terminal:\ncd deep-learning-with-pytorch\njupyterlab\n\n\nIf you‚Äôre not comfortable with git, GitHub or conda, feel free to just read through the material on this website - you‚Äôre not missing out on anything!"
  },
  {
    "objectID": "chapter1_gradient-descent.html#chapter-learning-objectives",
    "href": "chapter1_gradient-descent.html#chapter-learning-objectives",
    "title": "1¬† Chapter 1: Optimization & Gradient Descent",
    "section": "1.1 Chapter Learning Objectives",
    "text": "1.1 Chapter Learning Objectives\n\n\nExplain the difference between a model, loss function, and optimization algorithm in the context of machine learning.\nExplain how the gradient descent algorithm works.\nApply gradient descent to linear and logistic regression.\nUse scipy.optimize.minimize() to minimize a function."
  },
  {
    "objectID": "chapter1_gradient-descent.html#imports",
    "href": "chapter1_gradient-descent.html#imports",
    "title": "1¬† Chapter 1: Optimization & Gradient Descent",
    "section": "1.2 Imports",
    "text": "1.2 Imports\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nTrouble importing when importing PIL.PILLOW_VERSION inside of torchvision? Add below to $PREFIX/lib/python3.10/site-packages/PIL/__init__.py\nPILLOW_VERSION = _version.__version__\n\nfrom utils.plotting import *"
  },
  {
    "objectID": "chapter1_gradient-descent.html#optimization-and-machine-learning",
    "href": "chapter1_gradient-descent.html#optimization-and-machine-learning",
    "title": "1¬† Chapter 1: Optimization & Gradient Descent",
    "section": "1.3 1. Optimization and Machine Learning",
    "text": "1.3 1. Optimization and Machine Learning\n\nIn data science and computer science, we optimize a lot of stuff. For example, in linear regression we optimize for the intercept and coefficients of our model, in clustering algorithms like k-means we optimize our clusters, in neural networks we optimize the weights in our network (more on that in a later chapter!), etc.\nIn one sentence, ‚Äúoptimization‚Äù simply refers to minimizing/maximizing a function. For example, what value of \\(x\\) minimizes the function \\(f(x) = (x-2)^2 + 5\\)? What is the minimum value? Answers: \\(x=2\\), and \\(f(x)=5\\).\nIf you‚Äôre reading this, you‚Äôre likely already familiar with machine learning. You can start to think of machine learning as a three-step process: 1. Choose your model: controls the space of possible functions that map \\(X\\) to \\(y\\) (e.g., a linear model can only learn linear functions) 2. Choose your loss function: tells us how to compare these various functions (e.g., is \\(y=5 + 2x_1+3x_2\\) a better model than \\(y=1 + 10x_1-x_2\\)?) 3. Choose your optimization algorithm: finds the minimum of the loss function (e.g., what is the optimum value of \\(w_0\\) and \\(w_1\\) in \\(y=w_0 + w_1x\\)?)\nIn this chapter we‚Äôll be taking a look at optimization in detail and a particular optimization algorithm known as gradient descent."
  },
  {
    "objectID": "chapter1_gradient-descent.html#loss-functions",
    "href": "chapter1_gradient-descent.html#loss-functions",
    "title": "1¬† Chapter 1: Optimization & Gradient Descent",
    "section": "1.4 2. Loss Functions",
    "text": "1.4 2. Loss Functions\n\nLoss functions (also often called ‚Äúobjective functions‚Äù or ‚Äúcost functions‚Äù, although some debate that these are slightly different things) are what we use to map the performance of a model to a real number and it‚Äôs the thing we want to optimize! For example, here‚Äôs the mean squared error (MSE), which is a common loss function:\n\\[f(y,\\hat{y})=\\frac{1}{n}\\sum^{n}_{i=1}(\\hat{y_i}-y_i)^2\\]\nWhere \\(y_i\\) is the actual response and \\(\\hat{y_i}\\) is the predicted response.\nConsider a simple linear regression model \\(\\hat{y_i} = w_0 + w_1x_i\\), then our loss function is:\n\\[f(y,x,w)=\\frac{1}{n}\\sum^{n}_{i=1}((w_0 + w_1x_i)-y_i))^2\\]\nThe optimization problem here is to find the values of \\(w_0\\) and \\(w_1\\) that minimizes the MSE."
  },
  {
    "objectID": "chapter1_gradient-descent.html#optimizing-linear-regression",
    "href": "chapter1_gradient-descent.html#optimizing-linear-regression",
    "title": "1¬† Chapter 1: Optimization & Gradient Descent",
    "section": "1.5 3. Optimizing Linear Regression",
    "text": "1.5 3. Optimizing Linear Regression\n\nI‚Äôm going to build up the intuition for optimization in a practical and visual way with the help of our old friend linear regression. If you‚Äôd prefer a more mathematical approach I encourage you to check out my colleague Mike Gelbart‚Äôs lecture on Youtube Thinking about Optimization or Chapter 7 of Mathematics for Machine Learning, by Deisenroth et al..\nWe‚Äôll use a dataset of Pokemon ‚Äúattack‚Äù and ‚Äúdefense‚Äù stats to do this, I‚Äôm going to start with just 10 observations:\n\ndf = (pd.read_csv(\"data/pokemon.csv\", usecols=['name', 'defense', 'attack'], index_col=0)\n        .head(10)\n        .sort_values(by='defense')\n        .reset_index()\n     )\nx = df['defense']\ny = df['attack']\ndf\n\n\n\n\n\n\n\n\nname\nattack\ndefense\n\n\n\n\n0\nCaterpie\n30\n35\n\n\n1\nCharmander\n52\n43\n\n\n2\nBulbasaur\n49\n49\n\n\n3\nCharmeleon\n64\n58\n\n\n4\nIvysaur\n62\n63\n\n\n5\nSquirtle\n48\n65\n\n\n6\nCharizard\n104\n78\n\n\n7\nWartortle\n63\n80\n\n\n8\nBlastoise\n103\n120\n\n\n9\nVenusaur\n100\n123\n\n\n\n\n\n\n\n\nThroughout this chapter, I‚Äôm leveraging plotting scripts I imported from utils.plotting. I abstracted the code out of the notebook to avoid cluttering the material here and because how I made these plots is not important - but feel free to check out the source code if you wish!\n\n\nplot_pokemon(x, y)\n\n\n                                                \n\n\nRecall simple linear regression: \\(\\hat{y_i} = w_0 + w_1x_i\\) (where \\(w_0\\) is the intercept and \\(w_1\\) is the slope coefficient). If we assume (\\(w_0\\), \\(w_1\\)) = (10, 0.5) then we would have:\n\ny_hat = 10 + 0.5 * x\n\nLet‚Äôs plot that result:\n\nplot_pokemon(x, y, y_hat)\n\n\n                                                \n\n\nThe fit is not very good‚Ä¶ We need to optimize it! A loss function can help quantify the fit of our model and we want to find the parameters of our model that minimize the loss function. We‚Äôll use mean-squared-error (MSE) as our loss function:\n\\[f(w)=\\frac{1}{n}\\sum^{n}_{i=1}((w_0 + w_1x_i)-y_i))^2\\]\nWhere \\(n\\) is the number of data points we have (10 in our case). We‚Äôll use the sklearn function mean_squared_error() which I imported at the top of the notebook to calculate MSE for us:\n\nmean_squared_error(y, y_hat)\n\n680.75\n\n\nSo this is the MSE across all training examples. For now, let‚Äôs assume the intercept is 0 (\\(w_0 = 0\\)) and just focus on optimizing the slope (\\(w_1\\)). One thing we could do is try many different values for the slope and find the one that minimizes the MSE:\n\nslopes = np.arange(0.4, 1.65, 0.05)\nmse = pd.DataFrame({\"slope\": slopes,\n                    \"MSE\": [mean_squared_error(y, m * x) for m in slopes]})\nmse\n\n\n\n\n\n\n\n\nslope\nMSE\n\n\n\n\n0\n0.40\n1770.0760\n\n\n1\n0.45\n1478.6515\n\n\n2\n0.50\n1216.7500\n\n\n3\n0.55\n984.3715\n\n\n4\n0.60\n781.5160\n\n\n5\n0.65\n608.1835\n\n\n6\n0.70\n464.3740\n\n\n7\n0.75\n350.0875\n\n\n8\n0.80\n265.3240\n\n\n9\n0.85\n210.0835\n\n\n10\n0.90\n184.3660\n\n\n11\n0.95\n188.1715\n\n\n12\n1.00\n221.5000\n\n\n13\n1.05\n284.3515\n\n\n14\n1.10\n376.7260\n\n\n15\n1.15\n498.6235\n\n\n16\n1.20\n650.0440\n\n\n17\n1.25\n830.9875\n\n\n18\n1.30\n1041.4540\n\n\n19\n1.35\n1281.4435\n\n\n20\n1.40\n1550.9560\n\n\n21\n1.45\n1849.9915\n\n\n22\n1.50\n2178.5500\n\n\n23\n1.55\n2536.6315\n\n\n24\n1.60\n2924.2360\n\n\n\n\n\n\n\n\nplot_grid_search(x, y, slopes, mean_squared_error)\n\n\n                                                \n\n\nIt looks like a slope of 0.9 gives us the lowest MSE (~184.4). But you can imagine that this ‚Äúgrid search‚Äù approach quickly becomes computationally intractable as the size of our data set and number of model parameters increases! So we need a better way to optimize our parameters‚Ä¶ we need an optimization algorithm! The one we‚Äôll focus on in this chapter is Gradient Descent."
  },
  {
    "objectID": "chapter1_gradient-descent.html#gradient-descent-with-one-parameter",
    "href": "chapter1_gradient-descent.html#gradient-descent-with-one-parameter",
    "title": "1¬† Chapter 1: Optimization & Gradient Descent",
    "section": "1.6 4. Gradient Descent With One Parameter",
    "text": "1.6 4. Gradient Descent With One Parameter\n\nGradient descent is an optimization algorithm that can help us optimize our loss function more efficiently than the ‚Äúmanual‚Äù approach we tried above. As the name suggest, we are going to leverage the gradient of our loss function to help us optimize our model parameters. The gradient is just a vector of (partial) derivatives of the loss function w.r.t the model parameters. Sounds complicated but it‚Äôs not at all (as I‚Äôll hopefully show you).\nIn plain English, the gradient will tell us two things: 1. Which direction to move our parameter in to decrease loss (i.e., should we increase or decrease its value?) 2. How far to move it (i.e., should we adjust it by 0.1 or 2 or 50 etc.?)\n\nIf you need a refresher on gradients, check out Appendix A: Gradients Review.\n\nLet‚Äôs forget about the intercept now and just work with this simple linear model: \\(\\hat{y_i}=wx_i\\). For this model, the loss function has the form:\n\\[f(w)=\\frac{1}{n}\\sum^{n}_{i=1}((wx_i)-y_i))^2\\]\nThe gradient of this function with respect to the parameter \\(w\\) is:\n\\[\\frac{d}{dw}f(w)=\\frac{1}{n}\\sum^{n}_{i=1}2x_i(wx_i - y_i)\\]\nLet‚Äôs code that up and calculate the gradient of our loss function at a slope of \\(w = 0.5\\):\n\ndef gradient(x, y, w):\n    return 2 * (x * (w * x - y)).mean()\n\ngradient(x, y, w=0.5)\n\n-4942.8\n\n\nSo this is the gradient across all training examples and tells us how to adjust \\(w\\) to reduce the MSE loss over all training examples! Recall from calculus that the gradient actually points in the direction of steepest ascent (read more in Appendix A: Gradients Review). We want to move in the direction of steepest descent (the negative of the gradient) to reduce our loss. For example, the above gradient is negative, but we obviously need to increase the value of our slope (\\(w\\)) to reduce our loss as you can see here:\n\nplot_gradient_m(x, y, 0.5, mse[\"slope\"], mse[\"MSE\"], gradient)\n\n\n                                                \n\n\nThe amount we adjust our slope each iteration is controlled by a ‚Äúlearning rate‚Äù, denoted \\(\\alpha\\) (note the minus in the equation below which accounts for flipping the sign of the gradient as discussed above):\n\\[w_{new} = w - \\alpha \\times gradient\\]\n\\[w_{new} = w - \\alpha \\frac{d}{dw}f(w)\\]\n\\(\\alpha\\) is a hyperparameter that can be optimized, typical values range from 0.001 to 0.9. With the math out of the way, we‚Äôre now ready to use gradient descent to optimize our slope. Gradient descent is an iterative algorithm, meaning that we keep repeating it until we meet some stopping criteria. Typically we stop gradient descent when: 1. the step size is smaller than some pre-defined threshold; or, 2. a certain number of steps is completed.\nSo the pseudo code for gradient descent boils down to this:\n1. begin with with some arbitrary w\nwhile stopping criteria not met:\n    2. calculate mean gradient across all examples\n    3. update w based on gradient and learning rate\n    4. repeat\nLet‚Äôs go ahead and implement that now:\n\n# print(x, y)\nx @ y\nprint(sum(np.ones(3)*2))\n\n6.0\n\n\n\ndef gradient_descent(x, y, w, alpha, œµ=2e-4, max_iterations=5000, print_progress=10):\n    \"\"\"Gradient descent for optimizing slope in simple linear regression with no intercept.\"\"\"\n    \n    print(f\"Iteration 0. w = {w:.2f}.\")\n    iterations = 1  # init iterations\n    dw = 2 * œµ      # init. dw\n    \n    while abs(dw) &gt; œµ and iterations &lt;= max_iterations:\n        g = gradient(x, y, w)  # calculate current gradient\n        dw = alpha * g    # change in w\n        w -= dw                # adjust w based on gradient * learning rate\n        if iterations % print_progress == 0:  # periodically print progress\n            print(f\"Iteration {iterations}. w = {w:.2f}.\")\n        iterations += 1        # increase iteration\n        \n    print(\"Terminated!\")\n    print(f\"Iteration {iterations - 1}. w = {w:.2f}.\")\n\ngradient_descent(x, y, w=0.5, alpha=0.00001)\n\nIteration 0. w = 0.50.\nIteration 10. w = 0.80.\nIteration 20. w = 0.88.\nIteration 30. w = 0.91.\nIteration 40. w = 0.92.\nTerminated!\nIteration 45. w = 0.92.\n\n\nLet‚Äôs take a look at the journey our slope parameter of our simple linear model went on during its optimization:\n\nplot_gradient_descent(x, y, w=0.5, alpha=0.00001)\n\n\n                                                \n\n\nNow let‚Äôs see what happens if we increase the learning rate:\n\ngradient_descent(x, y, w=0.5, alpha=0.00005, print_progress=2)\n\nIteration 0. w = 0.50.\nIteration 2. w = 0.85.\nIteration 4. w = 0.91.\nIteration 6. w = 0.92.\nIteration 8. w = 0.92.\nTerminated!\nIteration 9. w = 0.92.\n\n\n\nplot_gradient_descent(x, y, w=0.5, alpha=0.00005)\n\n\n                                                \n\n\nLet‚Äôs increase a little more:\n\ngradient_descent(x, y, w=0.5, alpha=0.00015)\n\nIteration 0. w = 0.50.\nIteration 10. w = 0.89.\nIteration 20. w = 0.92.\nIteration 30. w = 0.92.\nTerminated!\nIteration 33. w = 0.92.\n\n\n\nplot_gradient_descent(x, y, w=0.5, alpha=0.00015)\n\n\n                                                \n\n\nIf our learning rate is too high, we‚Äôll overshoot and never converge (i.e., we‚Äôll diverge)!\n\ngradient_descent(x, y, w=0.5, alpha=0.00018, max_iterations=4, print_progress=1)\n\nIteration 0. w = 0.50.\nIteration 1. w = 1.39.\nIteration 2. w = 0.39.\nIteration 3. w = 1.52.\nIteration 4. w = 0.25.\nTerminated!\nIteration 4. w = 0.25.\n\n\n\nplot_gradient_descent(x, y, w=0.5, alpha=0.00018, max_iterations=4)\n\n\n                                                \n\n\nCool, we just implemented gradient descent from scratch! In the next seciton, we‚Äôll try optimizing for two parameters, the intercept and slope of a simple linear regression model, simultaneously‚Ä¶"
  },
  {
    "objectID": "chapter1_gradient-descent.html#gradient-descent-with-two-parameters",
    "href": "chapter1_gradient-descent.html#gradient-descent-with-two-parameters",
    "title": "1¬† Chapter 1: Optimization & Gradient Descent",
    "section": "1.7 5. Gradient Descent With Two Parameters",
    "text": "1.7 5. Gradient Descent With Two Parameters\n\nMost of the models you‚Äôll be working with will have more than just one parameter to update - neural networks typically have hundreds, thousands, and even millions of parameters! So, let‚Äôs extend the above workflow to two parameters, the intercept (\\(w_0\\)) and the slope (\\(w_1\\)). Just to help you get a visual of what‚Äôs going on, let‚Äôs take our ‚Äúmanual grid search approach‚Äù from earlier and make a plot of it but this time with two parameters:\n\nslopes = np.arange(0, 2.05, 0.05)\nintercepts = np.arange(-30, 31, 2)\nplot_grid_search_2d(x, y, slopes, intercepts)\n\n\n                                                \n\n\nAbove is the surface of MSE for different values of intercept (\\(w_0\\)) and slope (\\(w_1\\)). The approach for implementing gradient descent is exactly as before, but we‚Äôre operating on two parameters now and the gradient of the intercept is a little different to the slope:\n\\[f(w)=\\frac{1}{n}\\sum^{n}_{i=1}((w_0 + w_1x)-y_i))^2\\]\n\\[\\frac{\\partial{}}{\\partial{}w_0}f(w)=\\frac{1}{n}\\sum^{n}_{i=1}2((w_0 + w_1x) - y_i)\\]\n\\[\\frac{\\partial{}}{\\partial{}w_1}f(w)=\\frac{1}{n}\\sum^{n}_{i=1}2x_i((w_0 + w_1x) - y_i)\\]\nLet‚Äôs define a function that returns these two gradients (partial derivatives) of the slope and intercept.\n\n‚ÄúGradient‚Äù is technically the vector of all partial derivatives of the loss function with respect to all the model weights. But I use the term fairly loosely to mean either the vector of all partial derivatives, or just a particular partial derivative, which you can infer from the given context.\n\n\ndef gradient(x, y, w):\n    grad_w0 = (1/len(x)) * 2 * sum(w[0] + w[1] * x - y)\n    grad_w1 = (1/len(x)) * 2 * sum(x * (w[0] + w[1] * x - y))\n    return np.array([grad_w0, grad_w1])\n\ngradient(x, y, w=[10, 0.5])\n\narray([  -43.6, -3514.8])\n\n\nYou can already see that the gradient of our slope is orders of magnitude larger than our intercept‚Ä¶ we‚Äôll look more at this issue shortly. For now let‚Äôs re-write our gradient descent function from earlier. It‚Äôs almost exactly the same as before, but now we are updating the slope and the intercept each iteration:\n\ndef gradient_descent(x, y, w, alpha, œµ=2e-4, max_iterations=5000, print_progress=10):\n    \"\"\"Gradient descent for optimizing simple linear regression.\"\"\"\n    \n    print(f\"Iteration 0. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n    iterations = 1  # init iterations\n    dw = np.array(2 * œµ)      # init. dw\n    \n    while abs(dw.sum()) &gt; œµ and iterations &lt;= max_iterations:\n        g = gradient(x, y, w)  # calculate current gradient\n        dw = alpha * g         # change in w\n        w -= dw                # adjust w based on gradient * learning rate\n        if iterations % print_progress == 0:  # periodically print progress\n            print(f\"Iteration {iterations}. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n        iterations += 1        # increase iteration\n        \n    print(\"Terminated!\")\n    print(f\"Iteration {iterations - 1}. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n\ngradient_descent(x, y, w=[10, 0.5], alpha=0.00001)\n\nIteration 0. Intercept 10.00. Slope 0.50.\nIteration 10. Intercept 10.00. Slope 0.71.\nIteration 20. Intercept 10.00. Slope 0.77.\nIteration 30. Intercept 10.00. Slope 0.79.\nIteration 40. Intercept 10.00. Slope 0.80.\nTerminated!\nIteration 43. Intercept 10.00. Slope 0.80.\n\n\nHmm‚Ä¶ our algorithm worked but our intercept never changed. Let‚Äôs take a look at what happened:\n\nplot_gradient_descent_2d(x, y, w=[10, 0.5], m_range=np.arange(0, 2.05, 0.05), b_range=np.arange(-30, 31, 2), alpha=0.00001)\n\n\n                                                \n\n\nOnly the slope changed in value (we see a vertical line in the plot above, with no change in the intercept parameter). That‚Äôs because the slope gradient is wayyyyy bigger than the intercept gradient. Let‚Äôs see what the real value of these params are using sklearn‚Äôs implementation:\n\nm = LinearRegression().fit(np.atleast_2d(x).T, y)\nprint(f\"sklearn inter = {m.intercept_:.2f}\")\nprint(f\"sklearn slope = {m.coef_[0]:.2f}\")\n\nsklearn inter = 14.02\nsklearn slope = 0.75\n\n\nSo we were a bit off in our implementation. We need to get the slope and intercept on the same scale‚Ä¶\n\n1.7.1 5.1. Scaling\nThere are a few ways we can solve our problem above. The most common way is to simply scale your features before gradient descent:\n\nscaler = StandardScaler()\nx_scaled = scaler.fit_transform(np.atleast_2d(x).T).flatten()\nx_scaled\n\narray([-1.28162658, -0.99995041, -0.78869328, -0.47180759, -0.29575998,\n       -0.22534094,  0.23238284,  0.30280188,  1.71118274,  1.81681131])\n\n\n\nslopes = np.arange(-60, 101, 2)\nintercepts = np.arange(-30, 171, 2)\nplot_grid_search_2d(x_scaled, y, slopes, intercepts)\n\n\n                                                \n\n\nNow let‚Äôs check a random gradient after scaling:\n\ngradient(x_scaled, y, w=[10, 0.5])\n\narray([-115.        ,  -41.54718577])\n\n\nGreat, our gradients are on a similar scale now, let‚Äôs retry gradient descent with our scaled data:\n\ngradient_descent(x_scaled, y, w=[10, 2], alpha=0.2)\n\nIteration 0. Intercept 10.00. Slope 2.00.\nIteration 10. Intercept 67.15. Slope 21.16.\nIteration 20. Intercept 67.50. Slope 21.27.\nTerminated!\nIteration 25. Intercept 67.50. Slope 21.27.\n\n\n\nm = LinearRegression().fit(np.atleast_2d(x_scaled).T, y)\nprint(f\"sklearn inter = {m.intercept_:.2f}\")\nprint(f\"sklearn slope = {m.coef_[0]:.2f}\")\n\nsklearn inter = 67.50\nsklearn slope = 21.27\n\n\nMatches perfectly! Let‚Äôs look at the journey our parameters went on:\n\nplot_gradient_descent_2d(x_scaled, y, w=[-10, -50], alpha=0.2, m_range=np.arange(-60, 101, 2), b_range=np.arange(-30, 171, 2), markers=True)\n\n\n                                                \n\n\nJust to drive the point home, note once again that changing the learning rate will affect our optimization (I added some markers on this plot to show you that we‚Äôre bouncing back-and-forth):\n\nplot_gradient_descent_2d(x_scaled, y, w=[-10, -50], alpha=0.9, m_range=np.arange(-60, 101, 2), b_range=np.arange(-30, 171, 2), markers=True)"
  },
  {
    "objectID": "chapter1_gradient-descent.html#other-optimization-algorithms",
    "href": "chapter1_gradient-descent.html#other-optimization-algorithms",
    "title": "1¬† Chapter 1: Optimization & Gradient Descent",
    "section": "1.8 6. Other Optimization Algorithms",
    "text": "1.8 6. Other Optimization Algorithms\n\nWhen you saw us using gradients earlier on you might have thought, why not just set the derivative to 0 and solve? You sure could do this! And in general, if a closed form solution exists for your problem, you should typically use it. However: - Most problems in ML do not have a closed-form solution; - Even if a closed form solution exists (e.g., linear regression), it can be extremely computationally expensive to compute if your dataset is large (many observations and/or many features).\nIn these cases, optimization algorithms like gradient descent may be appropriate choices. In actuality you will almost never use vanilla gradient descent in practice because it‚Äôs actually very slow (but the intuition behind it forms the basis of tons of optimization algorithms so it‚Äôs a great place to start learning). We‚Äôll look at a computationally lighter version of gradient descent next chapter (stochastic gradient descent) and there are also many other algorithms available! You can explore the scipy function minimize to play around with some of these algorithms:\n\nfrom scipy.optimize import minimize\n\nHere was our gradient descent implementation from earlier:\n\ngradient_descent(x_scaled, y, w=[10, 2], alpha=0.2)\n\nIteration 0. Intercept 10.00. Slope 2.00.\nIteration 10. Intercept 67.15. Slope 21.16.\nIteration 20. Intercept 67.50. Slope 21.27.\nTerminated!\nIteration 25. Intercept 67.50. Slope 21.27.\n\n\nScipy‚Äôs minimize function takes as argument the function to minimize, the function‚Äôs gradient and the starting parameter values. For a linear regression model, the MSE loss and gradient take the general form:\n\\[f(w)=\\frac{1}{n}\\sum^{n}_{i=1}(w^Tx_i-y_i)^2\\]\n\\[\\frac{\\partial{}}{\\partial{}f(w)}=\\frac{2}{n}\\sum^{n}_{i=1}(w^Tx_i-y_i)x_i\\]\nI‚Äôve coded up that up using matrix multiplication:\n\ndef mse(w, X, y):\n    \"\"\"Mean squared error.\"\"\"\n    return np.mean((X @ w - y) ** 2)\n\n\ndef mse_grad(w, X, y):\n    \"\"\"Gradient of mean squared error.\"\"\"\n    return 2 * (X.T @ (X @ w) - X.T @ y) / len(X)\n\nNow we can call minimize:\n\nw = np.array([10, 2])\nx = np.hstack((np.ones((len(x), 1)), x_scaled[:, None]))  # appening a column of 1's for the intercept\nminimize(mse, w, jac=mse_grad, args=(x, y))\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 155.48424576019042\n        x: [ 6.750e+01  2.127e+01]\n      nit: 2\n      jac: [ 0.000e+00  2.274e-14]\n hess_inv: [[ 5.505e-01 -1.507e-01]\n            [-1.507e-01  9.495e-01]]\n     nfev: 5\n     njev: 5\n\n\n\nminimize(mse, w, jac=mse_grad, args=(x, y)).x  # these are the weights, it's a bit confusing because they are called \"x\"\n\narray([67.5       , 21.27359289])\n\n\nThere are plenty of methods you can look into (most give the same answers):\n\nfor method in [\"CG\", \"L-BFGS-B\", \"SLSQP\", \"TNC\"]:\n    print(f\"Method: {method}, Weights: {minimize(mse, w, jac=mse_grad, args=(x, y), method=method).x}\")\n\nMethod: CG, Weights: [67.5        21.27359289]\nMethod: L-BFGS-B, Weights: [67.5        21.27359289]\nMethod: SLSQP, Weights: [67.5        21.27359289]\nMethod: TNC, Weights: [67.5        21.27359287]"
  },
  {
    "objectID": "chapter1_gradient-descent.html#optimizing-logistic-regression",
    "href": "chapter1_gradient-descent.html#optimizing-logistic-regression",
    "title": "1¬† Chapter 1: Optimization & Gradient Descent",
    "section": "1.9 7. Optimizing Logistic Regression",
    "text": "1.9 7. Optimizing Logistic Regression\n\nIn this final section section I‚Äôm going to demonstrate optimizing a Logistic Regression model to drive home some of the points we learned in this chapter. I‚Äôm going to sample 70 ‚Äúlegendary‚Äù pokemon (which are typically super-powered) and ‚Äúnon-legendary‚Äù pokemon from our dataset:\n\ndf = pd.read_csv(\"data/pokemon.csv\", index_col=0, usecols=['name', 'defense', 'legendary']).reset_index()\nleg_ind = df[\"legendary\"] == 1\ndf = pd.concat(\n    (df[~leg_ind].sample(sum(leg_ind), random_state=123), df[leg_ind]),\n    ignore_index=True,\n).sort_values(by='defense')\n\n\ndf.head(10)\n\n\n\n\n\n\n\n\nname\ndefense\nlegendary\n\n\n\n\n23\nSunkern\n30\n0\n\n\n2\nGastly\n30\n0\n\n\n127\nCosmog\n31\n1\n\n\n25\nMankey\n35\n0\n\n\n5\nRemoraid\n35\n0\n\n\n6\nKirlia\n35\n0\n\n\n60\nTyrogue\n35\n0\n\n\n67\nPoochyena\n35\n0\n\n\n58\nBuizel\n35\n0\n\n\n11\nNoibat\n35\n0\n\n\n\n\n\n\n\n\nx = StandardScaler().fit_transform(df[['defense']]).flatten()  # we saw before the standardizing is a good idea for optimization\ny = df['legendary'].to_numpy()\nplot_logistic(x, y)\n\n\n                                                \n\n\nWe‚Äôll be using the ‚Äútrick of ones‚Äù to help us implement these computations efficiently. For example, consider the following simple linear regression model with an intercept and a slope:\n\\[\\hat{y} = \\boldsymbol{w^T}\\boldsymbol{x} = w_0\\times{}1 + w_1\\times{}x\\]\nLet‚Äôs represent that in matrix form:\n\\[\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}\\]\nNow we can calculate \\(\\mathbf{y}\\) using matrix multiplication and the ‚Äúmatmul‚Äù Python operator:\n\nw = np.array([2, 3])\nX = np.array([[1, 5], [1, 3], [1, 4]])\nX @ w\n\narray([17, 11, 14])\n\n\n\nWe‚Äôre going to create a logistic regression model to classify a Pokemon as ‚Äúlegendary‚Äù or not. Recall that in logistic regression we map our linear model to a probability:\n\\[z=\\boldsymbol{w^T}\\boldsymbol{x}\\]\n\\[P(y = 1) = \\frac{1}{(1+\\exp(-z))}\\]\nFor classification purposes, we typically then assign this probability to a discrete class (0 or 1) based on a threshold (0.5 by default):\n\\[y=\\left\\{\n\\begin{array}{ll}\n    0, & P(y = 1)\\le0.5 \\\\\n    1, & P(y = 1)&gt;0.5 \\\\\n\\end{array}\n\\right.\\]\nLet‚Äôs code that up:\n\ndef sigmoid(x, w, output=\"soft\", threshold=0.5):\n    p = 1 / (1 + np.exp(-x @ w))\n    if output == \"soft\":\n        return p\n    elif output == \"hard\":\n        return np.where(p &gt; threshold, 1, 0)\n\nFor example, if \\(w = [0, 0]\\):\n\nones = np.ones((len(x), 1))\nX = np.hstack((ones, x[:, None]))  # add column of ones for the intercept term\nw = [-1, 3]\n\n\ny_soft = sigmoid(X, w)\ny_hard = sigmoid(X, w, \"hard\")\n\n\nplot_logistic(x, y, y_soft, threshold=0.5)\n\n\n                                                \n\n\nLet‚Äôs calculate the accuracy of the above model:\n\ndef accuracy(y, y_hat):\n    return (y_hat == y).sum() / len(y)\n\naccuracy(y, y_hard)\n\n0.7142857142857143\n\n\nJust like in the linear regression example earlier, we want to optimize the values of our weights! We need a loss function! We use the log loss (cross-entropy loss) to optimize logistic regression. Here‚Äôs the loss function and its gradient (see Appendix B: Logistic Loss if you want to learn more about these, they‚Äôre actually quite simple!):\n\\[f(w)=-\\frac{1}{n}\\sum_{i=1}^ny_i\\log\\left(\\frac{1}{1 + \\exp(-w^Tx_i)}\\right) + (1 - y_i)\\log\\left(1 - \\frac{1}{1 + \\exp(-w^Tx_i)}\\right)\\]\n\\[\\frac{\\partial f(w)}{\\partial w}=\\frac{1}{n}\\sum_{i=1}^nx_i\\left(\\frac{1}{1 + \\exp(-w^Tx_i)} - y_i\\right)\\]\n\ndef logistic_loss(w, X, y):\n    return -(y * np.log(sigmoid(X, w)) + (1 - y) * np.log(1 - sigmoid(X, w))).mean()\n\n\ndef logistic_loss_grad(w, X, y):\n    return (X.T @ (sigmoid(X, w) - y)) / len(X)\n\n\nw_opt = minimize(logistic_loss, np.array([-1, 1]), jac=logistic_loss_grad, args=(X, y)).x\nw_opt\n\narray([0.05153269, 1.34147091])\n\n\nLet‚Äôs check our solution against the sklearn implementation:\n\nlr = LogisticRegression(penalty=None).fit(x.reshape(-1, 1), y)\nprint(f\"w0: {lr.intercept_[0]:.2f}\")\nprint(f\"w1: {lr.coef_[0][0]:.2f}\")\n\nw0: 0.05\nw1: 1.34\n\n\nThis is what the optimized model looks like:\n\ny_soft = sigmoid(X, w_opt)\nplot_logistic(x, y, y_soft, threshold=0.5)\n\n\n                                                \n\n\n\ny_hard = sigmoid(X, w_opt, \"hard\")\naccuracy(y, y_hard)\n\n0.8\n\n\nChecking that against the sklearn model:\n\nlr.score(x.reshape(-1, 1), y)\n\n0.8\n\n\nI mean, that‚Äôs so cool! We replicated the sklearn behavour from scratch!!!! By the way, I‚Äôve been doing things in 2D here because it‚Äôs easy to visualize, but let‚Äôs double check that we can work in more dimensions by using attack, defense and speed to classify a Pokemon as legendary or not:\n\ndf = pd.read_csv(\"data/pokemon.csv\", index_col=0, usecols=['name', 'defense', 'attack', 'speed', 'legendary']).reset_index()\nleg_ind = df[\"legendary\"] == 1\ndf = pd.concat(\n    (df[~leg_ind].sample(sum(leg_ind), random_state=123), df[leg_ind]),\n    ignore_index=True,\n)\ndf.head()\n\n\n\n\n\n\n\n\nname\nattack\ndefense\nspeed\nlegendary\n\n\n\n\n0\nRoggenrola\n75\n85\n15\n0\n\n\n1\nGible\n70\n45\n42\n0\n\n\n2\nGastly\n35\n30\n80\n0\n\n\n3\nMinun\n40\n50\n95\n0\n\n\n4\nMarill\n20\n50\n40\n0\n\n\n\n\n\n\n\n\nx = StandardScaler().fit_transform(df[[\"defense\", \"attack\", \"speed\"]])\nX = np.hstack((np.ones((len(x), 1)), x))\ny = df[\"legendary\"].to_numpy()\n\n\nw_opt = minimize(logistic_loss, np.zeros(X.shape[1]), jac=logistic_loss_grad, args=(X, y), method=\"L-BFGS-B\").x\nw_opt\n\narray([-0.23259512,  1.33705304,  0.52029373,  1.36780376])\n\n\n\nlr = LogisticRegression(penalty=None).fit(x, y)\nprint(f\"w0: {lr.intercept_[0]:.2f}\")\nfor n, w in enumerate(lr.coef_[0]):\n    print(f\"w{n+1}: {w:.2f}\")\n\nw0: -0.23\nw1: 1.34\nw2: 0.52\nw3: 1.37\n\n\nLooks good to me!"
  },
  {
    "objectID": "chapter2_stochastic-gradient-descent.html#chapter-learning-objectives",
    "href": "chapter2_stochastic-gradient-descent.html#chapter-learning-objectives",
    "title": "2¬† Chapter 2: Stochastic Gradient Descent",
    "section": "2.1 Chapter Learning Objectives",
    "text": "2.1 Chapter Learning Objectives\n\n\nExplain and implement the stochastic gradient descent algorithm.\nExplain the advantages and disadvantages of stochastic gradient descent as compared to gradient descent.\nExplain what are epochs, batch sizes, iterations, and computations in the context of gradient descent and stochastic gradient descent."
  },
  {
    "objectID": "chapter2_stochastic-gradient-descent.html#imports",
    "href": "chapter2_stochastic-gradient-descent.html#imports",
    "title": "2¬† Chapter 2: Stochastic Gradient Descent",
    "section": "2.2 Imports",
    "text": "2.2 Imports\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom utils.plotting import *"
  },
  {
    "objectID": "chapter2_stochastic-gradient-descent.html#motivation-for-stochastic-gradient-descent",
    "href": "chapter2_stochastic-gradient-descent.html#motivation-for-stochastic-gradient-descent",
    "title": "2¬† Chapter 2: Stochastic Gradient Descent",
    "section": "2.3 1. Motivation for Stochastic Gradient Descent",
    "text": "2.3 1. Motivation for Stochastic Gradient Descent\n\nLast chapter we looked at ‚Äúvanilla‚Äù gradient descent. Almost all loss functions you‚Äôll use in ML involve a sum over all the (training) data, e.g., mean squared error:\n\\[f(w)=\\frac{1}{n}\\sum^{n}_{i=1}(h_w(x_i)-y_i)^2\\]\nHere \\(f(w)\\) is the value of the loss function, \\(h_w(x)\\) is the model we wish to fit, e.g., for linear regression: \\(h_w(x)=w^Tx\\). The goal is to find the weights \\(\\boldsymbol{w}\\) that minimize the loss function. With gradient descent we: 1. Start with some arbitrary \\(\\boldsymbol{w}\\) 2. Calculate the gradient using all training examples 3. Use the gradient to adjust \\(\\boldsymbol{w}\\) 4. Repeat for \\(I\\) iterations or until the step-size is sufficiently small\nBut if we have large \\(n\\) and/or \\(\\boldsymbol{w}\\) vanilla gradient descent becomes very computationally expensive (when we get to deep learning, we‚Äôll have models where the number of weights to optimize is in the millions!). Say n = 1,000,000, we have 1000 parameters to optimize, and we do 1000 iterations - that‚Äôs \\(10^{12}\\) computations!\nWe can reduce this workload by using just a fraction of our dataset to update our parameters each iteration (rather than using the whole data set). This is called ‚Äústochastic gradient descent‚Äù."
  },
  {
    "objectID": "chapter2_stochastic-gradient-descent.html#stochastic-gradient-descent",
    "href": "chapter2_stochastic-gradient-descent.html#stochastic-gradient-descent",
    "title": "2¬† Chapter 2: Stochastic Gradient Descent",
    "section": "2.4 2. Stochastic Gradient Descent",
    "text": "2.4 2. Stochastic Gradient Descent\n\nGradient Descent:\n\\[w_{j+1}=w_{j}-\\alpha_t\\frac{\\partial}{\\partial w_j}f(w_j)\\]\nStochastic Gradient Descent:\n\\[w_{j+1}=w_{j}-\\alpha_t\\frac{\\partial}{\\partial w_j}f_i(w_j)\\]\nPretty simple! What exactly is the difference in the above equations? Well one of them includes a subscript \\(i\\). That means, instead of updating our parameters based on a gradient calculated using all training data, we simply use one of our data points (the \\(i\\)th one). This is best seen by example, let‚Äôs use the Pokemon dataset from last chapter:\n\ndf = (pd.read_csv(\"data/pokemon.csv\", usecols=['name', 'defense', 'attack'], index_col=0)\n        .reset_index()\n     )\nx = StandardScaler().fit_transform(df[['defense']]).flatten()\ny = df['attack'].to_numpy()\nplot_pokemon(x, y, x_range=[0, 1], y_range=[0, 200], dx=0.2, dy=50)\n\n\n                                                \n\n\nLet‚Äôs fit a simple linear model to this data: \\(\\hat{y_i}=w_0+w_1x_i=w^Tx_i\\). Recall from last chapter that the gradient of the MSE loss for this linear regression model is:\n\\[\\frac{\\partial{}}{\\partial{}f(w)}=\\frac{2}{n}\\sum^{n}_{i=1}(w^Tx_i-y_i)x_i\\]\n\ndef gradient(w, X, y):\n    \"\"\"Gradient of mean squared error.\"\"\"\n    return 2 * (X.T @ (X @ w) - X.T @ y) / len(X)\n\nLet‚Äôs use the ‚Äúcolumn of ones trick‚Äù to add a column of ones to our feature data to represent the intercept term:\n\nX = np.hstack((np.ones((len(x), 1)), x[:, None]))\nX\n\narray([[ 1.        , -0.78077335],\n       [ 1.        , -0.32548801],\n       [ 1.        ,  1.62573488],\n       ...,\n       [ 1.        , -0.65069183],\n       [ 1.        ,  0.91028648],\n       [ 1.        ,  1.36557183]])\n\n\nLet‚Äôs calculate the gradient for parameters (w0, w1) = (-20, -5) using the full data set:\n\nw = np.array([-20, -5])\ngradient(w, X, y)\n\narray([-195.71535581,  -40.14066881])\n\n\nNow, let‚Äôs calculate the gradient again using just a single random point (re-run this as many times as you like):\n\ni = np.random.randint(0, len(X), 1)\nprint(\"Selected point:\")\ndisplay(df.iloc[i])\nprint(\"      Gradient: [intercept, slope]\")\nprint(f\"      All data: {[round(_, 2) for _ in gradient(w, X, y)]}\")\nprint(f\"Data point {i[0]:03}: {[round(_, 2) for _ in gradient(w, X[i], y[i])]}\")\n\nSelected point:\n\n\n\n\n\n\n\n\n\nname\nattack\ndefense\n\n\n\n\n177\nXatu\n75\n70\n\n\n\n\n\n\n\n      Gradient: [intercept, slope]\n      All data: [-195.72, -40.14]\nData point 177: [-189.02, 18.49]\n\n\nNot surprisingly, the gradient calculated using just one data point is different to the gradient calculated using all data points. For visualization purposes, let‚Äôs assume the intercept is fixed and just look at the gradient of the slope parameter (\\(w_1\\)) at \\(w_1=-5\\) at different indiviudal data points:\n\nplot_random_gradients(x, y, w=w[1], seed=2023)  # re-run to get different datapoints\n\n\n                                                \n\n\nNotice here that at a slope of -5, our single data point gradients vary in direction and magnitude. Let‚Äôs plot a histogram of the gradient of each data point at a slope of -5:\n\nplot_gradient_histogram(x, y, w=w[1])\n\n\n                                                \n\n\nThe histogram is left-skewed, indicating that more often that not, our gradient is negative (i.e., we need to increase our slope to decrease our loss - if you look at the plot above this one, you‚Äôll see that makes sense). This means that we‚Äôre highly likely to move towards the minimum even if we only use a single data point!. Hopefully you‚Äôre semi-convinced that using just one data point is computational way faster, and mathematically not a totally horrible idea.\nLet‚Äôs see stochastic gradient descent in action in the 2d case. It‚Äôs pretty much the same as we saw last chapter, except that we pick a random data point at which to calculate the gradient (which is why we call it ‚Äústochastic‚Äù).\n\nFor those who notice it, I‚Äôve also removed the ‚Äútolerance‚Äù termination criteria in our stochastic gradient descent implementation below. The reason for this is that to fairly calculate the step size which we compare to our tolerance, we should use the whole data set, as we do in regular gradient descent, not just a fraction of it. But if we do this each iteration, we forfeit the computational savings of stochastic gradient descent! So we typically leave that stopping criteria out.\n\n\ndef stochastic_gradient_descent(x, y, w, alpha, num_iterations=300, print_progress=100, seed=None):\n    \"\"\"Stochastic gradient descent for simple linear regression.\"\"\"\n    \n    print(f\"Iteration 0. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n    iterations = 1        # init iterations\n    if seed is not None:  # init seed (if given)\n        np.random.seed(seed)\n        \n    while iterations &lt;= num_iterations:\n        i = np.random.randint(len(x))            # &lt;--- this is the only new bit! &lt;---\n        g = gradient(w, x[i, None], y[i, None])  # calculate current gradient\n        w -= alpha * g                           # adjust w based on gradient * learning rate\n        if iterations % print_progress == 0:     # periodically print progress\n            print(f\"Iteration {iterations}. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n        iterations += 1  # increase iteration\n        \n    print(\"Terminated!\")\n    print(f\"Iteration {iterations - 1}. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n\n\nw = [-20, -5]\nalpha = 0.01\nX = np.hstack((np.ones((len(x), 1)), x[:, None]))\n\n\nstochastic_gradient_descent(X, y, w, alpha, seed=2020)\n\nCPU times: user 1 ¬µs, sys: 1 ¬µs, total: 2 ¬µs\nWall time: 3.1 ¬µs\nIteration 0. Intercept -20.00. Slope -5.00.\nIteration 100. Intercept 69.19. Slope 21.12.\nIteration 200. Intercept 76.37. Slope 18.07.\nIteration 300. Intercept 78.32. Slope 17.21.\nTerminated!\nIteration 300. Intercept 78.32. Slope 17.21.\n\n\nThe correct values (according to sklearn) are:\n\nm = LinearRegression().fit(np.atleast_2d(x).T, y)\n\nCPU times: user 1 ¬µs, sys: 0 ns, total: 1 ¬µs\nWall time: 3.1 ¬µs\n\n\n\nprint(f\"sklearn Intercept = {m.intercept_:.2f}\")\nprint(f\"sklearn Slope = {m.coef_[0]:.2f}\")\n\nsklearn Intercept = 77.86\nsklearn Slope = 15.07\n\n\nWe got pretty close! Let‚Äôs see the path we took:\n\nplot_gradient_descent_2d(x, y, w, alpha, np.arange(-30, 60, 2), np.arange(-40, 140, 2), max_iterations=300, stochastic=True, seed=2020)\n\n\n                                                \n\n\nAbove, we get to the minimum fairly quickly and then bounce around. The ‚Äúnoisiness‚Äù in our approach to the minimum is because we are only basing our steps on one data point. Remember, each data point has it‚Äôs own idea of how the model‚Äôs parameters should change. But as we saw earlier, it turns out that most of them want us to move us in the direction of the minimum!\nLet‚Äôs compare the above result to regular gradient descent:\n\ndef gradient_descent(x, y, w, alpha, œµ=2e-4, max_iterations=5000, print_progress=100):\n    \"\"\"Gradient descent for optimizing simple linear regression.\"\"\"\n    \n    print(f\"Iteration 0. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n    iterations = 1  # init iterations\n    dw = np.array(2 * œµ)      # init. dw\n    \n    while abs(dw.sum()) &gt; œµ and iterations &lt;= max_iterations:\n        g = gradient(w, x, y)  # calculate current gradient\n        dw = alpha * g         # change in w\n        w -= dw                # adjust w based on gradient * learning rate\n        if iterations % print_progress == 0:  # periodically print progress\n            print(f\"Iteration {iterations}. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n        iterations += 1        # increase iteration\n        \n    print(\"Terminated!\")\n    print(f\"Iteration {iterations - 1}. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n\n\ngradient_descent(X, y, w, alpha)\n\nCPU times: user 2 ¬µs, sys: 1e+03 ns, total: 3 ¬µs\nWall time: 2.86 ¬µs\nIteration 0. Intercept -20.00. Slope -5.00.\nIteration 100. Intercept 64.88. Slope 12.41.\nIteration 200. Intercept 76.14. Slope 14.72.\nIteration 300. Intercept 77.63. Slope 15.02.\nIteration 400. Intercept 77.83. Slope 15.06.\nTerminated!\nIteration 466. Intercept 77.85. Slope 15.07.\n\n\nThis got us the exact same answer as sklearn, let‚Äôs take a look at the path:\n\nplot_gradient_descent_2d(x, y, w, alpha, np.arange(-30, 60, 2), np.arange(-40, 140, 2))\n\n\n                                                \n\n\nObviously vanilla gradient descent looks nice above, but consider the computational savings: - In stochastic gradient descent we converged after 300 ‚Äúiterations‚Äù: 1 iteration = 1 data point, so we did 300 computations (although I‚Äôll note that I told the algorithm to do 300 iterations, we could have specified less or more, it won‚Äôt change the fact that we saved a lot of computational time here!) - In gradient descent we converged after 466 ‚Äúiterations‚Äù: 1 iteration = 801 data points, so we did 466 x 801 = 373,266 computations"
  },
  {
    "objectID": "chapter2_stochastic-gradient-descent.html#mini-batch-gradient-descent",
    "href": "chapter2_stochastic-gradient-descent.html#mini-batch-gradient-descent",
    "title": "2¬† Chapter 2: Stochastic Gradient Descent",
    "section": "2.5 3. Mini-batch Gradient Descent",
    "text": "2.5 3. Mini-batch Gradient Descent\n\nOkay so it appears stochastic gradient descent is computationally better than gradient descent, but gradient descent makes better steps (‚Äúiterations‚Äù). Is there an in-between compromise? Yes!\nEnter: minibatch stochastic gradient descent. The idea here is simple, rather than calculating the gradient from just one random point, calculate it based on a ‚Äúbatch‚Äù of points:\n\nbatch_size = 5\ni = np.random.choice(range(len(X)), batch_size)\nprint(\"Selected point:\")\ndisplay(df.iloc[i])\nprint(\"   Gradient of: [intercept, slope]\")\nprint(f\"      All data: {[round(_, 2) for _ in gradient(w, X, y)]}\")\nprint(f\"Data point {i[0]:03}: {[round(_, 2) for _ in gradient(w, X[i], y[i])]}\")\n\nSelected point:\n\n\n\n\n\n\n\n\n\nname\nattack\ndefense\n\n\n\n\n65\nMachop\n80\n50\n\n\n278\nPelipper\n50\n100\n\n\n4\nCharmeleon\n64\n58\n\n\n19\nRaticate\n71\n70\n\n\n605\nBeheeyem\n75\n75\n\n\n\n\n\n\n\n   Gradient of: [intercept, slope]\n      All data: [-195.72, -40.14]\nData point 065: [-175.22, 19.69]\n\n\nWe would still expect the gradient calculated from a batch of data to be different to the gradient calculated using all the data, but not as different to using only a single point. Once again let‚Äôs visualize this for just the slope parameter to drive the point home:\n\nplot_minibatch_gradients(x, y, w=-1, batch_sizes=[1, 10, 50, 100, 200, 400, len(x)], seed=2023)\n\n\n                                                \n\n\nAbove we can see that the larger the batch of data we use, the closer we are to the gradient calculated using the whole dataset (which makes sense). But also the bigger the batch, the more computations will be needed!\nLet‚Äôs code up minibatch gradient descent. The code is almost the same as before but we are choosing batch_size of random points each iteration.\n\nWe‚Äôll be sampling without replacement in each iteration, I‚Äôll talk about that more later.\n\n\ndef minibatch_gradient_descent(x, y, w, alpha, batch_size, num_iterations=300, print_progress=100, seed=None):\n    \"\"\"Minibatch gradient descent for simple linear regression.\"\"\"\n    \n    print(f\"Iteration 0. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n    iterations = 1        # init iterations\n    if seed is not None:  # init seed (if given)\n        np.random.seed(seed)\n        \n    while iterations &lt;= num_iterations:\n        i = np.random.choice(range(len(x)), batch_size, replace=False)  # &lt;--- this is the only new bit! &lt;---\n        g = gradient(w, x[i], y[i])              # calculate current gradient\n        w -= alpha * g                           # adjust w based on gradient * learning rate\n        if iterations % print_progress == 0:     # periodically print progress\n            print(f\"Iteration {iterations}. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n        iterations += 1  # increase iteration\n        \n    print(\"Terminated!\")\n    print(f\"Iteration {iterations - 1}. Intercept {w[0]:.2f}. Slope {w[1]:.2f}.\")\n\n\nbatch_size = 10\nminibatch_gradient_descent(X, y, w, alpha, batch_size=batch_size, seed=2020)\n\nIteration 0. Intercept -20.00. Slope -5.00.\nIteration 100. Intercept 65.25. Slope 11.61.\nIteration 200. Intercept 75.84. Slope 12.49.\nIteration 300. Intercept 77.67. Slope 14.42.\nTerminated!\nIteration 300. Intercept 77.67. Slope 14.42.\n\n\n\nplot_gradient_descent_2d(x, y, w, alpha, np.arange(-30, 60, 2), np.arange(-40, 140, 2), max_iterations=300, stochastic=True, batch_size=batch_size, seed=2020)\n\n\n                                                \n\n\nAbove, we still get the ‚Äúwobbliness‚Äù of stochastic gradient descent but it‚Äôs much smoother now!\nLet‚Äôs view our three algorithms all together now!\n\nf1 = plot_gradient_descent_2d(x, y, w, alpha, np.arange(-30, 60, 2), np.arange(-40, 140, 2))\nf2 = plot_gradient_descent_2d(x, y, w, alpha, np.arange(-30, 60, 2), np.arange(-40, 140, 2), max_iterations=300, stochastic=True, seed=2023)\nf3 = plot_gradient_descent_2d(x, y, w, alpha, np.arange(-30, 60, 2), np.arange(-40, 140, 2), max_iterations=300, stochastic=True, batch_size=batch_size, seed=2020)\nplot_panel(f1, f2, f3)\n\n\n                                                \n\n\n\n\n\nMethod\nIntercept\nSlope\nIterations\nData Per Iteration\nData Points Used\n\n\n\n\nsklearn\n77.86\n15.07\n-\n-\n-\n\n\ngradient descent\n77.85\n15.07\n466\n801\n373,266\n\n\nstochastic gradient descent\n76.33\n11.26\n300\n1\n300\n\n\nMgradient descent\n75.60\n15.09\n300\n10\n3000"
  },
  {
    "objectID": "chapter2_stochastic-gradient-descent.html#terminology",
    "href": "chapter2_stochastic-gradient-descent.html#terminology",
    "title": "2¬† Chapter 2: Stochastic Gradient Descent",
    "section": "2.6 4. Terminology",
    "text": "2.6 4. Terminology\n\nSo I‚Äôve used a couple of important terms in the last two chapters and it‚Äôs time we clarify them. Assume we have a dataset of \\(n\\) observations. There are three terms you need to know when it comes to gradient descent algorithms: - Iteration: each time you update model weights - Batch: a subset of data used in an iteration - Epoch: the number of iterations to look at all \\(n\\) observations (people sometimes also say a ‚Äúfull pass through the dataset‚Äù)\nLet‚Äôs put those defintions in context: - In gradient descent, each iteration involves computing the gradient over all examples, so \\(1 \\text{ iteration} = 1 \\text{ epoch}\\) - In stochastic gradient descent, each iteration involves one data point, so \\(n \\text{ iterations} = 1 \\text{ epoch}\\) - In minibatch gradient descent, each iteration involves a batch of data, so \\(\\frac{n}{\\text{batch size}} \\text{ iterations} = 1 \\text{ epoch}\\)\nFrom our examples above (recall that \\(n=801\\)):\n\n\n\nMethod\nIterations\nData Per Iteration (batch size)\nEpochs\n\n\n\n\ngradient descent\n466\n801\n466\n\n\nstochastic gradient descent\n300\n1\n0.37\n\n\nMgradient descent\n300\n10\n3.74\n\n\n\nIn practice nobody really says ‚Äúminibatch stochastic gradient descent‚Äù, we just say stochastic gradient descent and in stochastic gradient descent you can specify a batch size of anything between 1 and \\(n\\). So, why do we bother with all this hassle? Because we want a quantity to talk about that is meaningful to humans: - ‚ÄúI did 10 iterations of gradient descent‚Äù means something (that‚Äôs 10 full passes through the dataset). - ‚ÄúI did 10 iterations of stochastic gradient descent‚Äù means nothing to me (it depends what the batch size is). - ‚ÄúI did 10 epochs of stochastic gradient descent‚Äù has more meaning to me (that‚Äôs 10 full passes through the dataset)."
  },
  {
    "objectID": "chapter2_stochastic-gradient-descent.html#final-remarks",
    "href": "chapter2_stochastic-gradient-descent.html#final-remarks",
    "title": "2¬† Chapter 2: Stochastic Gradient Descent",
    "section": "2.7 5. Final Remarks",
    "text": "2.7 5. Final Remarks\n\n\n2.7.1 5.1. Sampling With or Without Replacement\nWhen doing stochastic gradient descent we really have 3 options: 1. Shuffle the dataset and pre-divide it into batches, like cross-validation. This is ‚Äúwithout replacement‚Äù, every example is used once. 2. Sample a batch each iteration without replacement, so you won‚Äôt have the same sample occuring more that once in a batch but you might have the same example in both batch 1 and batch 2. Every example may not be used in this approach. 3. Like Approach 2, but even each batch is sampled with replacement, so you might have the same example twice in batch 1. Every example may not be used in this approach.\nWe typically use approach 1 or 2 (the default in Pytorch, the deep learning package we‚Äôll be using, is approach 1). Empirically, sampling without replacement tends to lead to more optimal solutions/faster convergence and this has been proved mathematically in some cases.\n\n\n2.7.2 5.2. Learning Rate\nUp until now we‚Äôve been using a constant learning rate (\\(\\alpha\\)) in our algorithms. There are two main ways we could potentially improve on this naive method: 1. Use an ‚Äúadaptive‚Äù learning rate - where we take big steps when we are far from the minimum of our loss function, and smaller steps when we are close to it; 2. Use ‚Äúmomentum‚Äù - using how our learning rate has changed in past iterations to influence how it changes in future iterations\nOften when people use stochastic gradient descent they leave \\(\\alpha\\) constant and things tend to work out okay. However these days in practice we often use fancier variants of gradient descent, such as Adam, which include more bells and whistles such as those I describe above. We‚Äôll be using Adam a lot through the rest of this course. These algorithms get pretty complex and it‚Äôs beyond the scope of the course to know exactly how to derive them, but just know they are essentially fancier versions of gradient descent (if you‚Äôre interested in reading more about these kinds of algorithms, see here).\n\n\n2.7.3 5.3. Local Minima\nOptimization algorithms like gradient descent are susceptible to getting stuck in local minima. That means, minima that are not the true global minima, but are still a ‚Äúdip‚Äù in the loss function. Stochastic gradient descent can help with this a bit due to the ‚Äújumpy‚Äù randomness in the optimization process, but note that the main advantage of stochastic gradient descent vs gradient descent is really the computational savings.\nThe above mentioned fancy algorithms like Adam that include things like momentum and adaptive learning rates are better designed to speed up convergence and try and avoid local minima. Here‚Äôs a comparison of stochastic gradient descent vs Adam for an arbitrary loss function and model - notice how stochastic gradient descent gets ‚Äústuck‚Äù in a local minima, while Adam is able to navigate all the way to the global minima:"
  },
  {
    "objectID": "chapter3_pytorch-neural-networks-pt1.html#chapter-learning-objectives",
    "href": "chapter3_pytorch-neural-networks-pt1.html#chapter-learning-objectives",
    "title": "3¬† Chapter 3: Introduction to Pytorch & Neural Networks",
    "section": "3.1 Chapter Learning Objectives",
    "text": "3.1 Chapter Learning Objectives\n\n\nDescribe the difference between NumPy and torch arrays (np.array vs.¬†torch.Tensor).\nExplain fundamental concepts of neural networks such as layers, nodes, activation functions, etc.\nCreate a simple neural network in PyTorch for regression or classification."
  },
  {
    "objectID": "chapter3_pytorch-neural-networks-pt1.html#imports",
    "href": "chapter3_pytorch-neural-networks-pt1.html#imports",
    "title": "3¬† Chapter 3: Introduction to Pytorch & Neural Networks",
    "section": "3.2 Imports",
    "text": "3.2 Imports\n\n\\(f(x) = 1\\)\n\nimport sys\nimport math\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torchinfo import summary\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.datasets import make_regression, make_circles, make_blobs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom utils.plotting import *\n\n\nnn.Softmax(\n\n\nInit signature: nn.Softmax(dim: Optional[int] = None) -&gt; None\nDocstring:     \nApplies the Softmax function to an n-dimensional input Tensor\nrescaling them so that the elements of the n-dimensional output Tensor\nlie in the range [0,1] and sum to 1.\nSoftmax is defined as:\n.. math::\n    \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\nWhen the input Tensor is a sparse tensor then the unspecifed\nvalues are treated as ``-inf``.\nShape:\n    - Input: :math:`(*)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(*)`, same shape as the input\nReturns:\n    a Tensor of the same dimension and shape as the input with\n    values in the range [0, 1]\nArgs:\n    dim (int): A dimension along which Softmax will be computed (so every slice\n        along dim will sum to 1).\n.. note::\n    This module doesn't work directly with NLLLoss,\n    which expects the Log to be computed between the Softmax and itself.\n    Use `LogSoftmax` instead (it's faster and has better numerical properties).\nExamples::\n    &gt;&gt;&gt; m = nn.Softmax(dim=1)\n    &gt;&gt;&gt; input = torch.randn(2, 3)\n    &gt;&gt;&gt; output = m(input)\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/dl-pytorch/lib/python3.10/site-packages/torch/nn/modules/activation.py\nType:           type\nSubclasses:     Softmax"
  },
  {
    "objectID": "chapter3_pytorch-neural-networks-pt1.html#introduction",
    "href": "chapter3_pytorch-neural-networks-pt1.html#introduction",
    "title": "3¬† Chapter 3: Introduction to Pytorch & Neural Networks",
    "section": "3.3 1. Introduction",
    "text": "3.3 1. Introduction\n\nPyTorch is a Python-based tool for scientific computing that provides several main features: - torch.Tensor, an n-dimensional array similar to that of NumPy, but which can run on GPUs - Computational graphs and an automatic differentiation enginge for building and training neural networks\nYou can install PyTorch from: https://pytorch.org/."
  },
  {
    "objectID": "chapter3_pytorch-neural-networks-pt1.html#pytorchs-tensor",
    "href": "chapter3_pytorch-neural-networks-pt1.html#pytorchs-tensor",
    "title": "3¬† Chapter 3: Introduction to Pytorch & Neural Networks",
    "section": "3.4 2. PyTorch‚Äôs Tensor",
    "text": "3.4 2. PyTorch‚Äôs Tensor\n\nIn PyTorch a tensor is just like NumPy‚Äôs ndarray which most readers will be familiar with already (if not, check out Chapter 5 and Chapter 6 of my Python Programming for Data Science course).\nA key difference between PyTorch‚Äôs torch.Tensor and NumPy‚Äôs np.array is that torch.Tensor was constructed to integrate with GPUs and PyTorch‚Äôs computational graphs (more on that next chapter though).\n\n3.4.1 2.1. ndarray vs tensor\nCreating and working with tensors is much the same as with NumPy ndarrays. You can create a tensor with torch.tensor():\n\ntensor_1 = torch.tensor([1, 2, 3])\ntensor_2 = torch.tensor([1, 2, 3], dtype=torch.float32)\ntensor_3 = torch.tensor(np.array([1, 2, 3]))\n\nfor t in [tensor_1, tensor_2, tensor_3]:\n    print(f\"{t}, dtype: {t.dtype}\")\n\ntensor([1, 2, 3]), dtype: torch.int64\ntensor([1., 2., 3.]), dtype: torch.float32\ntensor([1, 2, 3]), dtype: torch.int64\n\n\nPyTorch also comes with most of the NumPy functions you‚Äôre probably already familiar with:\n\ntorch.zeros(2, 2)  # zeroes\n\ntensor([[0., 0.],\n        [0., 0.]])\n\n\n\ntorch.ones(2, 2)  # ones\n\ntensor([[1., 1.],\n        [1., 1.]])\n\n\n\ntorch.randn(3, 2)  # random normal\n\ntensor([[ 1.9321,  0.8474],\n        [-0.3427,  1.0862],\n        [ 0.0753,  0.6788]])\n\n\n\ntorch.rand(2, 3, 2)  # rand uniform\n\ntensor([[[0.0975, 0.1262],\n         [0.8730, 0.9583],\n         [0.1787, 0.0684]],\n\n        [[0.3070, 0.3965],\n         [0.1442, 0.1952],\n         [0.2312, 0.6291]]])\n\n\nJust like in NumPy we can look at the shape of a tensor with the .shape attribute:\n\nx = torch.rand(2, 3, 2, 2)\nx.shape\n\ntorch.Size([2, 3, 2, 2])\n\n\n\nx.ndim\n\n4\n\n\n\n\n3.4.2 2.2. Tensors and Data Types\nDifferent data types have different memory and computational implications (see Chapter 6 of Python Programming for Data Science for more). In Pytorch we‚Äôll be building networks that require thousands or even millions of floating point calculations! In such cases, using a smaller dtype like float32 can significantly speed up computations and reduce memory requirements. The default float dtype in pytorch float32, as opposed to NumPy‚Äôs float64. In fact some operations in Pytorch will even throw an error if you pass a high-memory dtype!\n\nprint(np.array([3.14159]).dtype)\nprint(torch.tensor([3.14159]).dtype)\n\nfloat64\ntorch.float32\n\n\nBut just like in NumPy, you can always specify the particular dtype you want using the dtype argument:\n\nprint(torch.tensor([3.14159], dtype=torch.float64).dtype)\n\ntorch.float64\n\n\n\n\n3.4.3 2.3. Operations on Tensors\nTensors operate just like ndarrays and have a variety of familiar methods that can be called off them:\n\nprint(a)\nprint(b)\n\ntensor([[0.5628, 0.0204, 0.7567]])\ntensor([[0.7967],\n        [0.0798],\n        [0.6384]])\n\n\n\na = torch.rand(1, 3)\nb = torch.rand(3, 1)\n\na + b  # broadcasting betweean a 1 x 3 and 3 x 1 tensor\n\ntensor([[0.9794, 1.4947, 0.7680],\n        [1.2371, 1.7524, 1.0257],\n        [0.6201, 1.1354, 0.4087]])\n\n\n\na * b\n\ntensor([[0.1989, 0.5555, 0.0526],\n        [0.2729, 0.7624, 0.0722],\n        [0.0956, 0.2671, 0.0253]])\n\n\n\na.mean()\n\ntensor(0.3887)\n\n\n\na.sum()\n\ntensor(1.1662)\n\n\n\n\n3.4.4 2.4. Indexing\nOnce again, same as NumPy!\n\nX = torch.rand(5, 2)\nprint(X)\n\ntensor([[0.3466, 0.5252],\n        [0.5558, 0.3536],\n        [0.2546, 0.9431],\n        [0.2136, 0.0476],\n        [0.0478, 0.7530]])\n\n\n\nprint(X[0, :])\nprint(X[0])\nprint(X[:, 0])\n\ntensor([0.3466, 0.5252])\ntensor([0.3466, 0.5252])\ntensor([0.3466, 0.5558, 0.2546, 0.2136, 0.0478])\n\n\n\n\n3.4.5 2.5. NumPy Bridge\nSometimes we might want to convert a tensor back to a NumPy array. We can do that using the .numpy() method:\n\nX = torch.rand(3,3)\nprint(type(X))\nX_NumPy = X.numpy()\nprint(type(X_NumPy))\n\n&lt;class 'torch.Tensor'&gt;\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\n3.4.6 2.6. GPU and CUDA Tensors\nGPU stands for ‚Äúgraphical processing unit‚Äù (as opposed to a CPU: central processing unit). GPUs were originally developed for gaming, they are very fast at performing operations on large amounts of data by performing them in parallel (think about updating the value of all pixels on a screen very quickly as a player moves around in a game). More recently, GPUs have been adapted for more general purpose programming. Neural networks can typically be broken into smaller computations that can be performed in parallel on a GPU. PyTorch is tightly integrated with CUDA - a software layer that facilitates interactions with a GPU (if you have one). You can check if you have GPU capability using:\nIt was announced in May 2022 that Apple Silicon macOS devices can use PyTorch GPU-acceleration via the new M1 chips: - https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\nBy default, PyTorch uses the CPU is the default device (as seen below). However, this the device can be specified to use GPU-acceleration.\n\nprint(torch.cuda.is_available())\nprint(X.device)\n\nFalse\ncpu\n\n\nWhen training on a machine that has a GPU, you need to tell PyTorch you want to use it. You‚Äôll see the following at the top of most PyTorch code:\n\ndevice = torch.device('cuda' if torch.cuda.is_available() if else  else 'cpu')\nprint(device)\n\ncpu\n\n\nHowever, if we‚Äôre working on a team that uses macOS, we can edit this to use GPU-acceleration of Apple Silicon: - https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c\n\n# this ensures that the current MacOS version is at least 12.3+\nprint(torch.backends.mps.is_available())\n# this ensures that the current current PyTorch installation was built with MPS activated.\nprint(torch.backends.mps.is_built())\n\nTrue\nTrue\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() & torch.backends.mps.is_built() else 'cpu'))\nprint(device)\n\nmps\n\n\nYou can then use the device argument when creating tensors to specify whether you wish to use a CPU or GPU. Or if you want to move a tensor between the CPU and GPU, you can use the .to() method:\n\nX = torch.rand(2, 2, 2, device=device)\nprint(X.device)\n\nmps:0\n\n\n\n# X.to('cuda')  # this would give me an error as I don't have a GPU so I'm commenting out\n\nWe‚Äôll revisit GPUs later in the course when we are working with bigger datasets and more complex networks. For now, we can work on the CPU just fine.\nAlso, the for the example below, the CPU runs much fast than the GPU on the M1. So it‚Äôs not the best use case.\n\ndtype = torch.float\nprint(device)\n\n# Create random input and output data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n# Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n\nmps\n99 4473.283203125\n199 2969.77880859375\n299 1973.004150390625\n399 1312.0587158203125\n499 873.7158203125\n599 582.9464111328125\n699 390.02764892578125\n799 262.0018005371094\n899 177.02041625976562\n999 120.59703063964844\n1099 83.12467193603516\n1199 58.23103713989258\n1299 41.68899917602539\n1399 30.693021774291992\n1499 23.38113021850586\n1599 18.51732063293457\n1699 15.280743598937988\n1799 13.12615966796875\n1899 11.69122314453125\n1999 10.735124588012695\nResult: y = -0.017286736518144608 + 0.8172395825386047 x + 0.002982252510264516 x^2 + -0.08771169185638428 x^3\nCPU times: user 2.44 s, sys: 241 ms, total: 2.68 s\nWall time: 3.58 s\n\n\n\ndtype = torch.float\ndevice = 'cpu'\nprint(device)\n\n# Create random input and output data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n# Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n\ncpu\n99 1223.80224609375\n199 847.2777709960938\n299 588.0562133789062\n399 409.39373779296875\n499 286.12103271484375\n599 200.97528076171875\n699 142.1028289794922\n799 101.3544921875\n899 73.122802734375\n999 53.54385757446289\n1099 39.952796936035156\n1199 30.50957489013672\n1299 23.942550659179688\n1399 19.371662139892578\n1499 16.187564849853516\n1599 13.967682838439941\n1699 12.418875694274902\n1799 11.337446212768555\n1899 10.581811904907227\n1999 10.053459167480469\nResult: y = -0.03444155678153038 + 0.8437882661819458 x + 0.0059417434968054295 x^2 + -0.09148798882961273 x^3\nCPU times: user 83.7 ms, sys: 1.47 ms, total: 85.2 ms\nWall time: 84.4 ms"
  },
  {
    "objectID": "chapter3_pytorch-neural-networks-pt1.html#neural-network-basics",
    "href": "chapter3_pytorch-neural-networks-pt1.html#neural-network-basics",
    "title": "3¬† Chapter 3: Introduction to Pytorch & Neural Networks",
    "section": "3.5 3. Neural Network Basics",
    "text": "3.5 3. Neural Network Basics\n\nIt‚Äôs probably that you‚Äôve already learned about several machine learning algorithms (kNN, Random Forest, SVM, etc.). Neural networks are simply another algorithm and actually one of the simplest in my opinion! As we‚Äôll see, a neural network is just a sequence of linear and non-linear transformations. Often you see something like this when learning about/using neural networks:\n\nSo what on Earth does that all mean? Well we are going to build up some intuition one step at a time.\n\n3.5.1 3.1. Simple Linear Regression with a Neural Network\nLet‚Äôs create a simple regression dataset with 500 observations:\n\nX, y = make_regression(n_samples=500, n_features=1, random_state=0, noise=10.0)\nplot_regression(X, y)\n\n\n                                                \n\n\nWe can fit a simple linear regression to this data using sklearn:\n\nsk_model = LinearRegression().fit(X, y)\nplot_regression(X, y, sk_model.predict(X))\n\n\n                                                \n\n\nHere are the parameters of that fitted line:\n\nprint(f\"w_0: {sk_model.intercept_:.2f} (bias/intercept)\")\nprint(f\"w_1: {sk_model.coef_[0]:.2f}\")\n\nw_0: -0.77 (bias/intercept)\nw_1: 45.50\n\n\nAs an equation, that looks like this:\n\\[\\hat{y}=-0.77 + 45.50X\\]\nOr in matrix form:\n\\[\\begin{bmatrix} \\hat{y_1} \\\\ \\hat{y_2} \\\\ \\vdots \\\\ \\hat{y_n} \\end{bmatrix}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix} -0.77 \\\\ 45.55 \\end{bmatrix}\\]\nOr in graph form I‚Äôll represent it like this:\n\n\n\n3.5.2 3.2. Linear Regression with a Neural Network in PyTorch\nSo let‚Äôs implement the above in PyTorch to start gaining an intuition about neural networks! Almost every neural network model you build in PyTorch will inherit from torch.nn.Module. If you‚Äôre unfamiliar with class inheritance check out this section of Python Programming for Data Science. Basically, inheritance allows us to inherit functionality from an existing class into a new class without having to write the code ourselves!\nLet‚Äôs create a model called linearRegression and then I‚Äôll walk you through the syntax:\n\nnn.Module?\n\n\nInit signature: nn.Module() -&gt; None\nDocstring:     \nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::\n    import torch.nn as nn\n    import torch.nn.functional as F\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their\nparameters converted too when you call :meth:`to`, etc.\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/dl-pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\nType:           type\nSubclasses:     Identity, Linear, Bilinear, _ConvNd, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, ...\n\n\n\n\nclass linearRegression(nn.Module):  # our class inherits from nn.Module and we can call it anything we like\n    def __init__(self, input_size, output_size):\n        super().__init__()                                # super().__init__() makes our class inherit everything from torch.nn.Module\n        self.linear = nn.Linear(input_size, output_size)  # this is a simple linear layer: wX + b\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\nLet‚Äôs step through the above:\nclass linearRegression(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__() \n^ Here we‚Äôre creating a class called linearRegression and inheriting the methods and attributes of nn.Module (hint: try typing help(linearRegression) to see all the things we inheritied from nn.Module).\n        self.linear = nn.Linear(input_size, output_size)\n^ Here we‚Äôre defining a ‚ÄúLinear‚Äù layer, which just means wX + b, i.e., the weights of the network, multiplied by the input features plus the bias.\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n^ PyTorch networks created with nn.Module must have a forward() method. It accepts the input data x and passes it through the defined operations. In this case, we are passing x into our linear layer and getting an output out.\nAfter defining the model class, we can create an instance of that class:\n\nmodel = linearRegression(input_size=1, output_size=1)\n\n\nWe can check out our model using print():\n\nprint(model)\n\nlinearRegression(\n  (linear): Linear(in_features=1, out_features=1, bias=True)\n)\n\n\nOr the more useful summary() (which we imported at the top of this notebook with from torchsummary import summary):\n\nsummary?\n\n\nSignature:\nsummary(\n    model: 'nn.Module',\n    input_size: 'INPUT_SIZE_TYPE | None' = None,\n    input_data: 'INPUT_DATA_TYPE | None' = None,\n    batch_dim: 'int | None' = None,\n    cache_forward_pass: 'bool | None' = None,\n    col_names: 'Iterable[str] | None' = None,\n    col_width: 'int' = 25,\n    depth: 'int' = 3,\n    device: 'torch.device | str | None' = None,\n    dtypes: 'list[torch.dtype] | None' = None,\n    mode: 'str | None' = None,\n    row_settings: 'Iterable[str] | None' = None,\n    verbose: 'int | None' = None,\n    **kwargs: 'Any',\n) -&gt; 'ModelStatistics'\nDocstring:\nSummarize the given PyTorch model. Summarized information includes:\n    1) Layer names,\n    2) input/output shapes,\n    3) kernel shape,\n    4) # of parameters,\n    5) # of operations (Mult-Adds),\n    6) whether layer is trainable\nNOTE: If neither input_data or input_size are provided, no forward pass through the\nnetwork is performed, and the provided model information is limited to layer names.\nArgs:\n    model (nn.Module):\n            PyTorch model to summarize. The model should be fully in either train()\n            or eval() mode. If layers are not all in the same mode, running summary\n            may have side effects on batchnorm or dropout statistics. If you\n            encounter an issue with this, please open a GitHub issue.\n    input_size (Sequence of Sizes):\n            Shape of input data as a List/Tuple/torch.Size\n            (dtypes must match model input, default is FloatTensors).\n            You should include batch size in the tuple.\n            Default: None\n    input_data (Sequence of Tensors):\n            Arguments for the model's forward pass (dtypes inferred).\n            If the forward() function takes several parameters, pass in a list of\n            args or a dict of kwargs (if your forward() function takes in a dict\n            as its only argument, wrap it in a list).\n            Default: None\n    batch_dim (int):\n            Batch_dimension of input data. If batch_dim is None, assume\n            input_data / input_size contains the batch dimension, which is used\n            in all calculations. Else, expand all tensors to contain the batch_dim.\n            Specifying batch_dim can be an runtime optimization, since if batch_dim\n            is specified, torchinfo uses a batch size of 1 for the forward pass.\n            Default: None\n    cache_forward_pass (bool):\n            If True, cache the run of the forward() function using the model\n            class name as the key. If the forward pass is an expensive operation,\n            this can make it easier to modify the formatting of your model\n            summary, e.g. changing the depth or enabled column types, especially\n            in Jupyter Notebooks.\n            WARNING: Modifying the model architecture or input data/input size when\n            this feature is enabled does not invalidate the cache or re-run the\n            forward pass, and can cause incorrect summaries as a result.\n            Default: False\n    col_names (Iterable[str]):\n            Specify which columns to show in the output. Currently supported: (\n                \"input_size\",\n                \"output_size\",\n                \"num_params\",\n                \"params_percent\",\n                \"kernel_size\",\n                \"mult_adds\",\n                \"trainable\",\n            )\n            Default: (\"output_size\", \"num_params\")\n            If input_data / input_size are not provided, only \"num_params\" is used.\n    col_width (int):\n            Width of each column.\n            Default: 25\n    depth (int):\n            Depth of nested layers to display (e.g. Sequentials).\n            Nested layers below this depth will not be displayed in the summary.\n            Default: 3\n    device (torch.Device):\n            Uses this torch device for model and input_data.\n            If not specified, uses result of torch.cuda.is_available().\n            Default: None\n    dtypes (List[torch.dtype]):\n            If you use input_size, torchinfo assumes your input uses FloatTensors.\n            If your model use a different data type, specify that dtype.\n            For multiple inputs, specify the size of both inputs, and\n            also specify the types of each parameter here.\n            Default: None\n    mode (str)\n            Either \"train\" or \"eval\", which determines whether we call\n            model.train() or model.eval() before calling summary().\n            Default: \"eval\".\n    row_settings (Iterable[str]):\n            Specify which features to show in a row. Currently supported: (\n                \"ascii_only\",\n                \"depth\",\n                \"var_names\",\n            )\n            Default: (\"depth\",)\n    verbose (int):\n            0 (quiet): No output\n            1 (default): Print model summary\n            2 (verbose): Show weight and bias layers in full detail\n            Default: 1\n            If using a Juypter Notebook or Google Colab, the default is 0.\n    **kwargs:\n            Other arguments used in `model.forward` function. Passing *args is no\n            longer supported.\nReturn:\n    ModelStatistics object\n            See torchinfo/model_statistics.py for more information.\nFile:      ~/anaconda3/envs/dl-pytorch/lib/python3.10/site-packages/torchinfo/torchinfo.py\nType:      function\n\n\n\n\nprint(summary(model))\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nlinearRegression                         --\n‚îú‚îÄLinear: 1-1                            2\n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n=================================================================\n\n\n\nprint(summary(model, input_size = (1,1)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nlinearRegression                         [1, 1]                    --\n‚îú‚îÄLinear: 1-1                            [1, 1]                    2\n==========================================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n\n\nNotice how we have two parameters? We have one for the weight (w1) and one for the bias (w0). These were initialized randomly by PyTorch when we created our model. They can be accessed with model.state_dict():\n\nmodel.state_dict()\n\nOrderedDict([('linear.weight', tensor([[-0.0189]])),\n             ('linear.bias', tensor([0.6425]))])\n\n\nOkay, before we move on, the x and y data I created are currently NumPy arrays but they need to be PyTorch tensors. Let‚Äôs convert them:\n\nX_t = torch.tensor(X, dtype=torch.float32)  # I'll explain requires_grad next Chapter\ny_t = torch.tensor(y, dtype=torch.float32)\n\nWe have a working model right now and could tell it to give us some output with this syntax:\n\ny_p = model(X_t[0]).item()\nprint(f\"Predicted: {y_p:.2f}\")\nprint(f\"   Actual: {y[0]:.2f}\")\n\nPredicted: 0.63\n   Actual: 31.08\n\n\nOur prediction is pretty bad because our model is not trained/fitted yet! As we learned in the past few chapters, to fit our model we need: 1. a loss function (called ‚Äúcriterion‚Äù in PyTorch) to tell us how good/bad our predictions are - we‚Äôll use mean squared error, torch.nn.MSELoss() 2. an optimization algorithm to help optimise model parameters - we‚Äôll use GD, torch.optim.SGD()\n\nLEARNING_RATE = 0.1\ncriterion = nn.MSELoss()  # loss function\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  # optimization algorithm is SGD\n\nBefore we train I‚Äôm going to create a ‚Äúdata loader‚Äù to help batch my data. We‚Äôll talk more about these in later chapters but just think of them as generators that yield data to us on request (if you‚Äôre unfamiliar with generators, check out this section of Python Programming for Data Science). We‚Äôll use a BATCH_SIZE = 50 (which should give us 10 batches because we have 500 data points):\n\nBATCH_SIZE = 50\ndataset = TensorDataset(X_t, y_t)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nSo, we should have 10 batches:\n\nprint(f\"Total number of batches: {len(dataloader)}\")\n\nTotal number of batches: 10\n\n\nWe can look at a batch using this syntax:\n\nXX, yy = next(iter(dataloader))\nprint(f\" Shape of feature data (X) in batch: {XX.shape}\")\nprint(f\"Shape of response data (y) in batch: {yy.shape}\")\n\n Shape of feature data (X) in batch: torch.Size([50, 1])\nShape of response data (y) in batch: torch.Size([50])\n\n\nWith our data loader defined, let‚Äôs train our simple network for 5 epochs of SGD!\n\nI‚Äôll explain all the code here next chapter but scan throught it, it‚Äôs not too hard to see what‚Äôs going on!\n\n\ndef trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n    \n    for epoch in range(epochs):\n        losses = 0\n        for X, y in dataloader:\n            optimizer.zero_grad()       # Clear gradients w.r.t. parameters\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss\n            loss.backward()             # Getting gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n            losses += loss.item()       # Add loss for this batch to running total\n        if verbose: print(f\"epoch: {epoch + 1}, loss: {losses / len(dataloader):.4f}\")\n   \ntrainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)\n\nepoch: 1, loss: 662.5636\nepoch: 2, loss: 100.4235\nepoch: 3, loss: 93.8398\nepoch: 4, loss: 93.6000\nepoch: 5, loss: 94.0025\n\n\nNow our model has been trained, our parameters should be different than before:\n\nmodel.state_dict()\n\nOrderedDict([('linear.weight', tensor([[45.3594]])),\n             ('linear.bias', tensor([-0.7256]))])\n\n\nComparing to the sklearn model, we get a very similar answer:\n\npd.DataFrame({\"w0\": [sk_model.intercept_, model.state_dict()['linear.bias'].item()],\n              \"w1\": [sk_model.coef_[0], model.state_dict()['linear.weight'].item()]},\n             index=['sklearn', 'pytorch']).round(2)\n\n\n\n\n\n\n\n\nw0\nw1\n\n\n\n\nsklearn\n-0.77\n45.50\n\n\npytorch\n-1.22\n45.43\n\n\n\n\n\n\n\nWe got pretty close! We could do better by changing the number of epochs or the learning rate. So here is our simple network once again:\n\nBy the way, check out what happens if we run trainer() again:\n\ntrainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)\n\nepoch: 1, loss: 93.5937\nepoch: 2, loss: 93.5482\nepoch: 3, loss: 93.7983\nepoch: 4, loss: 93.7556\nepoch: 5, loss: 93.9157\n\n\nOur model continues where we left off! This may or may not be what you want. We can start from scratch by re-making our model and optimizer.\n\nsummary(model, [1, 1])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nlinearRegression                         [1, 1]                    --\n‚îú‚îÄLinear: 1-1                            [1, 1]                    2\n==========================================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n\n\n\nprint(model)\n\nlinearRegression(\n  (linear): Linear(in_features=1, out_features=1, bias=True)\n)\n\n\n\nprint(summary(model, input_size=[50,1,1], verbose=0))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nlinearRegression                         [50, 1, 1]                --\n‚îú‚îÄLinear: 1-1                            [50, 1, 1]                2\n==========================================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n\n\n\n\n3.5.3 3.3. Multiple Linear Regression with a Neural Network\nOkay, let‚Äôs do a multiple linear regression now with 3 features. So our network will look like this:\n\nLet‚Äôs go ahead and create some data:\n\n# Create dataset\nX, y = make_regression(n_samples=500, n_features=3, random_state=0, noise=10.0)\nX_t = torch.tensor(X, dtype=torch.float32)\ny_t = torch.tensor(y, dtype=torch.float32)\n# Create dataloader\ndataset = TensorDataset(X_t, y_t)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nAnd let‚Äôs create the above model:\n\nmodel = linearRegression(input_size=3, output_size=1)\n\nWe should now have 4 parameters (3 weights and 1 bias):\n\nsummary(model, [BATCH_SIZE, 3])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nlinearRegression                         [50, 1]                   --\n‚îú‚îÄLinear: 1-1                            [50, 1]                   4\n==========================================================================================\nTotal params: 4\nTrainable params: 4\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n\n\nLooks good to me! Let‚Äôs train the model and then compare it to sklearn‚Äôs LinearRegression():\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\ntrainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)\n\nepoch: 1, loss: 1002.1801\nepoch: 2, loss: 109.6776\nepoch: 3, loss: 101.6614\nepoch: 4, loss: 102.0013\nepoch: 5, loss: 102.2314\n\n\n\nsk_model = LinearRegression().fit(X, y)\npd.DataFrame({\"w0\": [sk_model.intercept_, model.state_dict()['linear.bias'].item()],\n              \"w1\": [sk_model.coef_[0], model.state_dict()['linear.weight'][0, 0].item()],\n              \"w2\": [sk_model.coef_[1], model.state_dict()['linear.weight'][0, 1].item()],\n              \"w3\": [sk_model.coef_[2], model.state_dict()['linear.weight'][0, 2].item()]},\n             index=['sklearn', 'pytorch']).round(2)\n\n\n\n\n\n\n\n\nw0\nw1\nw2\nw3\n\n\n\n\nsklearn\n0.43\n0.62\n55.99\n11.14\n\n\npytorch\n0.57\n0.69\n55.77\n11.67\n\n\n\n\n\n\n\n\nmodel.state_dict()\n\nOrderedDict([('linear.weight', tensor([[ 0.6860, 55.7720, 11.6717]])),\n             ('linear.bias', tensor([0.5728]))])\n\n\n\n\n3.5.4 3.4. Non-linear Regression with a Neural Network\nOkay so we‚Äôve made simple networks to imitate simple and multiple linear regression. You‚Äôre probably thinking, so what? But we‚Äôre getting to the good stuff I promise! For example, what happens when we have more complicated datasets like this?\n\n# Create dataset\nnp.random.seed(2023)\nX = np.sort(np.random.randn(500))\ny = X ** 2 + 15 * np.sin(X) **3\nX_t = torch.tensor(X[:, None], dtype=torch.float32)\ny_t = torch.tensor(y, dtype=torch.float32)\n# Create dataloader\ndataset = TensorDataset(X_t, y_t)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\nplot_regression(X, y, y_range=[-25, 25], dy=5)\n\n\n                                                \n\n\nThis is obviously non-linear, and we need to introduce some non-linearities into our network. These non-linearities are what make neural networks so powerful and they are called ‚Äúactivation functions‚Äù. We are going to create a new model class that includes a non-linearity - a sigmoid function:\n\\[S(X)=\\frac{1}{1+e^{-x}}\\]\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nxs = np.linspace(-15, 15, 100)\nplot_regression(xs, [0], sigmoid(xs), x_range=[-5, 5], y_range=[0, 1], dy=0.2)\n\n\n                                                \n\n\nWe‚Äôll talk more about activation functions later, but note how the sigmoid function non-linearly maps x to a value between 0 and 1. Okay, so let‚Äôs create the following network:\n\nAll this means is that the value of each node in the hidden layer will be transformed by the ‚Äúactivation function‚Äù, thus introducing non-linear elements to our model! There‚Äôs two main ways of creating the above model in PyTorch, I‚Äôll show you both:\n\nclass nonlinRegression(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.hidden = nn.Linear(input_size, hidden_size)\n        self.output = nn.Linear(hidden_size, output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.hidden(x)       # input -&gt; hidden layer\n        x = self.sigmoid(x)      # sigmoid activation function in hidden layer\n        x = self.output(x)       # hidden -&gt; output layer\n        return x\n\nNote how our forward() method now passes x through the nn.Sigmoid() function after the hidden layer. The above method is very clear and flexible, but I prefer using nn.Sequential() to combine my layers together in the constructor:\n\nclass nonlinRegression(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n            nn.Linear(input_size, hidden_size),  # input -&gt; hidden layer\n            nn.Sigmoid(),                        # sigmoid activation function in hidden layer\n            nn.Linear(hidden_size, output_size)  # hidden -&gt; output layer\n        )\n\n    def forward(self, x):\n        x = self.main(x)\n        return x\n\nLet‚Äôs make an instance of our new class and confirm it has 10 parameters (6 weights + 4 biases):\n\nmodel = nonlinRegression(1, 3, 1)\nprint(summary(model, (BATCH_SIZE,1)))\nsummary(model, (BATCH_SIZE,1), verbose = 2);\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nnonlinRegression                         [50, 1]                   --\n‚îú‚îÄSequential: 1-1                        [50, 1]                   --\n‚îÇ    ‚îî‚îÄLinear: 2-1                       [50, 3]                   6\n‚îÇ    ‚îî‚îÄSigmoid: 2-2                      [50, 3]                   --\n‚îÇ    ‚îî‚îÄLinear: 2-3                       [50, 1]                   4\n==========================================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nnonlinRegression                         [50, 1]                   --\n‚îú‚îÄSequential: 1-1                        [50, 1]                   --\n‚îÇ    ‚îî‚îÄ0.weight                                                    ‚îú‚îÄ3\n‚îÇ    ‚îî‚îÄ0.bias                                                      ‚îú‚îÄ3\n‚îÇ    ‚îî‚îÄ2.weight                                                    ‚îú‚îÄ3\n‚îÇ    ‚îî‚îÄ2.bias                                                      ‚îî‚îÄ1\n‚îÇ    ‚îî‚îÄLinear: 2-1                       [50, 3]                   6\n‚îÇ    ‚îÇ    ‚îî‚îÄweight                                                 ‚îú‚îÄ3\n‚îÇ    ‚îÇ    ‚îî‚îÄbias                                                   ‚îî‚îÄ3\n‚îÇ    ‚îî‚îÄSigmoid: 2-2                      [50, 3]                   --\n‚îÇ    ‚îî‚îÄLinear: 2-3                       [50, 1]                   4\n‚îÇ    ‚îÇ    ‚îî‚îÄweight                                                 ‚îú‚îÄ3\n‚îÇ    ‚îÇ    ‚îî‚îÄbias                                                   ‚îî‚îÄ1\n==========================================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n\n\nOkay, let‚Äôs train:\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.3)\ntrainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)\n\nepoch: 1, loss: 38.2799\nepoch: 2, loss: 14.7747\nepoch: 3, loss: 10.0422\nepoch: 4, loss: 9.1278\nepoch: 5, loss: 5.9785\n\n\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.3)\ntrainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)\n\nepoch: 1, loss: 5.9213\nepoch: 2, loss: 3.0709\nepoch: 3, loss: 1.5621\nepoch: 4, loss: 1.3159\nepoch: 5, loss: 1.5005\n\n\n\ny_p = model(X_t).detach().numpy().squeeze()\nplot_regression(X, y, y_p, y_range=[-25, 25], dy=5)\n\n\n                                                \n\n\nTake a look at those non-linear predictions! Cool! Our model is not great and we could make it better soon by adjusting the learning rate, the number of nodes, and the number of epochs. But I really want you to see how each of our hidden nodes is ‚Äúengineering a non-linear feature‚Äù to be used for the predictions, check it out:\n\nplot_nodes(X, y_p, model)\n\n\n                                                \n\n\n\n\n3.5.5 3.5. Deep Learning\nYou‚Äôve probably heard the magic term ‚Äúdeep learning‚Äù and you‚Äôre about to find out what it means! Really, it‚Äôs just a neural network with more than 1 hidden layer! Easy!\nI like to think of each layer in a neural network as a ‚Äúfeature engineerer‚Äù, it is trying to extract the maximum amount of information from the layer before it. There are arguments of ‚Äúdeep network‚Äù (many layers) vs ‚Äúwide network‚Äù (many nodes), but for big datasets, ‚Äúdeep networks‚Äù have been shown to be very effective in practice\nLet‚Äôs create a ‚Äúdeep‚Äù network of 2 layers:\n\n\nclass deepRegression(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Linear(input_size, hidden_size_1),\n            nn.Sigmoid(),\n            nn.Linear(hidden_size_1, hidden_size_2),\n            nn.Sigmoid(),\n            nn.Linear(hidden_size_2, output_size)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n\n\nmodel = deepRegression(1, 5, 3, 1)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.3)\ntrainer(model, criterion, optimizer, dataloader, epochs=20, verbose=False)\nplot_regression(X, y, model(X_t).detach(), y_range=[-25, 25], dy=5)\n\n\n                                                \n\n\nAbove, note that just because the network is deeper, doesn‚Äôt always mean it‚Äôs better!"
  },
  {
    "objectID": "chapter3_pytorch-neural-networks-pt1.html#activation-functions",
    "href": "chapter3_pytorch-neural-networks-pt1.html#activation-functions",
    "title": "3¬† Chapter 3: Introduction to Pytorch & Neural Networks",
    "section": "3.6 4. Activation Functions",
    "text": "3.6 4. Activation Functions\n\nAs we learned above, activation functions are what allow us to model complex, non-linear functions. There are many different activations functions:\n\nfunctions = [torch.nn.Sigmoid, torch.nn.Tanh, torch.nn.Softplus, torch.nn.ReLU, torch.nn.LeakyReLU, torch.nn.SELU]\nplot_activations(torch.linspace(-6, 6, 100), functions)\n\n\n                                                \n\n\nActivation functions should be non-linear and tend to be monotonic and continuously differentiable (smooth). But as you can see with the ReLU function above, that‚Äôs not always the case!\nI wanted to point this out because it highlights how much of an art deep learning really is. Here‚Äôs a great quote from Yoshua Bengio (famous for his work in AI and deep learning) on his group experimenting with ReLU:\n\n‚Äú‚Ä¶one of the biggest mistakes I made was to think, like everyone else in the 90s, that you needed smooth non-linearities in order for backpropagation to work. because I thought that if we had something like rectifying nonlinearities, where you have a flat part, that it would be really hard to train, because the derivative would be zero in so many places. And when we started experimenting with ReLU, with deep nets around 2010, I was obsessed with the idea that, we should be careful about whether neurons won‚Äôt saturate too much on the zero part. But in the end, it turned out that, actually, the ReLU was working a lot better than the sigmoids and tanh, and that was a big surprise‚Ä¶it turned out to work better, whereas I thought it would be harder to train!‚Äù\n\nAnyway, ReLU is probably the most popular these days, but you can treat activation functions as hyperparameters that need to be optimized in your workflow."
  },
  {
    "objectID": "chapter3_pytorch-neural-networks-pt1.html#neural-network-classification",
    "href": "chapter3_pytorch-neural-networks-pt1.html#neural-network-classification",
    "title": "3¬† Chapter 3: Introduction to Pytorch & Neural Networks",
    "section": "3.7 5. Neural Network Classification",
    "text": "3.7 5. Neural Network Classification\n\n\n3.7.1 5.1. Binary Classification\nThis will actually be the easiest part of the chapter. Up until now, we‚Äôve been looking at developing networks for regression tasks, but what if we want to do binary classification? Well, what do we do in Logistic Regression? We just pass the output of a regression model into the Sigmoid function to get a value between 0 and 1 (a probability of an observation belonging to the positive class) - we‚Äôll do the same thing here!\nLet‚Äôs create a toy dataset:\n\nX, y = make_circles(n_samples=300, factor=0.5, noise=0.1, random_state=2020)\nX_t = torch.tensor(X, dtype=torch.float32)\ny_t = torch.tensor(y, dtype=torch.float32)\n# Create dataloader\ndataset = TensorDataset(X_t, y_t)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\nplot_classification_2d(X, y)\n\n\n                                                \n\n\nLet‚Äôs create this network to model that dataset:\n\nI‚Äôm going to start using ReLU as our activation function(s) and Adam as our optimizer because these are what are currently, commonly used in practice. We are doing classification now so we‚Äôll need to use log loss (binary cross entropy) as our loss function:\n\\[f(w) = \\sum_{x,y \\in D} -y log(\\hat{y}) - (1-y)log(1-\\hat{y})\\]\nIn PyTorch, the binary cross entropy loss criterion is torch.nn.BCELoss. The formula expects a ‚Äúprobability‚Äù which is why we add a Sigmoid function to the end of out network.\n\nclass binaryClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, output_size),\n            nn.Sigmoid()  # &lt;-- this will squash our output to a probability between 0 and 1\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n\nBUT WAIT!\nWhile we can do the above and then train with a torch.nn.BCELoss loss function, there‚Äôs a better way! We can omit the Sigmoid function and just use torch.nn.BCEWithLogitsLoss (which combines a Sigmoid layer and the BCELoss). Why would we do this? It‚Äôs numerically stable! From the docs:\n\n‚ÄúThis version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.‚Äù\n\nSo with that said, here‚Äôs our model (no Sigmoid layer at the end because it‚Äôs included in the loss function we‚Äôll use):\n\nclass binaryClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, output_size)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n\n\noptimizer?\n\n\nType:        SGD\nString form:\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.3\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\nFile:        ~/anaconda3/envs/dl-pytorch/lib/python3.10/site-packages/torch/optim/sgd.py\nDocstring:  \nImplements stochastic gradient descent (optionally with momentum).\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n        \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n        &\\hspace{10mm}\\textbf{if} \\: t &gt; 1                                                   \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n        &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n        &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                             \\\\\n        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n        &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\nNesterov momentum is based on the formula from\n`On the importance of initialization and momentum in deep learning`__.\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float): learning rate\n    momentum (float, optional): momentum factor (default: 0)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    dampening (float, optional): dampening for momentum (default: 0)\n    nesterov (bool, optional): enables Nesterov momentum (default: False)\n    maximize (bool, optional): maximize the params based on the objective, instead of\n        minimizing (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used (default: None)\nExample:\n    &gt;&gt;&gt; # xdoctest: +SKIP\n    &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    &gt;&gt;&gt; optimizer.zero_grad()\n    &gt;&gt;&gt; loss_fn(model(input), target).backward()\n    &gt;&gt;&gt; optimizer.step()\n__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n.. note::\n    The implementation of SGD with Momentum/Nesterov subtly differs from\n    Sutskever et. al. and implementations in some other frameworks.\n    Considering the specific case of Momentum, the update can be written as\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n        \\end{aligned}\n    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n    parameters, gradient, velocity, and momentum respectively.\n    This is in contrast to Sutskever et. al. and\n    other frameworks which employ an update of the form\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - v_{t+1}.\n        \\end{aligned}\n    The Nesterov version is analogously modified.\n\n\n\nLet‚Äôs train the model:\n\nmodel.parameters\n\n&lt;bound method Module.parameters of deepRegression(\n  (main): Sequential(\n    (0): Linear(in_features=1, out_features=5, bias=True)\n    (1): Sigmoid()\n    (2): Linear(in_features=5, out_features=3, bias=True)\n    (3): Sigmoid()\n    (4): Linear(in_features=3, out_features=1, bias=True)\n  )\n)&gt;\n\n\n\nmodel = binaryClassifier(2, 5, 1)\ncriterion = torch.nn.BCEWithLogitsLoss() # loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  # optimization algorithm\ntrainer(model, criterion, optimizer, dataloader, epochs=20, verbose=True)\n\nepoch: 1, loss: 0.6767\nepoch: 2, loss: 0.5993\nepoch: 3, loss: 0.5116\nepoch: 4, loss: 0.4355\nepoch: 5, loss: 0.3593\nepoch: 6, loss: 0.3083\nepoch: 7, loss: 0.2705\nepoch: 8, loss: 0.2489\nepoch: 9, loss: 0.2314\nepoch: 10, loss: 0.2058\nepoch: 11, loss: 0.2004\nepoch: 12, loss: 0.1929\nepoch: 13, loss: 0.1869\nepoch: 14, loss: 0.1790\nepoch: 15, loss: 0.1795\nepoch: 16, loss: 0.1794\nepoch: 17, loss: 0.1688\nepoch: 18, loss: 0.1601\nepoch: 19, loss: 0.1554\nepoch: 20, loss: 0.1467\n\n\n\nmodel()\n\nbinaryClassifier(\n  (main): Sequential(\n    (0): Linear(in_features=2, out_features=5, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=5, out_features=1, bias=True)\n  )\n)\n\n\n\nplot_classification_2d(X, y, model)\n\n\n                                                \n\n\nTo be clear, our model is just outputting some number between -‚àû and +‚àû (we aren‚Äôt applying Sigmoid in the model), so: - To get the probabilities we would need to pass them through a Sigmoid; - To get classes, we can apply some threshold (usually 0.5) to this probability.\nFor example, we would expect the point (0,0) to have a high probability and the point (-1,-1) to have a low probability:\n\nprediction = model(torch.tensor([[0, 0], [-1, -1]], dtype=torch.float32)).detach()\nprint(prediction)\n\ntensor([[ 5.3439],\n        [-7.6239]])\n\n\n\nprobability = nn.Sigmoid()(prediction)\nprint(probability)\n\ntensor([[9.9525e-01],\n        [4.8839e-04]])\n\n\n\nclasses = np.where(probability &gt; 0.5, 1, 0)\nprint(classes)\n\n[[1]\n [0]]\n\n\n\n\n3.7.2 5.2. Multiclass Classification (Optional)\nFor multiclass classification, we‚Äôll need the softmax function:\n\\[\\sigma(\\vec{z})_i=\\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}}\\]\nIt basically outputs probabilities for each class we wish to predict, and they all sum to 1. If you‚Äôre not familiar with the different loss functions we‚Äôre using in this chapter, I highly recommend you read through this excellent blog post. torch.nn.CrossEntropyLoss is a loss that combines a softmax with cross entropy loss. Let‚Äôs try a 4-class classification problem using the following network:\n\n\nclass multiClassifier(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n            torch.nn.Linear(input_size, hidden_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_size, output_size)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n\n\nX, y = make_blobs(n_samples=200, centers=4, center_box=(-1.2, 1.2), cluster_std=[0.15, 0.15, 0.15, 0.15], random_state=12345)\nX_t = torch.tensor(X, dtype=torch.float32)\ny_t = torch.tensor(y, dtype=torch.int64)\n# Create dataloader\ndataset = TensorDataset(X_t, y_t)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\nplot_classification_2d(X, y)\n\n\n                                                \n\n\nLet‚Äôs train this model:\n\nBATCH_SIZE\n\n50\n\n\n\nmodel = multiClassifier(2, 5, 4)\n\nprint(summary(model, (BATCH_SIZE,2)))\nsummary(model, (BATCH_SIZE,2), verbose=2);\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nmultiClassifier                          [50, 4]                   --\n‚îú‚îÄSequential: 1-1                        [50, 4]                   --\n‚îÇ    ‚îî‚îÄLinear: 2-1                       [50, 5]                   15\n‚îÇ    ‚îî‚îÄReLU: 2-2                         [50, 5]                   --\n‚îÇ    ‚îî‚îÄLinear: 2-3                       [50, 4]                   24\n==========================================================================================\nTotal params: 39\nTrainable params: 39\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nmultiClassifier                          [50, 4]                   --\n‚îú‚îÄSequential: 1-1                        [50, 4]                   --\n‚îÇ    ‚îî‚îÄ0.weight                                                    ‚îú‚îÄ10\n‚îÇ    ‚îî‚îÄ0.bias                                                      ‚îú‚îÄ5\n‚îÇ    ‚îî‚îÄ2.weight                                                    ‚îú‚îÄ20\n‚îÇ    ‚îî‚îÄ2.bias                                                      ‚îî‚îÄ4\n‚îÇ    ‚îî‚îÄLinear: 2-1                       [50, 5]                   15\n‚îÇ    ‚îÇ    ‚îî‚îÄweight                                                 ‚îú‚îÄ10\n‚îÇ    ‚îÇ    ‚îî‚îÄbias                                                   ‚îî‚îÄ5\n‚îÇ    ‚îî‚îÄReLU: 2-2                         [50, 5]                   --\n‚îÇ    ‚îî‚îÄLinear: 2-3                       [50, 4]                   24\n‚îÇ    ‚îÇ    ‚îî‚îÄweight                                                 ‚îú‚îÄ20\n‚îÇ    ‚îÇ    ‚îî‚îÄbias                                                   ‚îî‚îÄ4\n==========================================================================================\nTotal params: 39\nTrainable params: 39\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n\n\n\ncriterion = torch.nn.CrossEntropyLoss() # loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=0.2)  # optimization algorithm\n\nfor epoch in range(10):\n    losses = 0\n    for X_batch, y_batch in dataloader:\n        optimizer.zero_grad()       # Clear gradients w.r.t. parameters\n        y_hat = model(X_batch)            # Forward pass to get output\n        loss = criterion(y_hat, y_batch)  # Calculate loss\n        loss.backward()             # Getting gradients w.r.t. parameters\n        optimizer.step()            # Update parameters\n        losses += loss.item()       # Add loss for this batch to running total\n    print(f\"epoch: {epoch + 1}, loss: {losses / len(dataloader):.4f}\")\n\nepoch: 1, loss: 1.2293\nepoch: 2, loss: 0.6483\nepoch: 3, loss: 0.2293\nepoch: 4, loss: 0.0686\nepoch: 5, loss: 0.0175\nepoch: 6, loss: 0.0074\nepoch: 7, loss: 0.0031\nepoch: 8, loss: 0.0019\nepoch: 9, loss: 0.0013\nepoch: 10, loss: 0.0010\n\n\n\nplot_classification_2d(X, y, model, transform=\"Softmax\")\n\n\n                                                \n\n\nTo be clear once again, our model is just outputting some number between -‚àû and +‚àû, so: - To get the probabilities we would need to pass them to a Softmax; - To get classes, we need to select the largest probability.\nFor example, we would expect the point (-1,-1) to have a high probability of belonging to class 1, and the point (0,0) to have the highest probability of belonging to class 2.\n\nprediction = model(torch.tensor([[-1, -1], [1,1]], dtype=torch.float32)).detach()\nprint(prediction)\n\ntensor([[-24.7038,  19.5978,  -0.9319, -36.8003],\n        [-13.1977, -32.6612,   8.0046,  18.5433]])\n\n\nNote how we get 4 predictions per data point (a prediction for each of the 4 classes):\n\nprobability = nn.Softmax(dim=1)(prediction)\nprint(probability)\n\ntensor([[5.7548e-20, 1.0000e+00, 1.2136e-09, 3.2109e-25],\n        [1.6408e-14, 5.7832e-23, 2.6491e-05, 9.9997e-01]])\n\n\nThe predictions should now sum to 1:\n\nprobability.sum(dim=1)\n\ntensor([1., 1.])\n\n\nWe can get the class with maximum probability using argmax():\n\nclasses = probability.argmax(dim=1)\nprint(classes)\n\ntensor([1, 3])"
  },
  {
    "objectID": "chapter4_neural-networks-pt2.html#chapter-learning-objectives",
    "href": "chapter4_neural-networks-pt2.html#chapter-learning-objectives",
    "title": "4¬† Chapter 4: Training Neural Networks",
    "section": "4.1 Chapter Learning Objectives",
    "text": "4.1 Chapter Learning Objectives\n\n\nExplain how backpropagation works at a high level.\nDescribe the difference between training loss and validation loss when creating a neural network.\nIdentify and describe common techniques to avoid overfitting/apply regularization to neural networks, e.g., early stopping, drop out, L2 regularization.\nUse PyTorch to develop a fully-connected neural network and training pipeline."
  },
  {
    "objectID": "chapter4_neural-networks-pt2.html#imports",
    "href": "chapter4_neural-networks-pt2.html#imports",
    "title": "4¬† Chapter 4: Training Neural Networks",
    "section": "4.2 Imports",
    "text": "4.2 Imports\n\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torchvision import transforms, datasets, utils\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom utils.plotting import *"
  },
  {
    "objectID": "chapter4_neural-networks-pt2.html#differentiation-backpropagation-autograd",
    "href": "chapter4_neural-networks-pt2.html#differentiation-backpropagation-autograd",
    "title": "4¬† Chapter 4: Training Neural Networks",
    "section": "4.3 1. Differentiation, Backpropagation, Autograd",
    "text": "4.3 1. Differentiation, Backpropagation, Autograd\n\nIn previous chapters we‚Äôve discussed optimization algorithms like gradient descent, stochastic gradient descent, ADAM, etc. These algorithms need the gradient of the loss function w.r.t the model parameters to optimize the parameters:\n\\[\\nabla \\mathscr{L}(\\mathbf{w}) = \\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1} \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_d} \\end{bmatrix}\\]\nWe‚Äôve been able to calculate the gradient by hand for things like linear regression and logistic regression. But how would you work out the gradient for this very simple network for regression:\n\nThe equation for calculating the output of that network is below, it‚Äôs the linear layers and activation functions (Sigmoid in this case) recursively stuck together:\n\\[S(x)=\\frac{1}{1+e^{-x}}\\]\n\\[\\hat{y}=w_3S(w_1x+b_1) + w_4S(w_2x+b_2) + b_3\\]\nSo how would we calculate the gradient of say the MSE loss w.r.t to all our parameters?\n\\[\\mathscr{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1}(y_i-\\hat{y_i})^2\\]\n\\[\\nabla \\mathscr{L}(\\mathbf{w}) = \\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1} \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_d} \\end{bmatrix}\\]\nWe have 3 options: 1. Symbolic differentiation: i.e., ‚Äúdo it by hand‚Äù like we learned in calculus. 2. Numerical differentiation: for example, approximating the derivative using finite differences \\(\\frac{df(x)}{dx} \\approx \\frac{f(x+h)-f(x)}{h}\\). 3. Automatic differentiation: the ‚Äúbest of both worlds‚Äù.\nWe‚Äôll be looking at option 3 Automatic Differentiation (AD) here, as we use a particular flavour of AD called ‚Äúbackpropagation‚Äù to train neural networks. But if you‚Äôre interested in learning more about the other methods, see Appendix C: Computing Derivatives.\n\n4.3.1 1.1. Backpropagation\nBackpropagation is the algorithm we use to compute the gradients needed to train the parameters of a neural network. In backpropagation, the main idea is to decompose our network into smaller operations with simple, codeable derivatives. We then combine all these smaller operations together with the chain rule. The term ‚Äúbackpropagation‚Äù stems from the fact that we start at the end of our network and then propagate backwards. I‚Äôm going to go through a short example based on this network:\n\nLet‚Äôs decompose that into smaller operations. I‚Äôve introduced some new variables to hold intermediate states \\(z_i\\) (node output before activation) and \\(a_i\\) (node output after activation). I‚Äôll also feed in one sample data point (x, y) = (1, 3) and am showing intermediate outputs in green and the final loss in red. This is called the ‚Äúforward pass‚Äù step - where I feed in data and calculate outputs from left to right:\n\nNow let‚Äôs zoom in to the outpout node and calculate the gradients for just the parameters connected to that node. It looks complicated but the derivatives are very simple - take some time to examine this figure and you‚Äôll see!\n\nThat all boils down to this:\n\nNow, the beauty of backpropagation is that we can use these results to easily calculate derivatives earlier in the network using the chain rule. I‚Äôll do that for \\(b_1\\) and \\(b_2\\) below. Once again, it looks complicated, but we‚Äôre simply combining a bunch of small, simple derivatives with the chain rule:\n\nI‚Äôve left calculating the gradients of \\(w_1\\) and \\(w_2\\) up to you. All the gradients for the network boil down to this:\n\nSo summarising the process: 1. We ‚Äúforward pass‚Äù some data through our network 2. We ‚Äúbackpropagate‚Äù the error through the network to calculate gradients\nLuckily, you‚Äôll never do this by hand again, because torch.autograd does all this for us!\n\n\n4.3.2 1.2. Autograd\ntorch.autograd is PyTorch‚Äôs automatic differentiation engine which helps us implement backpropagation. In plain English: torch.autograd automatically calculates and stores derivatives for your network. Consider our simple network above:\n\n\nclass network(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.hidden = torch.nn.Linear(input_size, hidden_size)\n        self.output = torch.nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.hidden(x)\n        x = torch.sigmoid(x)\n        x = self.output(x)\n        return x\n\n\nmodel = network(1, 2, 1)  # make an instance of our network\nmodel.state_dict()['hidden.weight'][:] = torch.tensor([[1], [-1]])  # fix the weights manually based on the earlier figure\nmodel.state_dict()['hidden.bias'][:] = torch.tensor([1, 2])\nmodel.state_dict()['output.weight'][:] = torch.tensor([[1, 2]])\nmodel.state_dict()['output.bias'][:] = torch.tensor([-1])\nx, y = torch.tensor([1.0]), torch.tensor([3.0])  # our x, y data\n\nNow let‚Äôs check the gradient of the bias of the output node:\n\nprint(model.output.bias.grad)\n\nNone\n\n\nIt‚Äôs currently None!\nPyTorch is tracking the operations in our network and how to calculate the gradient (more on that a bit later), but it hasn‚Äôt calculated anything yet because we don‚Äôt have a loss function and we haven‚Äôt done a forward pass to calculate the loss so there‚Äôs nothing to backpropagate yet!\nLet‚Äôs define a loss now:\n\ncriterion = torch.nn.MSELoss()\n\nNow we can force Pytorch to ‚Äúbackpropagate‚Äù the errors, like we just did by hand earlier by: 1. Doing a ‚Äúforward pass‚Äù of our (x, y) data and calculating the loss; 2. ‚ÄúBackpropagating‚Äù the loss by calling loss.backward()\n\nloss = criterion(model(x), y)\nloss.backward()  # backpropagates the error to calculate gradients!\n\nNow let‚Äôs check the gradient of the bias of the output node (\\(\\frac{\\partial \\mathscr{L}}{\\partial b_3}\\)):\n\nprint(model.output.bias.grad)\n\ntensor([-3.3142])\n\n\nIt matches what we calculated earlier!\n\nThat is just so fantastic! In fact, we can make sure that all our gradients match what we calculated by hand:\n\nprint(\"Hidden Layer Gradients\")\nprint(\"Bias:\", model.hidden.bias.grad)\nprint(\"Weights:\", model.hidden.weight.grad.squeeze())\nprint()\nprint(\"Output Layer Gradients\")\nprint(\"Bias:\", model.output.bias.grad)\nprint(\"Weights:\", model.output.weight.grad.squeeze())\n\nHidden Layer Gradients\nBias: tensor([-0.3480, -1.3032])\nWeights: tensor([-0.3480, -1.3032])\n\nOutput Layer Gradients\nBias: tensor([-3.3142])\nWeights: tensor([-2.9191, -2.4229])\n\n\nNow that we have the gradients, what‚Äôs the next step? We use our optimization algorithm to update our weights! These are our current weights:\n\nmodel.state_dict()\n\nOrderedDict([('hidden.weight',\n              tensor([[ 1.],\n                      [-1.]])),\n             ('hidden.bias', tensor([1., 2.])),\n             ('output.weight', tensor([[1., 2.]])),\n             ('output.bias', tensor([-1.]))])\n\n\nTo optimize them, we: 1. Define an optimizer; 2. Ask it to update our weights based on our gradients using optimizer.step().\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\noptimizer.step()\n\nOur weights should now be different:\n\nmodel.state_dict()\n\nOrderedDict([('hidden.weight',\n              tensor([[ 1.0348],\n                      [-0.8697]])),\n             ('hidden.bias', tensor([1.0348, 2.1303])),\n             ('output.weight', tensor([[1.2919, 2.2423]])),\n             ('output.bias', tensor([-0.6686]))])\n\n\nAmazing!\nOne last thing for you to know: Pytorch does not automatically clear the gradients after using them. So if I call loss.backward() again, my gradients accumulate:\n\noptimizer.zero_grad()  # &lt;- I'll explain this in the next cell\nfor _ in range(1, 6):\n    loss = criterion(model(x), y)\n    loss.backward()\n    print(f\"b3 gradient after call {_} of loss.backward():\", model.hidden.bias.grad)\n\nb3 gradient after call 1 of loss.backward(): tensor([-0.1991, -0.5976])\nb3 gradient after call 2 of loss.backward(): tensor([-0.3983, -1.1953])\nb3 gradient after call 3 of loss.backward(): tensor([-0.5974, -1.7929])\nb3 gradient after call 4 of loss.backward(): tensor([-0.7966, -2.3906])\nb3 gradient after call 5 of loss.backward(): tensor([-0.9957, -2.9882])\n\n\nOur gradients are accumulating each time we call loss.backward()! So we need to tell Pytorch to ‚Äúzero the gradients‚Äù each iteration using optimizer.zero_grad():\n\nfor _ in range(1, 6):\n    optimizer.zero_grad()  # &lt;- don't forget this!!!\n    loss = criterion(model(x), y)\n    loss.backward()\n    print(f\"b3 gradient after call {_} of loss.backward():\", model.hidden.bias.grad)\n\nb3 gradient after call 1 of loss.backward(): tensor([-0.1991, -0.5976])\nb3 gradient after call 2 of loss.backward(): tensor([-0.1991, -0.5976])\nb3 gradient after call 3 of loss.backward(): tensor([-0.1991, -0.5976])\nb3 gradient after call 4 of loss.backward(): tensor([-0.1991, -0.5976])\nb3 gradient after call 5 of loss.backward(): tensor([-0.1991, -0.5976])\n\n\n\nNote: you might wonder why PyTorch behaves like this. Well, there are some cases we might want to accumulate the gradient. For example, if we want to calculate the gradients over several batches before updating our weights. But don‚Äôt worry about that for now - most of the time, you‚Äôll want to be ‚Äúzeroing out‚Äù the gradients each iteration.\n\n\n\n4.3.3 1.3. Computational Graph (Optional)\nPyTorch‚Äôs autograd basically keeps a record of our data and network operations in a computational graph. That‚Äôs beyond the scope of this chapter, but if you‚Äôre interested in learning more, I recommend this excellent video. Also, torchviz is a useful package to look at the ‚Äúcomputational graph‚Äù PyTorch is building for us under the hood:\n\nfrom torchviz import make_dot\nmake_dot(model(torch.rand(1, 1)))"
  },
  {
    "objectID": "chapter4_neural-networks-pt2.html#training-neural-networks",
    "href": "chapter4_neural-networks-pt2.html#training-neural-networks",
    "title": "4¬† Chapter 4: Training Neural Networks",
    "section": "4.4 2. Training Neural Networks",
    "text": "4.4 2. Training Neural Networks\n\nThe big takeaway from the last section is that PyTorch‚Äôs autograd takes care of the gradients for us. We just need to put all the pieces together properly. Remember the below trainer() function I used last chapter to train my network. Now we know what all this means!\n\ndef trainer(model, criterion, optimizer, dataloader, epochs=5):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n    \n    train_loss = []\n    for epoch in range(epochs):  # for each epoch\n        losses = 0\n        for X, y in dataloader:  # for each batch\n            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss based on output\n            loss.backward()             # Calculate gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n            losses += loss.item()       # Add loss for this batch to running total\n        train_loss.append(losses / len(dataloader))  # loss = total loss in epoch / number of batches = loss per batch\n    return train_loss\n\nNotice how I calculate the loss for each epoch by summing up the loss for each batch in that epoch? I then divide the loss per epoch by total number of batches to get the average loss per batch in an epoch (I store that loss in train_loss).\n\nDividing by the number of batches ‚Äúdecouples‚Äù our loss from the batch size. So if I run another experiment with a different batch size, I‚Äôll still be able to compare losses for that experiment with this one. We‚Äôll explore this concept more later.\n\nIf our model is being trained correctly, our loss should go down over time. Let‚Äôs try it out with some sample data:\n\n# Create dataset\ntorch.manual_seed(0)\nX = torch.arange(-3, 3, 0.15)\ny = X ** 2 + X * torch.normal(0, 1, (40,))\ndataloader = DataLoader(TensorDataset(X[:, None], y), batch_size=1, shuffle=True)\nplot_regression(X, y, y_range=[-1, 10], dy=1)\n\n\n                                                \n\n\n\nmodel = network(1, 3, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), 0.1)\ntrain_loss = trainer(model, criterion, optimizer, dataloader, epochs=101)\nplot_regression(X, y, model(X[:, None]).detach(), y_range=[-1, 10], dy=1)\n\n\n                                                \n\n\nThe model looks like a good fit, so presumably the loss went down as epochs progressed, let‚Äôs take a look:\n\nplot_loss(train_loss)\n\n\n                                                \n\n\n\n4.4.1 2.1. Validation Loss\nWe‚Äôve been focussing on training loss so far, but as we know, we need to validate our model on new ‚Äúunseen‚Äù data! For this, we‚Äôll need some validation data, I‚Äôm going to split our dataset in half to create a trainloader and a validloader:\n\n# Create dataset\ntorch.manual_seed(0)\nX_valid = torch.arange(-3.0, 3.0)\ny_valid = X_valid ** 2\ntrainloader = DataLoader(TensorDataset(X, y), batch_size=1, shuffle=True)\nvalidloader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=1, shuffle=True)\n\nNow the wonderful thing about PyTorch is that you are in full control - you can do whatever you want! So here, after each epoch, I‚Äôm going to record the validation loss by looping over my validation batches, it‚Äôs just a little extra module I add to my training function:\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n    \n    train_loss = []\n    valid_loss = []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        valid_batch_loss = 0\n        \n        # Training\n        model.train()  # This puts the model in \"training mode\", this is the default mode.\n                       # We'll use a different mode, \"evaluation mode\", for validation.\n        for X, y in trainloader:\n            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss based on output\n            loss.backward()             # Calculate gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n        train_loss.append(train_batch_loss / len(trainloader))  # loss = total loss in epoch / number of batches = loss per batch\n        \n        # Validation\n        model.train()  # This puts the model in \"evaluation mode\". It's important to do this when our model\n                       # includes some randomness like dropout layers which we'll see later. It turns off \n                       # this randomness for validation purposes.\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n            for X_valid, y_valid in validloader:\n                y_hat = model(X_valid).flatten()  # Forward pass to get output\n                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n                valid_batch_loss += loss.item()   # Add loss for this batch to running total\n                \n            \n        valid_loss.append(valid_batch_loss / len(validloader))\n    return train_loss, valid_loss\n\n\nmodel = network(1, 6, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\ntrain_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201)\nplot_loss(train_loss, valid_loss)\n\n\n                                                \n\n\nWhat do we see above? Well, we‚Äôre obviously overfitting (fitting to closely to the training data such that we do poorly on the validation data). We are optimizing too well! One way we could avoid overfitting is by terminating the training if our validation loss starts going up, this is called ‚Äúearly stopping‚Äù.\n\n\n4.4.2 2.2. Early stopping\nEarly stopping is a way of avoiding overfitting. As training progresses, if we notice the validation loss increasing (while the training loss decreases), that‚Äôs usually an indication of overfitting. The validation loss may go up and down from epoch to epoch, so usually we define a ‚Äúpatience‚Äù which is a number of consecutive epochs we‚Äôre willing to allow the validation loss to increase before we stop. Once again, the beauty of PyTorch is how easy it is to customize your network in this way:\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n    \n    train_loss = []\n    valid_loss = []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        valid_batch_loss = 0\n        \n        # Training\n        for X, y in trainloader:\n            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss based on output\n            loss.backward()             # Calculate gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n        train_loss.append(train_batch_loss / len(trainloader))  # loss = total loss in epoch / number of batches = loss per batch\n        \n        # Validation\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n            for X_valid, y_valid in validloader:\n                y_hat = model(X_valid).flatten()  # Forward pass to get output\n                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n                valid_batch_loss += loss.item()   # Add loss for this batch to running total\n            \n        valid_loss.append(valid_batch_loss / len(validloader))\n        \n        # Early stopping\n        if epoch &gt; 0 and valid_loss[-1] &gt; valid_loss[-2]:\n            consec_increases += 1\n        else:\n            consec_increases = 0\n        if consec_increases == patience:\n            print(f\"Stopped early at epoch {epoch + 1} - val loss increased for {consec_increases} consecutive epochs!\")\n            break\n        \n    return train_loss, valid_loss\n\n\nmodel = network(1, 6, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\ntrain_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201, patience=3)\nplot_loss(train_loss, valid_loss)\n\nStopped early at epoch 37 - val loss increased for 3 consecutive epochs!\n\n\n\n                                                \n\n\nThere are more advanced implementations of early stopping out there, but you get the idea!"
  },
  {
    "objectID": "chapter4_neural-networks-pt2.html#regularization",
    "href": "chapter4_neural-networks-pt2.html#regularization",
    "title": "4¬† Chapter 4: Training Neural Networks",
    "section": "4.5 3. Regularization",
    "text": "4.5 3. Regularization\n\nRecall that regularization is a technique to help avoid overfitting. There are many regularization techniques available in neural networks. I‚Äôll discuss the two main ones here: 1. Drop out 2. L2 regularization"
  },
  {
    "objectID": "chapter4_neural-networks-pt2.html#drop-out",
    "href": "chapter4_neural-networks-pt2.html#drop-out",
    "title": "4¬† Chapter 4: Training Neural Networks",
    "section": "4.6 3.1. Drop Out",
    "text": "4.6 3.1. Drop Out\nDrop out is a common regularization technique and is very simple. Basically, each iteration, we randomly chose some nodes in a layer and don‚Äôt update their weights (to do this we set the output of the nodes to 0). A simple example:\n\ndropout_layer = torch.nn.Dropout(p=0.5)  # 50% probability that a node will be set to 0 (\"dropped out\")\ninputs = torch.randn(5, 3)\ninputs\n\ntensor([[-0.9962,  0.1853,  1.2287],\n        [-0.6026,  1.1602, -0.0921],\n        [-2.1831,  1.0611, -1.5108],\n        [-0.0180,  0.5990,  1.2808],\n        [ 0.7645,  1.8589, -0.8471]])\n\n\n\ndropout_layer(inputs)\n\ntensor([[-0.0000,  0.0000,  2.4574],\n        [-0.0000,  2.3204, -0.1841],\n        [-4.3661,  2.1222, -3.0217],\n        [-0.0000,  0.0000,  2.5616],\n        [ 0.0000,  3.7178, -0.0000]])\n\n\nIn the above, note how about 50% of nodes have been given a value of 0!"
  },
  {
    "objectID": "chapter4_neural-networks-pt2.html#l2-regularization",
    "href": "chapter4_neural-networks-pt2.html#l2-regularization",
    "title": "4¬† Chapter 4: Training Neural Networks",
    "section": "4.7 3.2. L2 Regularization",
    "text": "4.7 3.2. L2 Regularization\nRecall that in L2 we had this penalty to the loss: \\(\\frac{\\lambda}{2}||w||^2\\). \\(\\lambda\\) is the regularization parameter. L2 regularization is called ‚Äúweight-decay‚Äù in PyTorch (because we are coercing the weights to be smaller I suppose). It‚Äôs an argument in most optimizers which you can specify:\n\ntorch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.5)  # here weight_decay is Œª in the above equation\n\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: False\n    lr: 0.1\n    maximize: False\n    weight_decay: 0.5\n)"
  },
  {
    "objectID": "chapter4_neural-networks-pt2.html#putting-it-all-together-with-bitmojis",
    "href": "chapter4_neural-networks-pt2.html#putting-it-all-together-with-bitmojis",
    "title": "4¬† Chapter 4: Training Neural Networks",
    "section": "4.8 4. Putting it all Together with Bitmojis",
    "text": "4.8 4. Putting it all Together with Bitmojis\n\nHere I thought we‚Äôd put everything we learned in this chapter together to predict some bitmojis. I have a folder of images with the following structure:\ndata\n‚îî‚îÄ‚îÄ bitmoji_bw\n    ‚îú‚îÄ‚îÄ train\n    ‚îÇ   ‚îú‚îÄ‚îÄ not_tom\n    ‚îÇ   ‚îî‚îÄ‚îÄ tom\n    ‚îî‚îÄ‚îÄ valid\n        ‚îú‚îÄ‚îÄ not_tom\n        ‚îî‚îÄ‚îÄ tom\n\nTRAIN_DIR = \"data/bitmoji_bw/train/\"\nVALID_DIR = \"data/bitmoji_bw/valid/\"\nIMAGE_SIZE = 50\nBATCH_SIZE = 32\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.Grayscale(num_output_channels=1),\n    transforms.ToTensor()\n])\n# Training data\ntrain_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n# Validation data\nvalid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=data_transforms)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n\nsample_batch = next(iter(train_loader))\nplot_bitmojis(sample_batch)\n\n\n\n\nNow to the network. I‚Äôm going to make a function linear_block() to help create my network and keep things DRY:\n\ndef linear_block(input_size, output_size):\n    return nn.Sequential(\n        nn.Linear(input_size, output_size),\n        nn.LeakyReLU(),\n        nn.Dropout(0.1)\n    )\n\nclass BitmojiClassifier(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.main = nn.Sequential(\n            linear_block(input_size, 256),\n            linear_block(256, 128),\n            linear_block(128, 64),\n            linear_block(64, 16),\n            nn.Linear(16, 1)\n        )\n        \n    def forward(self, x):\n        out = self.main(x)\n        return out\n\nNow the training function. This is getting long but it‚Äôs just all the bits we‚Äôve seen before!\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5, verbose=True):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n    \n    train_loss = []\n    valid_loss = []\n    train_accuracy = []\n    valid_accuracy = []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        train_batch_acc = 0\n        valid_batch_loss = 0\n        valid_batch_acc = 0\n        \n        # Training\n        for X, y in trainloader:\n            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n            y_hat = model(X.view(X.shape[0], -1)).flatten()  # Forward pass to get output\n            y_hat_labels = torch.sigmoid(y_hat) &gt; 0.5        # convert probabilities to False (0) and True (1)\n            loss = criterion(y_hat, y.type(torch.float32))   # Calculate loss based on output\n            loss.backward()             # Calculate gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n            train_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()   # Average accuracy for this batch\n        train_loss.append(train_batch_loss / len(trainloader))     # loss = total loss in epoch / number of batches = loss per batch\n        train_accuracy.append(train_batch_acc / len(trainloader))  # accuracy\n        \n        # Validation\n        model.eval()  # this turns off those random dropout layers, we don't want them for validation!\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n            for X, y in validloader:\n                y_hat = model(X.view(X.shape[0], -1)).flatten()  # Forward pass to get output\n                y_hat_labels = torch.sigmoid(y_hat) &gt; 0.5        # convert probabilities to False (0) and True (1)\n                loss = criterion(y_hat, y.type(torch.float32))   # Calculate loss based on output\n                valid_batch_loss += loss.item()                  # Add loss for this batch to running total\n                valid_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()   # Average accuracy for this batch  \n        valid_loss.append(valid_batch_loss / len(validloader))\n        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n        model.train()  # turn back on the dropout layers for the next training loop\n        \n        # Print progress\n        if verbose:\n            print(f\"Epoch {epoch + 1}:\",\n                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n                  f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n        \n        # Early stopping\n        if epoch &gt; 0 and valid_loss[-1] &gt; valid_loss[-2]:\n            consec_increases += 1\n        else:\n            consec_increases = 0\n        if consec_increases == patience:\n            print(f\"Stopped early at epoch {epoch + 1} - val loss increased for {consec_increases} consecutive epochs!\")\n            break\n    \n    results = {\"train_loss\": train_loss,\n               \"valid_loss\": valid_loss,\n               \"train_accuracy\": train_accuracy,\n               \"valid_accuracy\": valid_accuracy}\n    return results\n\nLet‚Äôs do it!\n\nmodel = BitmojiClassifier(IMAGE_SIZE * IMAGE_SIZE)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters())\nresults = trainer(model, criterion, optimizer, train_loader, valid_loader, epochs=20, patience=3)\n\nEpoch 1: Train Loss: 0.699. Valid Loss: 0.694. Train Accuracy: 0.49. Valid Accuracy: 0.50.\nEpoch 2: Train Loss: 0.695. Valid Loss: 0.693. Train Accuracy: 0.50. Valid Accuracy: 0.50.\nEpoch 3: Train Loss: 0.695. Valid Loss: 0.692. Train Accuracy: 0.49. Valid Accuracy: 0.62.\nEpoch 4: Train Loss: 0.693. Valid Loss: 0.692. Train Accuracy: 0.52. Valid Accuracy: 0.50.\nEpoch 5: Train Loss: 0.693. Valid Loss: 0.692. Train Accuracy: 0.51. Valid Accuracy: 0.50.\nEpoch 6: Train Loss: 0.690. Valid Loss: 0.688. Train Accuracy: 0.54. Valid Accuracy: 0.50.\nEpoch 7: Train Loss: 0.693. Valid Loss: 0.692. Train Accuracy: 0.51. Valid Accuracy: 0.50.\nEpoch 8: Train Loss: 0.689. Valid Loss: 0.684. Train Accuracy: 0.54. Valid Accuracy: 0.52.\nEpoch 9: Train Loss: 0.686. Valid Loss: 0.686. Train Accuracy: 0.56. Valid Accuracy: 0.63.\nEpoch 10: Train Loss: 0.689. Valid Loss: 0.694. Train Accuracy: 0.54. Valid Accuracy: 0.50.\nEpoch 11: Train Loss: 0.694. Valid Loss: 0.689. Train Accuracy: 0.50. Valid Accuracy: 0.55.\nEpoch 12: Train Loss: 0.691. Valid Loss: 0.679. Train Accuracy: 0.54. Valid Accuracy: 0.65.\nEpoch 13: Train Loss: 0.686. Valid Loss: 0.682. Train Accuracy: 0.56. Valid Accuracy: 0.56.\nEpoch 14: Train Loss: 0.677. Valid Loss: 0.671. Train Accuracy: 0.58. Valid Accuracy: 0.59.\nEpoch 15: Train Loss: 0.681. Valid Loss: 0.674. Train Accuracy: 0.58. Valid Accuracy: 0.58.\nEpoch 16: Train Loss: 0.670. Valid Loss: 0.672. Train Accuracy: 0.58. Valid Accuracy: 0.57.\nEpoch 17: Train Loss: 0.668. Valid Loss: 0.665. Train Accuracy: 0.61. Valid Accuracy: 0.62.\nEpoch 18: Train Loss: 0.664. Valid Loss: 0.657. Train Accuracy: 0.60. Valid Accuracy: 0.62.\nEpoch 19: Train Loss: 0.665. Valid Loss: 0.656. Train Accuracy: 0.60. Valid Accuracy: 0.63.\nEpoch 20: Train Loss: 0.663. Valid Loss: 0.651. Train Accuracy: 0.61. Valid Accuracy: 0.64.\n\n\n\nplot_loss(results[\"train_loss\"], results[\"valid_loss\"], results[\"train_accuracy\"], results[\"valid_accuracy\"])\n\n\n                                                \n\n\nI couldn‚Äôt get very good accuracy with this model and there‚Äôs a reason for that - we‚Äôre not considering the structure in our image. We‚Äôre flattening our images down into independent pixels, but the relationship between pixels is probably important! We‚Äôll exploit that next chapter when we get to CNNs. For now, let‚Äôs try a random image for fun:\n\nimport PIL\n\nimage = PIL.Image.open(\"img/tom.png\")\ntransform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor()\n])\nimage_t = transform(image)\nprediction = model(image_t.flatten())   # Flatten image to shape (1, 2500) and predict it\nprediction = torch.sigmoid(prediction)  # Coerce predictions to probabilities\nlabel = int(prediction &gt; 0.5)           # Get class label - 1 if propbability &gt; 0.5, else 0\n\n\nimage\n\n\n\n\n\nplt.figure(figsize=(4, 4))\nplt.axis(\"off\")\nplt.title(f\"Prediction: {['not_tom', 'tom'][label]}\", pad=10)\nplt.imshow(image_t[0], cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x2a186cbb0&gt;\n\n\n\n\n\n\nprint(prediction)\nprint(label)\n\ntensor([0.3696], grad_fn=&lt;SigmoidBackward0&gt;)\n0\n\n\nWell, at least we got that one!  Dang it! ‚Ä¶ so close ‚Ä¶ let‚Äôs try building some CNNs next!"
  },
  {
    "objectID": "chapter5_cnns-pt1.html#chapter-learning-objectives",
    "href": "chapter5_cnns-pt1.html#chapter-learning-objectives",
    "title": "5¬† Chapter 5: Introduction to Convolutional Neural Networks",
    "section": "5.1 Chapter Learning Objectives",
    "text": "5.1 Chapter Learning Objectives\n\n\nDescribe the terms convolution, kernel/filter, pooling, and flattening\nExplain how convolutional neural networks (CNNs) work\nCalculate the number of parameters in a given CNN architecture\nCreate a CNN in PyTorch\nDiscuss the key differences between CNNs and fully connected NNs"
  },
  {
    "objectID": "chapter5_cnns-pt1.html#imports",
    "href": "chapter5_cnns-pt1.html#imports",
    "title": "5¬† Chapter 5: Introduction to Convolutional Neural Networks",
    "section": "5.2 Imports",
    "text": "5.2 Imports\n\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torchinfo import summary\nimport matplotlib.pyplot as plt\nfrom utils.plotting import *\nplt.style.use('ggplot')\nplt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'axes.grid': False})"
  },
  {
    "objectID": "chapter5_cnns-pt1.html#convolutional-neural-networks-cnns",
    "href": "chapter5_cnns-pt1.html#convolutional-neural-networks-cnns",
    "title": "5¬† Chapter 5: Introduction to Convolutional Neural Networks",
    "section": "5.3 1. Convolutional Neural Networks (CNNs)",
    "text": "5.3 1. Convolutional Neural Networks (CNNs)\n\n\n5.3.1 1.1. Motivation\nUp until now we‚Äôve been dealing with ‚Äúfully connected neural networks‚Äù meaning that every neuron in a given layer is connected to every neuron in the next layer. This has two key implications: 1. It results in a LOT of parameters. 2. The order of our features doesn‚Äôt matter.\nConsider the simple image and fully connected network below:\n\nEvery input node is connected to every node in the next layer - is that really necessary? When you look at this image, how do you know that it‚Äôs me? - You notice the structure in the image (there‚Äôs a face, shoulders, a smile, etc.) - You notice how different structures are positioned and related (the face is on top of the shoulders, etc.) - You probably use the shading (colour) to infer things about the image too but we‚Äôll talk more about that later.\nThe point here is that the structure of our data (the pixels) is important. So maybe, we should have each hidden node only look at a small area of the image, like this:\n\nWe have far fewer parameters now because we‚Äôre acknowledging that pixels that are far apart are probably not all that related and so don‚Äôt need to be connected. We‚Äôre seeing that structure is important here, so then why should I need to flatten my image at all? Let‚Äôs be crazy and not flatten the image, but instead, make our hidden layer a 2D matrix:\n\nWe‚Äôre almost there!\nAs it stands, each group of 2 x 2 pixels has 4 unique weights associated with it (one for each pixel), which are being summed up into a single value in the hidden layer. But we don‚Äôt need the weights to be different for each group, we‚Äôre looking for structure, we don‚Äôt care if my face is in the top left or the bottom right, we‚Äôre just looking for a face!\nLet‚Äôs summarise the weights into a weight ‚Äúfilter‚Äù:\n\n - Let‚Äôs see how the filter works - We‚Äôll display some arbitrary values for our pixels - The filter ‚Äúconvolves‚Äù over each group of pixels, multiplies corresponding elements and sums them up to give the values in the output nodes:\n\nAs we‚Äôll see, we can add as many of these ‚Äúfilters‚Äù as we like to make more complex models that can identify more useful things:\n\nWe just made a convolutional neural network (CNN). Instead of fully-connected hidden nodes, we have 2D filters that we ‚Äúconvolve‚Äù over our input data. This has two key advantages: 1. We have less parameters than a fully connected network. 2. We preserve the useful structure of our data.\n\n\n5.3.2 1.2. Convolutions and Filters\nConvolution really just means ‚Äúto pass over the data‚Äù. What are we ‚Äúpassing‚Äù? Our filters - which are also called kernels. Here‚Äôs another gif like the one we saw earlier:\n\n\nSource: modified after theano-pymc.readthedocs.io.\n\nSo how does this help us extract structure from the data? Well let‚Äôs see some examples!\n\nimage = torch.from_numpy(plt.imread(\"img/tom_bw.png\"))\nplt.imshow(image, cmap='gray')\nplt.axis('off');\n\n\n\n\nWe can blur this image by applying a filter with the following weights:\n\\[\\begin{bmatrix} 0.0625 & 0.125 & 0.0625 \\\\ 0.125 & 0.25 & 0.125 \\\\ 0.0625 & 0.125 & 0.0625 \\end{bmatrix}\\]\ndef plot_conv(image, kernel):\n    \"\"\"Plot convs with matplotlib.\"\"\"\n    d = kernel.shape[-1]\n    conv = torch.nn.Conv2d(1, 1, kernel_size=(d, d), padding=1)\n    conv.weight.data[:] = kernel\n    fig, (ax1, ax2) = plt.subplots(figsize=(8, 4), ncols=2)\n    ax1.imshow(image, cmap='gray')\n    ax1.axis('off')\n    ax1.set_title(\"Original\")\n    ax2.imshow(conv(image[None, None, :]).detach().squeeze(), cmap='gray')  \n    ax2.set_title(\"Filtered\")\n    ax2.axis('off')\n    plt.tight_layout();\n\nkernel = torch.tensor([[[[ 0.0625,  0.1250,  0.0625],\n                         [ 0.1250,  0.2500,  0.1250],\n                         [ 0.0625,  0.1250,  0.0625]]]])\nplot_conv(image, kernel)\n\n\n\n\nHow about this one:\n\\[\\begin{bmatrix} -2 & -1 & 0 \\\\ -1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}\\]\n\nkernel = torch.tensor([[[[ -2,  -1,  0],\n                         [ -1,   1,  1],\n                         [  0,   1,  2]]]])\nplot_conv(image, kernel)\n\n\n\n\nOne more:\n\\[\\begin{bmatrix} -1 & -1 & -1 \\\\ -1 & 8 & -1 \\\\ -1 & -1 & -1 \\end{bmatrix}\\]\n\nkernel = torch.tensor([[[[  -1,  -1,   -1],\n                         [  -1,   8,   -1],\n                         [  -1,  -1,   -1]]]])\nplot_conv(image, kernel)\n\n\n\n\nHere‚Äôs a great website where you can play around with other filters. We usually use odd numbers for filters so that they are applied symmetrically around our input data. Did you notice in the gif earlier that the output from applying our kernel was smaller than the input? Take a look again:\n\n\nSource: modified after theano-pymc.readthedocs.io.\n\nBy default, our kernels are only applied where the filter fully fits on top of the input. But we can control this behaviour and the size of our output with: - padding: ‚Äúpads‚Äù the outside of the input 0‚Äôs to allow the kernel to reach the boundary pixels - strides: controls how far the kernel ‚Äústeps‚Äù over pixels.\nBelow is an example with: - padding=1: we have 1 layer of 0‚Äôs around our border - strides=(2,2): our kernel moves 2 data points to the right for each row, then moves 2 data points down to the next row\n\n\nSource: modified after theano-pymc.readthedocs.io.\n\nWe‚Äôll look more at these below."
  },
  {
    "objectID": "chapter5_cnns-pt1.html#cooking-up-a-cnn",
    "href": "chapter5_cnns-pt1.html#cooking-up-a-cnn",
    "title": "5¬† Chapter 5: Introduction to Convolutional Neural Networks",
    "section": "5.4 2. Cooking up a CNN",
    "text": "5.4 2. Cooking up a CNN\n\n\n5.4.1 2.1. Ingredient 1: Convolutional Layers\nI showed some example kernels above. In CNNs the actual values in the kernels are the weights your network will learn during training: your network will learn what structures are important for prediction.\nIn PyTorch, convolutional layers are defined as torch.nn.Conv2d, there are 5 important arguments we need to know: 1. in_channels: how many features are we passing in. Our features are our colour bands, in greyscale, we have 1 feature, in colour, we have 3 channels. 2. out_channels: how many kernels do we want to use. Analogous to the number of hidden nodes in a hidden layer of a fully connected network. 3. kernel_size: the size of the kernel. Above we were using 3x3. Common sizes are 3x3, 5x5, 7x7. 4. stride: the ‚Äústep-size‚Äù of the kernel. 5. padding: the number of pixels we should pad to the outside of the image so we can get edge pixels.\n\n# 1 kernel of (5,5)\nconv_layer = torch.nn.Conv2d(1, 1, kernel_size=(5, 5))\nplot_convs(image, conv_layer)\n\n\n\n\n\n# 2 kernels of (3,3)\nconv_layer = torch.nn.Conv2d(1, 2, kernel_size=(3, 3))\nplot_convs(image, conv_layer)\n\n\n\n\n\n# 3 kernels of (5,5)\nconv_layer = torch.nn.Conv2d(1, 3, kernel_size=(5, 5))\nplot_convs(image, conv_layer)\n\n\n\n\nIf we use a kernel with no padding, our output image will be smaller as we noted earlier. Let‚Äôs demonstrate that by using a larger kernel now:\n\n# 1 kernel of (51,51)\nconv_layer = torch.nn.Conv2d(1, 1, kernel_size=(50, 50))\nplot_convs(image, conv_layer, axis=True)\n\n\n\n\nAs we saw, we can add padding to the outside of the image to avoid this:\n\n# 1 kernel of (51,51) with padding\nconv_layer = torch.nn.Conv2d(1, 1, kernel_size=(51, 51), padding=25)\nplot_convs(image, conv_layer, axis=True)\n\n\n\n\n\nSetting padding = kernel_size // 2 will always result in an output the same shape as the input. Think about why this is‚Ä¶\n\nFinally, we also saw before how strides influence the size of the output:\n\n# 1 kernel of (25,25) with stride of 3\nconv_layer = torch.nn.Conv2d(1, 1, kernel_size=(25, 25), stride=3)\nplot_convs(image, conv_layer, axis=True)\n\n\n\n\nWith CNN we are no longer flattening our data, so what are our ‚Äúfeatures‚Äù? Our features are called ‚Äúchannels‚Äù in CNN-lingo -&gt; they are like the colour channels in an image: - A grayscale image has 1 feature/channel - A coloured image has 3 features/channel\n\n\nWhat‚Äôs important with CNNs is that the size of our input data does not impact how many parameters we have in our convolutonal layers. For example, your kernels don‚Äôt care how big your image is (i.e., 28 x 28 or 256 x 256), all that matters is: 1. How many features (‚Äúchannels‚Äù) you have: in_channels 2. How many filters you use in each layer: out_channels 3. How big the filters are: kernel_size\nLet‚Äôs see some diagrams:\n\nFor coloured images (3 channels):\n\n\n\n5.4.2 2.2. Ingredient 2: Flattening\nWith our brand new, shiny convolutional layers, we‚Äôre basically just passing images through the network - cool!\nBut we‚Äôre going to eventually want to do some regression or classification. That means that by the end of our network, we are going to need to torch.nn.Flatten() our images:\n\nLet‚Äôs make that simple CNN above in PyTorch:\n\nfrom math import prod\n\nclass CNN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), padding=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(3, 3), padding=1),\n            torch.nn.ReLU(),\n            torch.nn.Flatten(),\n            torch.nn.Linear(20000, 1)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n\n\nmodel = CNN()\nsummary(model, (1, 1, 100, 100))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 1]                    --\n‚îú‚îÄSequential: 1-1                        [1, 1]                    --\n‚îÇ    ‚îî‚îÄConv2d: 2-1                       [1, 3, 100, 100]          30\n‚îÇ    ‚îî‚îÄReLU: 2-2                         [1, 3, 100, 100]          --\n‚îÇ    ‚îî‚îÄConv2d: 2-3                       [1, 2, 100, 100]          56\n‚îÇ    ‚îî‚îÄReLU: 2-4                         [1, 2, 100, 100]          --\n‚îÇ    ‚îî‚îÄFlatten: 2-5                      [1, 20000]                --\n‚îÇ    ‚îî‚îÄLinear: 2-6                       [1, 1]                    20,001\n==========================================================================================\nTotal params: 20,087\nTrainable params: 20,087\nNon-trainable params: 0\nTotal mult-adds (M): 0.88\n==========================================================================================\nInput size (MB): 0.04\nForward/backward pass size (MB): 0.40\nParams size (MB): 0.08\nEstimated Total Size (MB): 0.52\n==========================================================================================\n\n\nOh man! 20,000 parameters in that last layer, geez. Is there a way we can reduce this somehow? Glad you asked! See you in the next section.\n\n\n5.4.3 2.3. Ingredient 3: Pooling\nPooling is how we can reduce the number of parameters we get out of a torch.nn.Flatten(). It‚Äôs pretty simple, we just aggregate the data, usually using the maximum or average of a window of pixels. Here‚Äôs an example of max pooling:\n\n\nSource: modified after www.oreilly.com/.\n\nWe use ‚Äúpooling layers‚Äù to reduce the shape of our image as it‚Äôs passing through the network. So when we eventually torch.nn.Flatten(), we‚Äôll have less features in that flattened layer! We can implement pooling with torch.nn.MaxPool2d(). Let‚Äôs try it out and reduce the number of parameters:\n\nclass CNN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(mp_kernel_size),\n            torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(3, 3), padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(mp_kernel_size),\n            torch.nn.Flatten(),\n            torch.nn.Linear(1250, 1)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n\n\nmodel = CNN()\nsummary(model, (1, 1, 100, 100))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 1]                    --\n‚îú‚îÄSequential: 1-1                        [1, 1]                    --\n‚îÇ    ‚îî‚îÄConv2d: 2-1                       [1, 3, 100, 100]          30\n‚îÇ    ‚îî‚îÄReLU: 2-2                         [1, 3, 100, 100]          --\n‚îÇ    ‚îî‚îÄMaxPool2d: 2-3                    [1, 3, 50, 50]            --\n‚îÇ    ‚îî‚îÄConv2d: 2-4                       [1, 2, 50, 50]            56\n‚îÇ    ‚îî‚îÄReLU: 2-5                         [1, 2, 50, 50]            --\n‚îÇ    ‚îî‚îÄMaxPool2d: 2-6                    [1, 2, 25, 25]            --\n‚îÇ    ‚îî‚îÄFlatten: 2-7                      [1, 1250]                 --\n‚îÇ    ‚îî‚îÄLinear: 2-8                       [1, 1]                    1,251\n==========================================================================================\nTotal params: 1,337\nTrainable params: 1,337\nNon-trainable params: 0\nTotal mult-adds (M): 0.44\n==========================================================================================\nInput size (MB): 0.04\nForward/backward pass size (MB): 0.28\nParams size (MB): 0.01\nEstimated Total Size (MB): 0.33\n==========================================================================================\n\n\nWe reduced that last layer to 1,251 parameters. Nice job!"
  },
  {
    "objectID": "chapter5_cnns-pt1.html#the-cnn-recipe-book",
    "href": "chapter5_cnns-pt1.html#the-cnn-recipe-book",
    "title": "5¬† Chapter 5: Introduction to Convolutional Neural Networks",
    "section": "5.5 3. The CNN Recipe Book",
    "text": "5.5 3. The CNN Recipe Book\n\nHere‚Äôs a CNN diagram of a famous architecture called AlexNet (we‚Äôll talk more about ‚Äúfamous architectures‚Äù next Chapter):\n\nYou actually know what all of the above means now! But, deep learning and CNN architecture remains very much an art. Here is my general recipe book (based on experience, common practice, and popular pre-made architectures - more on those next chapter).\nTypical ingredients (in order): - Convolution layer(s): torch.nn.Conv2d - Activation function: torch.nn.ReLU, torch.nn.Sigmoid, torch.nn.Softplus, etc. - (optional) Batch normalization: torch.nn.BatchNorm2d (more on that next Chapter) - (optional) Pooling: torch.nn.MaxPool2d - (optional) Drop out: torch.nn.Dropout - Flatten: torch.nn.Flatten"
  },
  {
    "objectID": "chapter5_cnns-pt1.html#cnn-vs-fully-connected-nn",
    "href": "chapter5_cnns-pt1.html#cnn-vs-fully-connected-nn",
    "title": "5¬† Chapter 5: Introduction to Convolutional Neural Networks",
    "section": "5.6 4. CNN vs Fully Connected NN",
    "text": "5.6 4. CNN vs Fully Connected NN\n\nAs an example of the parameter savings introduced when using CNNs with structured data, let‚Äôs compare the Bitmoji classifier from last chapter, with an equivalent CNN version.\nWe‚Äôll replace all linear layers with convolutional layers with 3 kernels of size (3, 3) and will assume an image size of 128 x 128:\n\ndef linear_block(input_size, output_size):\n    return torch.nn.Sequential(\n        torch.nn.Linear(input_size, output_size),\n        torch.nn.ReLU()\n    )\n\ndef conv_block(input_channels, output_channels):\n    return torch.nn.Sequential(\n        torch.nn.Conv2d(input_channels, output_channels, (3, 3), padding=1),\n        torch.nn.ReLU()\n    )\n\nclass NN(torch.nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n            linear_block(input_size, 256),\n            linear_block(256, 128),\n            linear_block(128, 64),\n            linear_block(64, 16),\n            torch.nn.Linear(16, 1)\n        )\n        \n    def forward(self, x):\n        out = self.main(x)\n        return out\n    \nclass CNN(torch.nn.Module):\n    def __init__(self, input_channels):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n            conv_block(input_channels, 3),\n            conv_block(3, 3),\n            conv_block(3, 3),\n            conv_block(3, 3),\n            conv_block(3, 3),\n            torch.nn.Flatten(),\n            torch.nn.Linear(49152, 1)\n        )\n        \n    def forward(self, x):\n        out = self.main(x)\n        return out\n\n\nmodel = NN(input_size=128*128)\nsummary(model, (1, 128*128))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nNN                                       [1, 1]                    --\n‚îú‚îÄSequential: 1-1                        [1, 1]                    --\n‚îÇ    ‚îî‚îÄSequential: 2-1                   [1, 256]                  --\n‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-1                  [1, 256]                  4,194,560\n‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-2                    [1, 256]                  --\n‚îÇ    ‚îî‚îÄSequential: 2-2                   [1, 128]                  --\n‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-3                  [1, 128]                  32,896\n‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-4                    [1, 128]                  --\n‚îÇ    ‚îî‚îÄSequential: 2-3                   [1, 64]                   --\n‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-5                  [1, 64]                   8,256\n‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-6                    [1, 64]                   --\n‚îÇ    ‚îî‚îÄSequential: 2-4                   [1, 16]                   --\n‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-7                  [1, 16]                   1,040\n‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-8                    [1, 16]                   --\n‚îÇ    ‚îî‚îÄLinear: 2-5                       [1, 1]                    17\n==========================================================================================\nTotal params: 4,236,769\nTrainable params: 4,236,769\nNon-trainable params: 0\nTotal mult-adds (M): 4.24\n==========================================================================================\nInput size (MB): 0.07\nForward/backward pass size (MB): 0.00\nParams size (MB): 16.95\nEstimated Total Size (MB): 17.02\n==========================================================================================\n\n\n\nmodel = CNN(input_channels=1)\nsummary(model, (1, 1, 128, 128))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 1]                    --\n‚îú‚îÄSequential: 1-1                        [1, 1]                    --\n‚îÇ    ‚îî‚îÄSequential: 2-1                   [1, 3, 128, 128]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                  [1, 3, 128, 128]          30\n‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-2                    [1, 3, 128, 128]          --\n‚îÇ    ‚îî‚îÄSequential: 2-2                   [1, 3, 128, 128]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-3                  [1, 3, 128, 128]          84\n‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-4                    [1, 3, 128, 128]          --\n‚îÇ    ‚îî‚îÄSequential: 2-3                   [1, 3, 128, 128]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-5                  [1, 3, 128, 128]          84\n‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-6                    [1, 3, 128, 128]          --\n‚îÇ    ‚îî‚îÄSequential: 2-4                   [1, 3, 128, 128]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-7                  [1, 3, 128, 128]          84\n‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-8                    [1, 3, 128, 128]          --\n‚îÇ    ‚îî‚îÄSequential: 2-5                   [1, 3, 128, 128]          --\n‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-9                  [1, 3, 128, 128]          84\n‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-10                   [1, 3, 128, 128]          --\n‚îÇ    ‚îî‚îÄFlatten: 2-6                      [1, 49152]                --\n‚îÇ    ‚îî‚îÄLinear: 2-7                       [1, 1]                    49,153\n==========================================================================================\nTotal params: 49,519\nTrainable params: 49,519\nNon-trainable params: 0\nTotal mult-adds (M): 6.05\n==========================================================================================\nInput size (MB): 0.07\nForward/backward pass size (MB): 1.97\nParams size (MB): 0.20\nEstimated Total Size (MB): 2.23\n==========================================================================================\n\n\nWe don‚Äôt even have any pooling and our CNN still has a ‚Äúmeager‚Äù 49,519 parameters vs 4,236,769 for the fully-connected network. This is a somewhat arbitray comparison but it proves my point."
  },
  {
    "objectID": "chapter6_cnns-pt2.html#chapter-learning-objectives",
    "href": "chapter6_cnns-pt2.html#chapter-learning-objectives",
    "title": "6¬† Chapter 6: Advanced Convolutional Neural Networks",
    "section": "6.1 Chapter Learning Objectives",
    "text": "6.1 Chapter Learning Objectives\n\n\nLoad image data using torchvision.datasets.ImageFolder() to train a network in PyTorch.\nExplain what ‚Äúdata augmentation‚Äù is and why we might want to do it.\nBe able to save and re-load a PyTorch model.\nTune the hyperparameters of a PyTorch model using Ax.\nDescribe what transfer learning is and the different flavours of it: ‚Äúout-of-the-box‚Äù, ‚Äúfeature extractor‚Äù, ‚Äúfine tuning‚Äù."
  },
  {
    "objectID": "chapter6_cnns-pt2.html#imports",
    "href": "chapter6_cnns-pt2.html#imports",
    "title": "6¬† Chapter 6: Advanced Convolutional Neural Networks",
    "section": "6.2 Imports",
    "text": "6.2 Imports\n\n\nimport json\nimport numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\nimport torch\nimport torchvision\nfrom torch import nn, optim\nfrom torchvision import transforms, models, datasets\nfrom torchsummary import summary\nfrom PIL import Image\nimport memory_profiler  # conda install -c anaconda memory_profiler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom utils.plotting import *"
  },
  {
    "objectID": "chapter6_cnns-pt2.html#datasets-dataloaders-and-transforms",
    "href": "chapter6_cnns-pt2.html#datasets-dataloaders-and-transforms",
    "title": "6¬† Chapter 6: Advanced Convolutional Neural Networks",
    "section": "6.3 1. Datasets, Dataloaders, and Transforms",
    "text": "6.3 1. Datasets, Dataloaders, and Transforms\n\n\n6.3.1 1.1. Preparing Data\ntorch and torchvision provide out-of-the-box functionality for loading in lots of different kinds of data. The way you create a dataloader depends on the data you have (i.e., do you have numpy arrays, tensors, images, or something else?) and the PyTorch docs can help you out\nLoading data into PyTorch is usually a two-step process: 1. Create a dataset (this is your raw data) 2. Create a dataloader (this will help you batch your data)\nWorking with CNNs and images, you‚Äôll mostly be using torchvision.datasets.ImageFolder() (docs), it‚Äôs very easy to use. It assumes you have a directory structure with sub-directories for each class like this:\ndata\n‚îÇ\n‚îú‚îÄ‚îÄ class_1\n‚îÇ   ‚îú‚îÄ‚îÄ image_1.png \n‚îÇ   ‚îú‚îÄ‚îÄ image_2.png\n‚îÇ   ‚îú‚îÄ‚îÄ image_3.png\n‚îÇ   ‚îî‚îÄ‚îÄ etc.\n‚îî‚îÄ‚îÄ class_2\n    ‚îú‚îÄ‚îÄ image_1.png \n    ‚îú‚îÄ‚îÄ image_2.png\n    ‚îú‚îÄ‚îÄ image_3.png\n    ‚îî‚îÄ‚îÄ etc.\nFor example, consider the training dataset I have in the current directory at Chapters/data/bitmoji_rgb:\nbitmoji_rgb\n‚îî‚îÄ‚îÄ train\n    ‚îú‚îÄ‚îÄ not_tom\n    ‚îÇ   ‚îú‚îÄ‚îÄ image_1.png \n    ‚îÇ   ‚îú‚îÄ‚îÄ image_2.png\n    ‚îÇ   ‚îú‚îÄ‚îÄ image_3.png\n    ‚îÇ   ‚îî‚îÄ‚îÄ etc.\n    ‚îî‚îÄ‚îÄ tom\n        ‚îú‚îÄ‚îÄ image_1.png \n        ‚îú‚îÄ‚îÄ image_2.png\n        ‚îú‚îÄ‚îÄ image_3.png\n        ‚îî‚îÄ‚îÄ etc.\n\nTRAIN_DIR = \"data/bitmoji_rgb/train/\"\n\nmem = memory_profiler.memory_usage()[0]\ntrain_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR)\nprint(f\"Memory consumed: {memory_profiler.memory_usage()[0] - mem:.0f} mb\")\n\nMemory consumed: 0 mb\n\n\nNotice how our memory usage is the same, we aren‚Äôt loading anything in yet, just making PyTorch aware of what kind of data we have and where it is. We can now check various information about our train_dataset:\n\nprint(f\"Classes: {train_dataset.classes}\")\nprint(f\"Class count: {train_dataset.targets.count(0)}, {train_dataset.targets.count(1)}\")\nprint(f\"Samples:\",len(train_dataset))\nprint(f\"First sample: {train_dataset.samples[0]}\")\n\nClasses: ['not_tom', 'tom']\nClass count: 857, 857\nSamples: 1714\nFirst sample: ('data/bitmoji_rgb/train/not_tom/bitmoji_10187.png', 0)\n\n\nNow, we could start working with this dataset directly. For example, here‚Äôs the first sample:\n\nimg, target = next(iter(train_dataset))\nprint(f\"Class: {train_dataset.classes[target]}\")\nimg\n\nClass: not_tom\n\n\n\n\n\nBut often we want to apply some pre-processing to our data. For example, ImageFolder loads our data using the PIL package, but we need tensors!\n\nprint(f\"Image data type: {type(img)}\")\nprint(f\"     Image size: {img.size}\")\n\nImage data type: &lt;class 'PIL.Image.Image'&gt;\n     Image size: (128, 128)\n\n\nAny pre-processing we wish to apply to our images is done using torchvision.transforms. There are a lot of transformation options here - we‚Äôll explore some more later - for now, we‚Äôll Resize() our images and convert them ToTensor(). We use transforms.Compose() to chain multiple transformations together:\n\nIMAGE_SIZE = 64\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor()\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR,\n                                                 transform=data_transforms)\nimg, target = next(iter(train_dataset))\nprint(f\"Image data type: {type(img)}\")\nprint(f\"     Image size: {img.shape}\")\n\nImage data type: &lt;class 'torch.Tensor'&gt;\n     Image size: torch.Size([3, 64, 64])\n\n\nOkay cool, but there‚Äôs one more issue: we want to work with batches of data, because most of the time, we won‚Äôt be able to fit an entire dataset into RAM at once (especially when it comes to image data). This is where PyTorch‚Äôs dataloader comes in. It allows us to specify how we want to batch our data:\n\nBATCH_SIZE = 64\n\nmem = memory_profiler.memory_usage()[0]\ntrain_loader = torch.utils.data.DataLoader(train_dataset,          # our raw data\n                                           batch_size=BATCH_SIZE,  # the size of batches we want the dataloader to return\n                                           shuffle=True,           # shuffle our data before batching\n                                           drop_last=False)        # don't drop the last batch even if it's smaller than batch_size\nprint(f\"Memory consumed: {memory_profiler.memory_usage()[0] - mem:.0f} mb\")\n\nMemory consumed: 0 mb\n\n\nOnce again, we aren‚Äôt loading anything yet, we just prepared the loader. We can now query the loader to return a batch of data (this will consume memory):\n\nmem = memory_profiler.memory_usage()[0]\nimgs, targets = next(iter(train_loader))\nprint(f\"       # of batches: {len(train_loader)}\")\nprint(f\"    Image data type: {type(imgs)}\")\nprint(f\"   Image batch size: {imgs.shape}\")  # dimensions are (batch size, image channels, image height, image width)\nprint(f\"  Target batch size: {targets.shape}\")\nprint(f\"       Batch memory: {memory_profiler.memory_usage()[0] - mem:.2f} mb\")  # memory usage after loading batch\n\n       # of batches: 27\n    Image data type: &lt;class 'torch.Tensor'&gt;\n   Image batch size: torch.Size([64, 3, 64, 64])\n  Target batch size: torch.Size([64])\n       Batch memory: 0.14 mb\n\n\n\n\n6.3.2 1.2. Saving and Loading PyTorch Models\nThe PyTorch documentation about saving and loading models is fantastic and the process is very easy. It‚Äôs common PyTorch convention to save models using either a .pt or .pth file extension. It is recommended that you just save your model learned parameters from model.state_dict():\n# Save model\nPATH = \"models/my_model.pt\"\ntorch.save(model.state_dict(), PATH)     # save model at PATH\n# Load model\nmodel = MyModelClass()                   # create an instance of the model\nmodel.load_state_dict(torch.load(PATH))  # load model from PATH\nIf you‚Äôre using the model for inference (not training), make sure to switch it to eval mode: model.eval(). There are other options for saving models, in particular, if you want to save a model and continue training it later, you‚Äôll want to save other necessary information like the optimizer state, the epoch you‚Äôre on, etc. This is all documented here in the PyTorch docs.\nLet‚Äôs see an example of a model I saved earlier:\n\nclass bitmoji_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 8, (5, 5)),\n            nn.ReLU(),\n            nn.MaxPool2d((2, 2)),\n            nn.Conv2d(8, 4, (3, 3)),\n            nn.ReLU(),\n            nn.MaxPool2d((3, 3)),\n            nn.Flatten(),\n            nn.Linear(324, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n        \n    def forward(self, x):\n        out = self.main(x)\n        return out\n\n\nPATH = \"models/bitmoji_cnn.pt\"\nmodel = bitmoji_CNN()\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()\n\nbitmoji_CNN(\n  (main): Sequential(\n    (0): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=324, out_features=128, bias=True)\n    (8): ReLU()\n    (9): Linear(in_features=128, out_features=1, bias=True)\n  )\n)\n\n\n\n\n6.3.3 1.3. Data Augmentation\nData augmentation is used for two main purposes: 1. Make your CNN more robust to scale/rotation/translation in your images 2. Increase the size of your training set\nLet‚Äôs explore point 1 a bit further. We can see below is a Bitmoji of me, does the CNN I loaded above predict this?\n\nimage = Image.open('img/tom-bitmoji.png')\nimage\n\n\n\n\n\nIMAGE_SIZE = 64\nimage_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0)\nprediction = int(torch.sigmoid(model(image_tensor)) &gt; 0.5)\nprint(f\"Prediction: {train_dataset.classes[prediction]}\")\n\nPrediction: tom\n\n\nLooks good! But what happens if I flip my image. You can still tell it‚Äôs me, but can my CNN?\n\nimage = image.rotate(180)\nimage\n\n\n\n\n\nimage_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0)\nprediction = int(torch.sigmoid(model(image_tensor)) &gt; 0.5)\nprint(f\"Prediction: {train_dataset.classes[prediction]}\")\n\nPrediction: not_tom\n\n\nWell that‚Äôs problematic. We‚Äôd like our CNN to be robust against these kinds of differences. We can expose our CNN to flipped images, so that it can learn to better predict them, with data augmentation. Common image augmentations include: - rotation/flipping - cropping - adding noise - You can view others in the PyTorch docs\nWe add transforms just like we did previously, using the transform argument of torchvision.datasets.ImageFolder():\n\ndata_transforms = transforms.Compose([\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomRotation(degrees=20),\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor()\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR,\n                                                 transform=data_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=BATCH_SIZE,\n                                           shuffle=True,\n                                           drop_last=False)\nsample_batch, target = next(iter(train_loader))\nplot_bitmojis(sample_batch, rgb=True)\n\n\n\n\nHere‚Äôs a model I trained earlier using the above augmentations (see Appendix D: Creating a CNN to Predict Bitmojis for the full code):\n\nPATH = \"models/bitmoji_cnn_augmented.pt\"\nmodel = bitmoji_CNN()\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval();\n\nLet‚Äôs try it out on the flipped image:\n\nimage\n\n\n\n\n\nimage_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0)\nprediction = int(torch.sigmoid(model(image_tensor)) &gt; 0.5)\nprint(f\"Prediction: {train_dataset.classes[prediction]}\")\n\nPrediction: tom\n\n\nNow we got it!\n\n\n6.3.4 1.4. Batch Normalization\nEarlier in the course, we saw how normalizing the inputs to our neural network can help our optimization (by making sure the scale of one feature doesn‚Äôt overwhelm others). But what about the hidden layers of our network? They also have data flowing into them and parameters to optimize, can we normalize them too to make optimization better? Sure can! Batch normalization is the normalization of data in hidden layers.\nIt is usually applied after the activation function of a hidden layer (but it doesn‚Äôt have to be):\n\\[z^* = \\frac{z - \\mu}{\\sqrt{\\sigma{}^2} + \\eta}\\times\\gamma+\\beta\\]\nWhere: - \\(z\\) = the output of your hideen layers before the activation function - \\(\\mu = \\frac{1}{n}\\sum_{i=1}^{n}z_i\\) (i.e., the mean of \\(z\\)) - \\(\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(z_i-\\mu)^2\\) (i.e, the variance of \\(z\\))\nBatch normalization can help stabilize and speed up optimization, make your network more invariant to changes in the training distribution, and often has a slight regularization effect. See this video by Andrew Ng if you want to learn more about the details."
  },
  {
    "objectID": "chapter6_cnns-pt2.html#hyperparameter-tuning",
    "href": "chapter6_cnns-pt2.html#hyperparameter-tuning",
    "title": "6¬† Chapter 6: Advanced Convolutional Neural Networks",
    "section": "6.4 2. Hyperparameter Tuning",
    "text": "6.4 2. Hyperparameter Tuning\n\nWith neural networks we potentially have a lot of hyperparameters to tune: - Number of layers - Number of nodes in each layer - Activation functions - Regularization - Initialization (starting weights) - Optimization hyperparams (learning rate, momentum, weight decay) - etc.\nWith so many parameters, a grid-search approach to optimization is not feasible. Luckily, there are many packages out there that make neural network hyperparameter tuning fast and easy: - Ax - Raytune - Neptune - skorch\nWe‚Äôll be using Ax, created by Facebook (just like PyTorch):\npip install ax-platform\nBelow, I‚Äôve adapted a tutorial from their docs:\n\nfrom ax.service.managed_loop import optimize\nfrom ax.plot.contour import plot_contour\nfrom ax.plot.trace import optimization_trace_single_method\nfrom ax.utils.notebook.plotting import render, init_notebook_plotting\n\nFirst, I‚Äôll create some simple training and validation loaders:\n\nTRAIN_DIR = \"data/bitmoji_rgb/train/\"\nVALID_DIR = \"data/bitmoji_rgb/valid/\"\nIMAGE_SIZE = 64\nBATCH_SIZE = 128\n# Transforms\ndata_transforms = transforms.Compose([transforms.Resize(IMAGE_SIZE), transforms.ToTensor()])\n# Load data and create dataloaders\ntrain_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_dataset = torchvision.datasets.ImageFolder(root=VALID_DIR, transform=data_transforms)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n# GPU available?\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using: {device}\")\n# Model\nmodel = bitmoji_CNN()\nmodel.to(device);\n\nUsing: cpu\n\n\nNow, we need a training function. This function will be re-run multiple times throughout the hyperparameter optimization process, as we wish to train the model on different hyperparameter configurations. The argument parameters is a dictionary containing the hyperparameters we wish to tune:\n\ndef train(model, train_loader, hyperparameters, epochs=20):\n    \"\"\"Training wrapper for PyTorch network.\"\"\"\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(),\n                           lr=hyperparameters.get(\"lr\", 0.001),\n                           betas=(hyperparameters.get(\"beta1\", 0.9), 0.999))\n    for epoch in range(epochs):\n        for X, y in train_loader:\n            if device.type == 'cuda':\n                X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_hat = model(X).flatten()\n            loss = criterion(y_hat, y.type(torch.float32))\n            loss.backward()\n            optimizer.step()\n    \n    return model\n\nWe also need an evaluate() function that reports how well our model is doing on some validation data. This will also be called multiple times during the hyperparameter optimization:\n\ndef evaluate(model, valid_loader):\n    \"\"\"Validation wrapper for PyTorch network.\"\"\"\n    \n    model.eval()\n    accuracy = 0\n    with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n        for X, y in valid_loader:\n            if device.type == 'cuda':\n                X, y = X.to(device), y.to(device)\n            y_hat = model(X).flatten()\n            y_hat_labels = torch.sigmoid(y_hat) &gt; 0.5\n            accuracy += (y_hat_labels == y).type(torch.float32).mean().item()\n    accuracy /= len(valid_loader)  # avg accuracy\n    \n    return accuracy\n\nLet‚Äôs make sure our evaluation function is working:\n\nevaluate(model, valid_loader)\n\n0.46875\n\n\nLooks good! The accuracy is bad right now because we haven‚Äôt trained our model yet.\nWe now need a wrapper function that puts everything together. Basically each iteration of hyperparameter optimization (i.e., each time we try a new set of hyperparameters), this function is executed. It trains the model using the given hyperparameters, and then evaluates the model‚Äôs performance:\n\ndef train_evaluate(parameterization):\n    model = bitmoji_CNN()\n    model.to(device)\n    model = train(model, train_loader, hyperparameters=parameterization)\n    return evaluate(model, valid_loader)\n\nFinally, we use optimize() to run Bayesian optimization on a hyperparameter dictionary. I ran this on a GPU and have included the output below:\n\nbest_parameters, values, experiment, model = optimize(\n    parameters=[\n        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-6, 0.4], \"log_scale\": True, \"value_type\": 'float'},\n        {\"name\": \"beta1\", \"type\": \"range\", \"bounds\": [0.5, 0.999], \"value_type\": 'float'},\n    ],\n    evaluation_function=train_evaluate,\n    objective_name='accuracy',\n    total_trials = 20\n)\n\n[INFO 01-28 17:25:27] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 trials, GPEI for subsequent trials]). Iterations after 5 will take longer to generate due to  model-fitting.\n[INFO 01-28 17:25:27] ax.service.managed_loop: Started full optimization with 20 steps.\n[INFO 01-28 17:25:27] ax.service.managed_loop: Running optimization trial 1...\n[INFO 01-28 17:26:28] ax.service.managed_loop: Running optimization trial 2...\n[INFO 01-28 17:27:25] ax.service.managed_loop: Running optimization trial 3...\n[INFO 01-28 17:28:20] ax.service.managed_loop: Running optimization trial 4...\n[INFO 01-28 17:29:13] ax.service.managed_loop: Running optimization trial 5...\n[INFO 01-28 17:30:06] ax.service.managed_loop: Running optimization trial 6...\n[INFO 01-28 17:31:01] ax.service.managed_loop: Running optimization trial 7...\n[INFO 01-28 17:31:57] ax.service.managed_loop: Running optimization trial 8...\n[INFO 01-28 17:32:52] ax.service.managed_loop: Running optimization trial 9...\n[INFO 01-28 17:33:46] ax.service.managed_loop: Running optimization trial 10...\n[INFO 01-28 17:34:41] ax.service.managed_loop: Running optimization trial 11...\n[INFO 01-28 17:35:36] ax.service.managed_loop: Running optimization trial 12...\n[INFO 01-28 17:36:30] ax.service.managed_loop: Running optimization trial 13...\n[INFO 01-28 17:37:25] ax.service.managed_loop: Running optimization trial 14...\n[INFO 01-28 17:38:20] ax.service.managed_loop: Running optimization trial 15...\n[INFO 01-28 17:39:15] ax.service.managed_loop: Running optimization trial 16...\n[INFO 01-28 17:40:08] ax.service.managed_loop: Running optimization trial 17...\n[INFO 01-28 17:41:03] ax.service.managed_loop: Running optimization trial 18...\n[INFO 01-28 17:41:59] ax.service.managed_loop: Running optimization trial 19...\n[INFO 01-28 17:43:01] ax.service.managed_loop: Running optimization trial 20...\n\nbest_parameters\n\n{'lr': 0.0015762617170424081, 'beta1': 0.6464455205729447}\n\nmeans, covariances = values\nprint(f\"Accuracy: {means['accuracy']*100:.2f}%\")\n\nAccuracy: 86.91%\n\nrender(plot_contour(model=model, param_x='lr', param_y='beta1', metric_name='accuracy'))\n\n\n\nbest_objectives = np.array([[trial.objective_mean*100 for trial in experiment.trials.values()]])\nbest_objective_plot = optimization_trace_single_method(\n    y=np.maximum.accumulate(best_objectives, axis=1),\n    title=\"Model performance vs. # of iterations\",\n    ylabel=\"Classification Accuracy, %\",\n)\nrender(best_objective_plot)"
  },
  {
    "objectID": "chapter6_cnns-pt2.html#explaining-cnns",
    "href": "chapter6_cnns-pt2.html#explaining-cnns",
    "title": "6¬† Chapter 6: Advanced Convolutional Neural Networks",
    "section": "6.5 3. Explaining CNNs",
    "text": "6.5 3. Explaining CNNs\n\nCNNs and neural networks in general are primarily used for prediction (i.e., we want the best prediction performance, and we might not care how we get it). However interpreting why a model makes certain predictions can be useful. Interpreting neural networks is an active area of research and it is difficult to do. There are a few main options: - SHAP - Grad-CAM - Captum\nCaptum is a library for specifically interpreting PyTorch models. Captum contains a variety of state-of-the-art algorithms for interpreting models, see the docs here. Let‚Äôs use it quickly to find what areas of the following bitmoji are important for the models prediction of ‚Äútom‚Äù:\n\nfrom captum.attr import GradientShap\nfrom captum.attr import Occlusion\nfrom captum.attr import visualization as viz\n\n\nPATH = \"models/bitmoji_cnn_augmented.pt\"\nmodel = bitmoji_CNN()\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval();\n\n\nimage\n\n\n\n\n\nimage_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0)\nprediction = int(torch.sigmoid(model(image_tensor)) &gt; 0.5)\nprint(f\"Prediction: {train_dataset.classes[prediction]}\")\n\nPrediction: tom\n\n\n\n# Occlusion\nocclusion = Occlusion(model)\nattributions_occ = occlusion.attribute(image_tensor,\n                                       strides = (1, 3, 3),\n                                       sliding_window_shapes=(1, 10, 10),\n                                       baselines=0)\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().detach().numpy(), (1,2,0)),\n                                      np.transpose(image_tensor.squeeze().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"blended_heat_map\"],\n                                      [\"all\", \"positive\"],\n                                      titles = [\"Original Image\", \"Occlusion\"],\n                                      cmap=\"plasma\",\n                                      fig_size=(6, 6),\n                                      alpha_overlay=0.7\n                                     )\n\n\n\n\n\n# Gradient SHAP\ntorch.manual_seed(2020); np.random.seed(2020)\ngradient_shap = GradientShap(model)\nrand_img_dist = torch.cat([image_tensor * 0, image_tensor * 1])\nattributions_gs = gradient_shap.attribute(image_tensor,\n                                          n_samples=20,\n                                          stdevs=0.15,\n                                          baselines=rand_img_dist,\n                                          target=0)\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_gs.squeeze().detach().numpy(), (1,2,0)),\n                                      np.transpose(image_tensor.squeeze().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"blended_heat_map\"],\n                                      [\"all\", \"absolute_value\"],\n                                      titles = [\"Original Image\", \"Gradient SHAP\"],\n                                      cmap=\"plasma\",\n                                      show_colorbar=True,\n                                      fig_size=(6, 6),\n                                      alpha_overlay=0.7)"
  },
  {
    "objectID": "chapter6_cnns-pt2.html#transfer-learning",
    "href": "chapter6_cnns-pt2.html#transfer-learning",
    "title": "6¬† Chapter 6: Advanced Convolutional Neural Networks",
    "section": "6.6 4. Transfer Learning",
    "text": "6.6 4. Transfer Learning\n\nTransfer learning is one of the most common techniques used in deep learning. It refers to using a model already trained on one task as a starting point for learning to perform another task. There are many famous deep learning architectures out there that have been very successful across a wide range of problem, e.g.: AlexNet, VGG, ResNet, Inception, MobileNet, etc.\nMany of these models have been pre-trained on famous datasets like ImageNet (which contains 1.2 million labelled images with 1000 categories). So, why not use these famous architectures for our own tasks?! I like to think of there being three main kinds of transfer learning: 1. Use a pre-trained network out-of-the-box 2. Use a pre-trained network as a ‚Äúfeature extractor‚Äù and add new layers to it for your own task 3. Same as 2 but ‚Äúfine-tune‚Äù the weights of the pre-trained network using your own data\nWe‚Äôll briefly explore these options below.\n\n6.6.1 4.1. Out-Of-The-Box\nThis is the simplest option of transfer learning. You basically download a model that performs the same task as you want to do and just use it to predict your own images. We can easily download famous models using the torchvision.models module. All models are available with pre-trained weights (based on ImageNet‚Äôs 224 x 224 images). For example:\n\ndensenet = models.densenet121(pretrained=True)\ndensenet.eval();\n\nYou can check out densenet‚Äôs architecture by printing it to screen but it‚Äôs huge so I won‚Äôt do that here. The layers can be accessed using the .named_children() method, the last one is the classification layer, a fully-connected layer outputting 1000 values (one for each ImageNet class):\n\nlist(densenet.named_children())[-1]\n\n('classifier', Linear(in_features=1024, out_features=1000, bias=True))\n\n\nThe ImageNet class labels are stored in a file in this directory:\n\nclasses = json.load(open(\"data/imagenet_class_index.json\"))\nidx2label = [classes[str(k)][1] for k in range(len(classes))]\nprint(\"First 10 ImageNet classes:\")\nidx2label[:10]\n\nFirst 10 ImageNet classes:\n\n\n['tench',\n 'goldfish',\n 'great_white_shark',\n 'tiger_shark',\n 'hammerhead',\n 'electric_ray',\n 'stingray',\n 'cock',\n 'hen',\n 'ostrich']\n\n\nLet‚Äôs try the model out on some random images (like my dog Evie):\n\nimage = Image.open('img/evie.png')\nimage\n\n\n\n\n\nimage = transforms.functional.to_tensor(image.resize((224, 224))).unsqueeze(0)\n_, idx = torch.softmax(densenet(image), dim=1).topk(5)\nprint(f\"Top 3 predictions: {[idx2label[_.item()] for _ in idx[0]]}\")\n\nTop 3 predictions: ['toy_poodle', 'Yorkshire_terrier', 'Maltese_dog', 'miniature_poodle', 'Shih-Tzu']\n\n\nNot bad! Can we trick it with an image of me in a panda mask?\n\nimage = Image.open('img/panda-tom.png')\nimage\n\n\n\n\n\nimage = transforms.functional.to_tensor(image).unsqueeze(0)\n_, idx = torch.softmax(densenet(image), dim=1).topk(5)\nprint(f\"Top 5 predictions: {[idx2label[_.item()] for _ in idx[0]]}\")\n\nTop 5 predictions: ['mask', 'ski_mask', 'teddy', 'giant_panda', 'jersey']\n\n\nNot bad either! Anyway, you get the point. This workflow is constrained to the architecture of the model, i.e., we can only predict the ImageNet classes at this point. What if I wanted to make prediction for another problem, say a binary classification problem? Read on.\n\n\n6.6.2 4.2. Feature Extractor\nIn this method, we use a pre-trained model as a ‚Äúfeature extractor‚Äù which creates useful features for us that we can use to train some other model. We really have two options here: 1. Add some extra layers to the pre-trained network to suit our particular task 2. Pass training data through the network and save the output to use as features for training some other model\nLet‚Äôs do approach 1 first. Let‚Äôs adapt densenet to predict our bitmoji data. I‚Äôm going to load the model, and then ‚Äúfreeze‚Äù all of its parameters (we don‚Äôt want to update them!):\n\ndensenet = models.densenet121(pretrained=True)\n\nfor param in densenet.parameters():  # Freeze parameters so we don't update them\n    param.requires_grad = False\n\nWe saw before that the last layer of densenet is a fully-connected linear layer Linear(in_features=1024, out_features=1000). We are going to do binary classification, so I‚Äôm going to replace this layer with my own layers (I‚Äôm using OrderedDict() here so I can name my layers, but you don‚Äôt have to do this):\n\nnew_layers = nn.Sequential(OrderedDict([\n    ('new1', nn.Linear(1024, 500)),\n    ('relu', nn.ReLU()),\n    ('new2', nn.Linear(500, 1))\n]))\ndensenet.classifier = new_layers\n\nLet‚Äôs check that the last layer of our model is updated:\n\ndensenet.classifier\n\nSequential(\n  (new1): Linear(in_features=1024, out_features=500, bias=True)\n  (relu): ReLU()\n  (new2): Linear(in_features=500, out_features=1, bias=True)\n)\n\n\nLooks good! Now we need to train our new layers:\n\nIMAGE_SIZE = 224\nBATCH_SIZE = 128\n# New dataloaders\ntrain_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_dataset = torchvision.datasets.ImageFolder(root=VALID_DIR, transform=data_transforms)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n\ndef trainer(model, criterion, optimizer, train_loader, valid_loader, device, epochs=5, verbose=True):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n    \n    train_accuracy = []\n    valid_accuracy = []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        train_batch_acc = 0\n        valid_batch_loss = 0\n        valid_batch_acc = 0\n        \n        # Training\n        for X, y in train_loader:\n            if device.type == 'cuda':\n                X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_hat = model(X).flatten()\n            y_hat_labels = torch.sigmoid(y_hat) &gt; 0.5\n            loss = criterion(y_hat, y.type(torch.float32))\n            loss.backward()\n            optimizer.step()\n            train_batch_loss += loss.item()\n            train_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()\n        train_accuracy.append(train_batch_acc / len(train_loader))\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            for X, y in valid_loader:\n                if device.type == 'cuda':\n                    X, y = X.to(device), y.to(device)\n                y_hat = model(X).flatten()\n                y_hat_labels = torch.sigmoid(y_hat) &gt; 0.5\n                loss = criterion(y_hat, y.type(torch.float32))\n                valid_batch_loss += loss.item()\n                valid_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()\n        valid_accuracy.append(valid_batch_acc / len(valid_loader))\n        model.train()\n        \n        # Print progress\n        if verbose:\n            print(f\"Epoch {epoch + 1}:\",\n                  f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n    \n    return {\"train_accuracy\": train_accuracy, \"valid_accuracy\": valid_accuracy}\n\n\n# We have a big model so this will take some time to run! If you have a GPU, things could be much faster!\ndensenet.to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(densenet.parameters())\nresults = trainer(densenet, criterion, optimizer, train_loader, valid_loader, device, epochs=10)\n\nEpoch 1: Train Accuracy: 0.60. Valid Accuracy: 0.63.\nEpoch 2: Train Accuracy: 0.72. Valid Accuracy: 0.74.\nEpoch 3: Train Accuracy: 0.75. Valid Accuracy: 0.72.\nEpoch 4: Train Accuracy: 0.81. Valid Accuracy: 0.73.\nEpoch 5: Train Accuracy: 0.80. Valid Accuracy: 0.69.\nEpoch 6: Train Accuracy: 0.81. Valid Accuracy: 0.72.\nEpoch 7: Train Accuracy: 0.86. Valid Accuracy: 0.78.\nEpoch 8: Train Accuracy: 0.87. Valid Accuracy: 0.74.\nEpoch 9: Train Accuracy: 0.89. Valid Accuracy: 0.73.\nEpoch 10: Train Accuracy: 0.90. Valid Accuracy: 0.80.\nCool, we leveraged the power of densenet to get a really great model!\nNow, you can use pre-trained model as arbitrary feature extractors, you don‚Äôt have to add on layers, you can just extract the output values of the network (well, you can extract values from any layer you like) and use those values as ‚Äúfeatures‚Äù to train another model. Below, I‚Äôm going to pass all my bitmoji data through the network and save the outputs:\n\ndef get_features(model, train_loader, valid_loader):\n    \"\"\"Extract output of squeezenet model\"\"\"\n    \n    with torch.no_grad():  # turn off computational graph stuff\n        Z_train = torch.empty((0, 1024))  # Initialize empty tensors\n        y_train = torch.empty((0))\n        Z_valid = torch.empty((0, 1024))\n        y_valid = torch.empty((0))\n        for X, y in train_loader:\n            Z_train = torch.cat((Z_train, model(X)), dim=0)\n            y_train = torch.cat((y_train, y))\n        for X, y in valid_loader:\n            Z_valid = torch.cat((Z_valid, model(X)), dim=0)\n            y_valid = torch.cat((y_valid, y))\n    return Z_train.detach(), y_train.detach(), Z_valid.detach(), y_valid.detach()\n\n\ndensenet = models.densenet121(pretrained=True)\ndensenet.classifier = nn.Identity()  # remove that last \"classification\" layer\nZ_train, y_train, Z_valid, y_valid = get_features(densenet, train_loader, valid_loader)\n\nNow we have some extracted features. Let‚Äôs train a classifier on the data, say, a LogisticRegression() model:\n\n# Let's scale our data\nscaler = StandardScaler()\nZ_train = scaler.fit_transform(Z_train)\nZ_valid = scaler.transform(Z_valid)\n# Fit a model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(Z_train, y_train)\nprint(f\"Train accuracy: {model.score(Z_train, y_train) * 100:.2f}%\")\nprint(f\"Valid accuracy: {model.score(Z_valid, y_valid) * 100:.2f}%\")\n\nTrain accuracy: 100.00%\nValid accuracy: 76.75%\n\n\nSo what did we just do: 1. We passed out bitmoji images through squeezenet and saved all the output values. Squeezenet outputs 1000 values per input. We had 1714 bitmoji images, so we extracted a tensor of shape (1714, 1000) from squeezenet. 2. So we now have a dataset of 1000 features and 1714 examples. Our target remains binary (\"not_tom\", \"tom\") = (0, 1). We used this data to train a logistic regression model. Cool!\n\n\n6.6.3 4.3. Fine Tuning\nOkay, this is the final and most common workflow of transfer learning. Above, we stacked some extra layers onto densenet and just trained those layer (we ‚Äúfroze‚Äù all of densenet‚Äôs weights). But we can also ‚Äúfine tune‚Äù densenet‚Äôs weights if we like, to make it more suited to our data. We can choose to ‚Äúfine tune‚Äù all of densenet‚Äôs ~8 million parameters, or just some of them. To do this, we use the same workflow as above, but we unfreeze the layers we wish to ‚Äúfine-tune‚Äù:\n\n# Load (but don't freeze!) the model\ndensenet = models.densenet121(pretrained=True)\n# Replace classification layer\nnew_layers = nn.Sequential(OrderedDict([\n    ('new1', nn.Linear(1024, 500)),\n    ('relu', nn.ReLU()),\n    ('new2', nn.Linear(500, 1))\n]))\ndensenet.classifier = new_layers\n# Move to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndensenet.to(device);\n\n\n# Train the model (I did this on a GPU)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(densenet.parameters())\nresults = trainer(densenet, criterion, optimizer, train_loader, valid_loader, device, epochs=10)\n\nEpoch 1: Train Accuracy: 0.79. Valid Accuracy: 0.61.\nEpoch 2: Train Accuracy: 0.95. Valid Accuracy: 0.97.\nEpoch 3: Train Accuracy: 0.98. Valid Accuracy: 0.98.\nEpoch 4: Train Accuracy: 0.98. Valid Accuracy: 0.96.\nEpoch 5: Train Accuracy: 0.98. Valid Accuracy: 0.96.\nEpoch 6: Train Accuracy: 0.98. Valid Accuracy: 0.96.\nEpoch 7: Train Accuracy: 0.99. Valid Accuracy: 0.97.\nEpoch 8: Train Accuracy: 0.99. Valid Accuracy: 0.98.\nEpoch 9: Train Accuracy: 0.99. Valid Accuracy: 0.94.\nEpoch 10: Train Accuracy: 0.98. Valid Accuracy: 0.97.\nWow! By far our best results yet. You could also choose to fine-tune just some layers, for example, below I‚Äôll freeze everything but the last two layers:\n\n# Freeze all but the last two layers\nfor layer in densenet.features[:-2]:\n    for param in layer.parameters():\n        param.requires_grad = False\n# Now re-train...\n\n\n\n6.6.4 4.4. Transfer Learning Summary\n\nUse a pre-trained model out-of-the-box (good if a model already exists for your problem).\nUse a pre-trained model as a ‚Äúfeature extractor‚Äù (good if you want to adapt a pre-trained model for a specific problem).\nFine-tune a pre-trained model (same as 2 but generally yields better results, although at more computational cost)."
  },
  {
    "objectID": "chapter7_advanced-deep-learning.html#chapter-learning-objectives",
    "href": "chapter7_advanced-deep-learning.html#chapter-learning-objectives",
    "title": "7¬† Chapter 7: Advanced Deep Learning",
    "section": "7.1 Chapter Learning Objectives",
    "text": "7.1 Chapter Learning Objectives\n\n\nDescribe what an autoencoder is at a high level and what they can be useful for.\nDescribe what a generative adversarial network is at a high level and what they can be useful for.\nDescribe what a multi-input model is and what they can be useful for."
  },
  {
    "objectID": "chapter7_advanced-deep-learning.html#imports",
    "href": "chapter7_advanced-deep-learning.html#imports",
    "title": "7¬† Chapter 7: Advanced Deep Learning",
    "section": "7.2 Imports",
    "text": "7.2 Imports\n\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import transforms, datasets, utils, models\nfrom torchsummary import summary\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nfrom utils.plotting import *\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfrom PIL import Image\nplt.style.use('ggplot')\nplt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'axes.grid': False})"
  },
  {
    "objectID": "chapter7_advanced-deep-learning.html#autoencoders",
    "href": "chapter7_advanced-deep-learning.html#autoencoders",
    "title": "7¬† Chapter 7: Advanced Deep Learning",
    "section": "7.3 1. Autoencoders",
    "text": "7.3 1. Autoencoders\n\nAutoencoders (AE) are networks that are designed to reproduce their input at the output layer. They are composed of an ‚Äúencoder‚Äù and ‚Äúdecoder‚Äù. The hidden layers of the AE are typically smaller than the input layers, such that the dimensionality of the data is reduced as it is passed through the encoder, and then expanded again in the decoder:\n\nWhy would you want to use such a model? As you can see, AEs perform dimensionality reduction by learning to represent your input features using fewer dimensions. That can be useful for a range of tasks but we‚Äôll look at some specific examples below.\n\n7.3.1 1.1. Example 1: Dimensionality Reduction\nHere‚Äôs some synthetic data of three features and two classes. Can we reduce the dimensionality of this data to two features while preserving the class separation?\n\nn_samples = 500\nX, y = make_blobs(n_samples, n_features=2, centers=2, cluster_std=1, random_state=123)\nX = np.concatenate((X, np.random.random((n_samples, 1))), axis=1)\nX = StandardScaler().fit_transform(X)\nplot_scatter3D(X, y)\n\n\n                                                \n\n\nWe can see that X1 and X2 split the data nicely, and the X3 is just noise. The question is, can an AE learn that this data can be nicely separated in just two of the three dimensions?\nLet‚Äôs build a simple AE with the following neurons in each layer: 3 -&gt; 2 -&gt; 3:\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 2),\n            nn.Sigmoid()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(2, input_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\nBATCH_SIZE = 100\ntorch.manual_seed(1)\nX_tensor = torch.tensor(X, dtype=torch.float32)\ndataloader = DataLoader(X_tensor,\n                        batch_size=BATCH_SIZE)\nmodel = autoencoder(3, 2)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\n\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    for batch in dataloader:\n        optimizer.zero_grad()           # Clear gradients w.r.t. parameters\n        y_hat = model(batch)            # Forward pass to get output\n        loss = criterion(y_hat, batch)  # Calculate loss\n        loss.backward()                 # Getting gradients w.r.t. parameters\n        optimizer.step()                # Update parameters\n\nWe only care about the encoder now, does it represent our data nicely in reduced dimensions?\n\nmodel.eval()\nprint(f\"Original X shape = {X_tensor.shape}\")\nX_encoded = model.encoder(X_tensor)\nprint(f\" Encoded X shape = {X_encoded.shape}\")\n\nOriginal X shape = torch.Size([500, 3])\n Encoded X shape = torch.Size([500, 2])\n\n\n\nplot_scatter2D(X_encoded, y)\n\n\n                                                \n\n\nWhat did we just do? We used an AE to effectively reduce the number of features in our data.\n\n\n7.3.2 1.2. Example 2: Image Denoising\nOkay, let‚Äôs do something more interesting. We saw above that AEs can be useful feature reducers (i.e., they can remove unimportant features from our data). This also applies to images and it‚Äôs a fun application to de-noise images!\nTake a look at these images of 8‚Äôs from the MNIST dataset, I‚Äôm going to mess them up by adding some noise to them:\n\nBATCH_SIZE = 32\n\n# Download data\ntransform = transforms.Compose([transforms.ToTensor()])\ntrainset = datasets.MNIST('data/', download=True, train=True, transform=transform)\nidx = trainset.targets == 8  # let's only work with the number 8\ntrainset.targets = trainset.targets[idx]\ntrainset.data = trainset.data[idx]\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Sample plot\nX, y = next(iter(trainloader))\nnoise = 0.5\nplot_eights(X, noise)\n\n\n\n\nCan we train an AE to get rid of that noise and reconstruct the original 8‚Äôs? Let‚Äôs give it a try!\nI‚Äôm going to use convolutional layers in my AE now as we are dealing with images. We‚Äôll use Conv2D() layers to compress our images into a reduced dimensonality, and then we need to ‚Äúupsample‚Äù it back to the original size. One ingredient you‚Äôll need to know to do this is ‚Äútransposed convolutional layers‚Äù. These are just like ‚Äúconvolutional layers‚Äù but for the purpose of ‚Äúupsampling‚Äù (increasing the size of) our data. Rather than simply expanding the size of our data and interpolating, we use nn.ConvTranspose2d() layers to help us learn how to best upsample our data:\n\n\n\nSource: modified from A guide to convolution arithmetic for deep learning, Vincent Dumoulin (2018)\n\n\ndef conv_block(input_channels, output_channels):\n    return nn.Sequential(\n        nn.Conv2d(input_channels, output_channels, 3, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2)  # reduce x-y dims by two; window and stride of 2\n    )\n\ndef deconv_block(input_channels, output_channels, kernel_size):\n    return nn.Sequential(\n        nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride=2),\n        nn.ReLU()\n    )\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            conv_block(1, 32),\n            conv_block(32, 16),\n            conv_block(16, 8)\n        )\n        self.decoder = nn.Sequential(\n            deconv_block(8, 8, 3),\n            deconv_block(8, 16, 2),\n            deconv_block(16, 32, 2),\n            nn.Conv2d(32, 1, 3, padding=1)  # final conv layer to decrease channel back to 1\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        x = torch.sigmoid(x)  # get pixels between 0 and 1\n        return x\n\nSo we want to train our model to remove that noise I added. Generally speaking, the idea is that the model learns what pixel values are important, we are reducing the dimensionality of the imaages, so our model must learn only the crucial information (i.e., not the noise) needed to reproduce the image. Right now, our model probably produces gibberish because it isn‚Äôt trained:\n\nmodel = autoencoder()\ninput_8 = X[:1, :1, :, :]\noutput_8 = model(input_8)\nplot_eight_pair(input_8, output_8)\n\n\n\n\nHow do we train it? Well we feed in a noisy image, compare it to the non-noisy version, and let the network learn how to make that happen. We want the value of the predicted pixels to be as close as possible to the real pixel values, so we‚Äôll use MSELoss() as our loss function:\n\nEPOCHS = 20\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nimg_list = []\n\nfor epoch in range(EPOCHS):\n    losses = 0\n    for batch, _ in trainloader:\n        noisy_batch = batch + noise * torch.randn(*batch.shape)\n        noisy_batch = torch.clip(noisy_batch, 0.0, 1.0)\n        optimizer.zero_grad()\n        y_hat = model(noisy_batch)\n        loss = criterion(y_hat, batch)\n        loss.backward()\n        optimizer.step()\n        losses += loss.item()\n    print(f\"epoch: {epoch + 1}, loss: {losses / len(trainloader):.4f}\")\n    # Save example results each epoch so we can see what's going on\n    with torch.no_grad():\n        noisy_8 = noisy_batch[:1, :1, :, :]\n        model_8 = model(input_8)\n        real_8 = batch[:1, :1, :, :]\n    img_list.append(utils.make_grid([noisy_8[0], model_8[0], real_8[0]], padding=1))\n\nepoch: 1, loss: 0.0914\nepoch: 2, loss: 0.0501\nepoch: 3, loss: 0.0431\n...\nepoch: 28, loss: 0.0275\nepoch: 29, loss: 0.0273\nepoch: 30, loss: 0.0273\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\nax.set_title(\"Input        Prediction        Actual\")\nims = [[plt.imshow(np.transpose(i, (1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n# ani.save('eights.gif', writer='imagemagick', fps=2)\n# HTML(ani.to_jshtml()) # run this in a new cell to produce the below animation\n\n\nPretty cool!"
  },
  {
    "objectID": "chapter7_advanced-deep-learning.html#generative-adversarial-networks-gans",
    "href": "chapter7_advanced-deep-learning.html#generative-adversarial-networks-gans",
    "title": "7¬† Chapter 7: Advanced Deep Learning",
    "section": "7.4 2. Generative Adversarial Networks (GANs)",
    "text": "7.4 2. Generative Adversarial Networks (GANs)\n\n\n7.4.1 2.1. What are GANs?\nGANs were invented in 2014 by Ian Goodfellow and colleagues. GANs are pretty complex but they are just so cool so I wanted to talk about them briefly. The goal of a GAN is to develop a model that can generate realistic ‚Äúfake‚Äù data, like the face below. &gt;If you look closely, you‚Äôll notice that some things don‚Äôt look quite right, like the glasses‚Ä¶\n\n\nSource: modified from thispersondoesnotexist.com\n\nGANs are mostly used to generate imagery at the moment so I‚Äôll speak about them in that context. The ‚Äúadversarial‚Äù comes from the fact that we actually have two networks ‚Äúbattling‚Äù each other: 1. The ‚ÄúGenerator‚Äù: takes some noise (like Gaussian noise) as input and generates ‚Äúfake‚Äù data (like an image) 2. The ‚ÄúDiscriminator‚Äù: takes in real and fake data from the generator and attempts to classify it as ‚Äúreal‚Äù or ‚Äúfake‚Äù\n\nAn analogy always helps. Think of the ‚ÄúGenerator‚Äù as a new counterfeit artist trying to produce realistic-looking famous artworks to sell:\n\nThe ‚ÄúDiscriminator‚Äù is an art-critic, trying to determine if a piece of art is ‚Äúreal‚Äù or ‚Äúfake‚Äù. At first, the ‚ÄúGenerator‚Äù produces poor art-replicas which the ‚ÄúDiscriminator‚Äù can easily tell are fake.\n\nBut over time, the ‚ÄúGenerator‚Äù learns ways to produce art that fools the ‚ÄúDiscriminator‚Äù. Eventually, the ‚ÄúGenerator‚Äù gets so good, that the ‚ÄúDiscriminator‚Äù can‚Äôt tell if a piece of art is real or fake - the ‚ÄúGenerator‚Äù is now able to generate realistic fake artwork! Money, money, money!\n\nGANs are mostly used to generate imagery at the moment so I‚Äôll focus on that. Training a GAN really happens in two iterative phases: 1. Train the Discriminator: - Generate some fake images with the generator - Show the discriminator real images and fake images and get it to classify them correctly (a simple binary classification problem) 2. Train the Generator: - Generate fake images with the generator but label them as ‚Äúreal‚Äù - At first, the discriminator will easily identify the ‚Äúfake‚Äù images as ‚Äúfake‚Äù (it‚Äôs already been trained a little bit to do this) - This means the generator will have a large loss (the discriminator predicted the images as ‚Äúfake‚Äù but the label is ‚Äúreal‚Äù) - We use this loss to update the generator‚Äôs weights. Next time around, the loss will be less, because the generator will produce some images that the discriminator thinks are real. 3. Repeat!\nEnough talk, let‚Äôs code it up!\n\n\n7.4.2 2.2. An Example: Generating Bitmojis\nI‚Äôm going to use our favourite bitmoji dataset to try and develop a GAN (this may or may not go well):\n\n# Training Parameters\nIMAGE_SIZE = 64\nBATCH_SIZE = 64\nLATENT_SIZE = 100\nNUM_EPOCHS = 200  # we often need a lot of epochs to train GANs\nLR = 0.0008\nIMAGE_DIR = \"data/bitmoji_rgb/all/\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device.type}\")\n\n# Transforms\ndata_transforms = transforms.Compose([transforms.Resize(IMAGE_SIZE), transforms.ToTensor()])\n# Loader\nbitmoji_dataset = datasets.ImageFolder(root=IMAGE_DIR, transform=data_transforms)\nbitmoji_loader = torch.utils.data.DataLoader(bitmoji_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nUsing device: cpu\n\n\nI‚Äôm going to base my architecture off the one in the original Deep Convolutional (DCGAN) paper. Here‚Äôs the generator (the disciminator is the inverse of this):\n\nThis is pretty complex stuff, but amazingly, we know what all of this is going to mean. Let‚Äôs create the ‚ÄúGenerator‚Äù first:\n\ndef convtrans_block(in_channels, out_channels, kernel_size, stride, padding):\n    return nn.Sequential(\n        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU()\n    )\n\nclass Generator(nn.Module):\n    def __init__(self, LATENT_SIZE):\n        super().__init__()\n        self.main = nn.Sequential(\n            convtrans_block(LATENT_SIZE, 512, 4, stride=1, padding=0),\n            convtrans_block(512, 256, 4, stride=2, padding=1),\n            convtrans_block(256, 128, 4, stride=2, padding=1),\n            convtrans_block(128, 64, 4, stride=2, padding=1),\n            convtrans_block(64, 3, 4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.main(x)\n\nWe should be able to pass some noise to our generator and it will spit out an image (probably not a good one yet, as we haven‚Äôt trained!):\n\ngenerator = Generator(LATENT_SIZE)\nexample_noise = torch.randn(1, LATENT_SIZE, 1, 1)\nfake = generator(example_noise).squeeze().detach()\nplt.figure(figsize=(5, 5))\nplt.title(\"Example Image from Generator\")\nplt.imshow(np.transpose(fake, (1, 2, 0)));\n\n\n\n\nNow let‚Äôs create discriminator:\n\ndef conv_block(in_channels, out_channels, kernel_size, stride, padding):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(0.2, True)\n    )\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, True),\n            conv_block(64, 128, 4, 2, 1),\n            conv_block(128, 256, 4, 2, 1),\n            conv_block(256, 512, 4, 2, 1),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.main(x)\n\nLet‚Äôs make sure our discriminator is able to make a prediction:\n\ndiscriminator = Discriminator()\nfake = generator(example_noise)\ndiscriminator(fake).item()\n\n0.5285460352897644\n\n\nOkay, we‚Äôre almost ready to train, the DCGAN paper recommends initializing the GAN with values from a normal distribution. I‚Äôll talk more about this later, for now, I‚Äôm just going to run this function:\n\ndef weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n        \ngenerator = Generator(LATENT_SIZE)\ngenerator.apply(weights_init)\ngenerator.to(device)\ndiscriminator = Discriminator()\ndiscriminator.apply(weights_init)\ndiscriminator.to(device);\n\nOkay, we‚Äôre ready to train. Our loss functions will be BCEWithLogitsLoss() - remember, the discriminator is just a binary classification network. However, because we train the generator and discriminator separately, we‚Äôll need two optimizers:\n\ncriterion = nn.BCELoss()\noptimizerG = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\noptimizerD = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\nfixed_noise = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1, device=device)  # Fixed noise vector we'll use to track image generation evolution\n\nI‚Äôve meticulously commented each line in the following training loop, hopefully it‚Äôs not too hard to follow!\n\n# Lists to keep track of progress\nimg_list = []\nITERS = 0\n\nprint(\"Begin training...\")\nfor epoch in range(NUM_EPOCHS):\n    \n    for real_batch, _ in bitmoji_loader:\n\n        ### STEP 1: train discriminator\n        # Train with real data\n        discriminator.zero_grad()\n        real_batch = real_batch.to(device)\n        real_labels = torch.ones((real_batch.shape[0],), dtype=torch.float, device=device)  # Real labels\n        output = discriminator(real_batch).view(-1)  # Forward pass real batch through discriminator\n        loss_real = criterion(output, real_labels)   # Calculate discriminator loss on real batch\n        loss_real.backward()                         # Calculate gradients for discriminator with backward pass\n        D_real = output.mean().item()                # Avg. D output on real batch\n        \n        # Train with fake data\n        noise = torch.randn(real_batch.shape[0], LATENT_SIZE, 1, 1, device=device)  # Generate some noise to feed to generator\n        fake_batch = generator(noise)                                      # Generate fake images with generator using the noise vector\n        fake_labels = torch.zeros_like(real_labels)           # Fake labels\n        output = discriminator(fake_batch.detach()).view(-1)  # Forward pass fake batch through discriminator (detach the generator though! we don't want to backprop through it)\n        loss_fake = criterion(output, fake_labels)            # Calculate discriminator loss on real batch\n        loss_fake.backward()                                  # Calculate gradients for discriminator with backward pass\n        D_fake = output.mean().item()                         # Avg. D output on fake batch\n        \n        # Update discriminator weights and store loss\n        optimizerD.step()\n        loss_dis = loss_real + loss_fake\n        \n        ### STEP 2: train generator\n        generator.zero_grad()\n        output = discriminator(fake_batch).view(-1)           # Forward pass fake batch through updated discriminator\n        loss_gen = criterion(output, real_labels)             # Calculate generator loss on fake batch\n        loss_gen.backward()                                   # Calculate gradients for generator with backward pass\n        \n        # Update generator weights and store loss\n        optimizerG.step()\n        \n        \n        ### Print performance info every 50 iterations (weight updates)\n        if ITERS % 50 == 0:\n            print(f\"Epoch ({epoch + 1}/{NUM_EPOCHS})\",\n                  f\"Iteration ({ITERS + 1})\",\n                  f\"Loss_G: {loss_gen.item():.4f}\",\n                  f\"Loss_D: {loss_dis.item():.4f}\",\n                  f\"D_real: {D_real:.4f}\",  # this should start around 1 and go down to 0.5 over time because\n                                            # the discriminator can easily tell real images are real at first,\n                                            # but as the generator gets better, the discriminator becomes unsure\n                  f\"D_fake: {D_fake:.4f}\")  # this should start around 0 and go up to 0.5 over time, see above.\n\n        ITERS += 1\n    \n    ### Keep a record of image generation as epochs progress\n    if epoch % 2 == 0:\n        with torch.no_grad():\n            fake_images = generator(fixed_noise).detach().cpu()\n        img_list.append(utils.make_grid(fake_images, nrow=4, normalize=True))\n        \n        \nprint(\"Finished training!\")\n\nBegin training...\nEpoch (1/200) Iteration (1) Loss_G: 1.7893 Loss_D: 3.2550 D_real: 0.2664 D_fake: 0.3662\nEpoch (2/200) Iteration (51) Loss_G: 3.4757 Loss_D: 0.8228 D_real: 0.8536 D_fake: 0.2715\nEpoch (3/200) Iteration (101) Loss_G: 4.5845 Loss_D: 0.5271 D_real: 0.8543 D_fake: 0.1388\nEpoch (5/200) Iteration (151) Loss_G: 5.2458 Loss_D: 0.2388 D_real: 0.9584 D_fake: 0.1332\n...\nNow, here‚Äôs a fancy animation that shows how our generator got better over time. I could have trained for longer, but the results are pretty cool!\n\nfig = plt.figure(figsize=(6,6))\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n# ani.save('bitmoji.gif', writer='imagemagick', fps=2)\n# HTML(ani.to_jshtml()) # run this in a new cell to produce the below animation"
  },
  {
    "objectID": "chapter7_advanced-deep-learning.html#multi-input-networks",
    "href": "chapter7_advanced-deep-learning.html#multi-input-networks",
    "title": "7¬† Chapter 7: Advanced Deep Learning",
    "section": "7.5 3. Multi-input Networks",
    "text": "7.5 3. Multi-input Networks\n\nSometimes you‚Äôll want to combine different types of data in a single network. The most common case is combining tabular data with image data, for example, using both real estate data and images of a house to predict its sale price:\n\nSource: ‚ÄúHouse‚Äù by oatsy40, ‚ÄúHouse in Vancouver‚Äù by pnwra, ‚ÄúHouse‚Äù by noona11 all licensed under CC BY 2.0.\nIn such a problem you may want to combine: 1. a NN for the tabular data 2. a CNN for the image data\nThe way we often do this is create these two models, and then combine them together into a model that produces a single output. This sounds complicated but it‚Äôs pretty easy! We only need one new ingredient which is a concatenation function: torch.cat(). Below is a simple example:\n\nclass MultiModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 16, 3, 2, 1),\n            nn.ReLU(),\n            nn.Conv2d(16, 8, 5, 2, 1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(1800, 5)\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(10, 50),\n            nn.ReLU(),\n            nn.Linear(50, 5)\n        )\n        \n        self.multi = nn.Sequential(\n            nn.Linear(10, 5),\n            nn.ReLU(),\n            nn.Linear(5, 1)\n        )\n        \n    def forward(self, image, data):\n        x_cnn = self.cnn(image)\n        x_fc = self.fc(data)\n        x_multi = torch.cat((x_cnn, x_fc), dim=1)\n        return self.multi(x_multi)\n\n\nmodel = MultiModel()\nimage = torch.randn(1, 3, 64, 64)\ndata = torch.randn(1, 10)\nmodel(image, data).item()\n\n-0.20222929120063782\n\n\nThe above network doesn‚Äôt really do anything but show that we can easily combine a CNN and fully connected NN!"
  },
  {
    "objectID": "chapter7_advanced-deep-learning.html#things-i-havent-talked-about",
    "href": "chapter7_advanced-deep-learning.html#things-i-havent-talked-about",
    "title": "7¬† Chapter 7: Advanced Deep Learning",
    "section": "7.6 4. Things I Haven‚Äôt Talked About",
    "text": "7.6 4. Things I Haven‚Äôt Talked About\n\nWe‚Äôve talked about a lot in this course, but there‚Äôs a lot I haven‚Äôt talked about too!\n\nRecurrent neural networks (good for sequential data like text and time series)\nBayesian neural networks (in one sentence: capture uncertainty by learning a distribution over the network parameters rather than learning point estimates)\nTraining at scale/distributed computing\nWorking with other forms of data like time series, text, audio, video, etc.\nPruning (removing unnecessary weights from a deep network) and quantization (use a small data type for model weights and operations, e.g., integers rather than floats) to reduce memory foot print and increase speed\nHigh-level PyTorch APIs: Ignite, Skorch, Pytorch-Lightning, etc.\nAdversarial examples. In one sentence: this is about ‚Äútricking‚Äù a model by adding some artifact, like noise, to the input image. See this OpenAI blog post or some work on adversarial stickers\n\n\ndensenet = models.densenet121(pretrained=True)\ndensenet.eval()\nclasses = json.load(open(\"data/imagenet_class_index.json\"))\nidx2label = [classes[str(k)][1] for k in range(len(classes))]\ntorch.manual_seed(0)\nimage = Image.open('img/evie.png')\nimage = transforms.functional.to_tensor(image).unsqueeze(0)\nimage_noise = image + 0.30 * torch.randn_like(image)\n_, image_idx = torch.softmax(densenet(image), dim=1).topk(1)\n_, image_noise_idx = torch.softmax(densenet(image_noise), dim=1).topk(1)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 4))\nax1.imshow(np.transpose(image.squeeze(), (1, 2, 0)))\nax1.set_title(f\"{idx2label[image_idx.item()]}\")\nax2.imshow(np.transpose(image_noise.squeeze(), (1, 2, 0)))\nax2.set_title(f\"{idx2label[image_noise_idx.item()]}\")\nplt.tight_layout();\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nWeight initiailization in layers: The intial weights of your network can affect the final result of your optimization. The general rule for setting the weights in a neural network is to set them to be close to zero without being too small. PyTorch does this for you automatically, but if you want to make your code exactly reproducible and be in control of the initialization, you can do that too. It‚Äôs easy to do in PyTorch using the .apply() method:\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        torch.nn.init.xavier_uniform_(m.weight)\n        torch.nn.init.zeros_(m.bias)\n\nmodel.apply(weights_init)\n\nclass example(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 3, (3, 3)),\n            nn.Flatten(),\n            nn.Linear(2028, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 10)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        print(\"Initializing weights of a Conv2d layer!\")\n        nn.init.normal_(m.weight, mean=0, std=0.1)\n        nn.init.zeros_(m.bias)\n    if isinstance(m, nn.Linear):\n        print(\"Initializing weights of a Linear layer!\")\n        nn.init.xavier_uniform_(m.weight)\n        nn.init.zeros_(m.bias)\n\ntorch.manual_seed(123)\nmodel = example()\nmodel.apply(weights_init);\n\nInitializing weights of a Conv2d layer!\nInitializing weights of a Linear layer!\nInitializing weights of a Linear layer!"
  },
  {
    "objectID": "chapter7_advanced-deep-learning.html#toms-cheat-sheet",
    "href": "chapter7_advanced-deep-learning.html#toms-cheat-sheet",
    "title": "7¬† Chapter 7: Advanced Deep Learning",
    "section": "7.7 5. Tom‚Äôs Cheat Sheet",
    "text": "7.7 5. Tom‚Äôs Cheat Sheet\n\nI thought it might be helpful to write down the general guidelines I follow when approaching a new data modelling problem.\nTABULAR DATA - Classification: 1. Logisitic Regression + feature engineer 2. XGBoost / LGBM + feature engineer 3. Neural network + feature engineer - Regression: 1. Linear Regression + feature engineer 2. XGBoost / LGBM + feature engineer 3. Neural network + feature engineer\nIMAGE DATA - Use a CNN: - Option 1: if data is relatively simple, develop CNN from scratch: - Start with 3x3 filters with strides of 1 or 2 - Number of filters proprtional to data complexity, usually 4, 8, 16, 24 - The first convolutional layer should have more channels that the input data - Option 2: transfer learning\nTIME SERIES DATA 1. AutoRegressive Integrated Moving Average (ARIMA) 2. XGBoost / LGBM + feature engineer 3. Recurrent neural network (RNN)\nAUDIO, VIDEO, OTHER - Probably some kind of NN"
  },
  {
    "objectID": "appendixa_gradients.html#introduction",
    "href": "appendixa_gradients.html#introduction",
    "title": "Appendix A ‚Äî Appendix A: Gradients Review",
    "section": "A.1 Introduction",
    "text": "A.1 Introduction\n\nThis short appendix provides a refresher on gradients and calculus. Some of you may never have seen gradients before. Multivariate calculus is not a required prerequisite for the MDS program but it will be helpful to know for this course, so we‚Äôll cover the basics in this appendix. This material has been modified after material originally created by Mike Gelbart."
  },
  {
    "objectID": "appendixa_gradients.html#imports",
    "href": "appendixa_gradients.html#imports",
    "title": "Appendix A ‚Äî Appendix A: Gradients Review",
    "section": "A.2 Imports",
    "text": "A.2 Imports\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.rcParams.update({'font.size': 16,\n                     'axes.labelweight': 'bold',\n                     'axes.grid': False,\n                     'figure.figsize': (8,6)})"
  },
  {
    "objectID": "appendixa_gradients.html#ingredient-1-functions-of-multiple-variables",
    "href": "appendixa_gradients.html#ingredient-1-functions-of-multiple-variables",
    "title": "Appendix A ‚Äî Appendix A: Gradients Review",
    "section": "A.3 1. Ingredient 1: Functions of Multiple Variables",
    "text": "A.3 1. Ingredient 1: Functions of Multiple Variables\n\n\nWe can write such a function as \\(f(x,y,z)\\) (for 3 inputs) or \\(f(x)\\) if \\(x\\) is a vector.\nExample: \\(f(x,y,z) = x^2 + y^2 + e^z + x^z + xyz\\).\n\n\ndef f(x, y, z):\n    return x**2 + y**2 + np.exp(z) + np.power(x,z) + x*y*z\n\nf(1,2,3)\n\n32.08553692318767\n\n\n\nAnother example: \\(f(x,y) = \\sin(xy)\\)\nWe can visualize functions of two variables, but it gets much harder after that.\n\n\nf = lambda x, y: np.sin(x * y)\n\nx = np.linspace(0, 4, 1000)\ny = np.linspace(0, 4, 1000)\nxx, yy = np.meshgrid(x, y)\nzz = f(xx, yy)\n\nplt.imshow(zz, extent=(np.min(x), np.max(x), np.min(y), np.max(y)), origin=\"lower\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"$\\sin(xy)$\")\nplt.colorbar();"
  },
  {
    "objectID": "appendixa_gradients.html#ingredient-2-vector-valued-functions",
    "href": "appendixa_gradients.html#ingredient-2-vector-valued-functions",
    "title": "Appendix A ‚Äî Appendix A: Gradients Review",
    "section": "A.4 2. Ingredient 2: Vector-valued Functions",
    "text": "A.4 2. Ingredient 2: Vector-valued Functions\n\n\nThese are functions with multiple outputs (and may or may not have multiple inputs).\nExample with 1 input and 3 outputs:\n\n\\[f(x)=\\begin{bmatrix} x^2 \\\\ 2x \\\\ -x\\end{bmatrix}\\]\n\nExample with 3 inputs and 4 outputs:\n\n\\[f(x,y,z)=\\begin{bmatrix} yz^2 \\\\ 0 \\\\ xyz \\\\ x-y\\end{bmatrix}\\]\n\nExample with 2 inputs and 2 outputs:\n\n\\[f(x,y)=\\begin{bmatrix} x \\\\ \\sin(y) \\end{bmatrix}\\]\n\ndef f(x, y):\n    return np.array([x, np.sin(y)])\n\n\nf(2, 10)\n\narray([ 2.        , -0.54402111])\n\n\n\nWe can visualize functions with two outputs (and two inputs), but it gets much harder after that.\n\n\nx = np.linspace(-5, 5, 20)\ny = np.linspace(-5, 5, 20)\nxx, yy = np.meshgrid(x, y)\nzz = f(xx, yy)\n\nplt.quiver(xx, yy, zz[0], zz[1])\n# plt.axis('square');\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"$f(x,y) = [x; \\; \\sin(y)]$\")\nplt.show()\n\n\n\n\nNotes:\n\nFor a fixed \\(y\\), when \\(x\\) grows, the \\(x\\)-component of the output grows (horizontal length of the arrows)\nA similar argument can be made for \\(y\\).\nIt‚Äôs not always the case that the number of inputs equals the number of outputs - this is a special case!\n\nBut it‚Äôs a very important special case, as we‚Äôll see below.\nWhat it means is that the ‚Äúinput space‚Äù and the ‚Äúoutput space‚Äù are the same.\nWhich allows for this kind of visualization.\n\n(optional) It‚Äôs not always the case that the \\(i\\)th component of the output depends on the \\(i\\)th component of the inputs - this is also a special case!"
  },
  {
    "objectID": "appendixa_gradients.html#ingredient-3-partial-derivatives",
    "href": "appendixa_gradients.html#ingredient-3-partial-derivatives",
    "title": "Appendix A ‚Äî Appendix A: Gradients Review",
    "section": "A.5 3. Ingredient 3: Partial Derivatives",
    "text": "A.5 3. Ingredient 3: Partial Derivatives\n\n\nA partial derivative is just a derivative of a multivariable function with respect to one of the input variables.\nWhen taking this derivative, we treat all the other variables as constants.\nExample: let \\(f(x,y,z) = x^2 + y^2 + e^x + x^z + xyz\\), let‚Äôs compute \\(\\frac{\\partial}{\\partial x} f(x,y,z)\\)\n\n\\[\\begin{align}\\frac{\\partial}{\\partial x} \\quad &x^2 &+ \\quad &y^2 &+ \\quad &e^x &+ \\quad &x^z &+ \\quad &xyz\\\\=\\quad &2x &+ \\quad &0 &+\\quad  &e^x &+ \\quad &zx^{z-1} &+ \\quad &yz\\end{align}\\]\n\nImportant note: $ $ is itself a function of \\(x,y,z\\), not just a function of \\(x\\). Think about the picture from the PDF slide above: the slope depends on your position in all coordinates.\n(optional) Thus, the partial derivative operator \\(\\frac{\\partial}{\\partial x}\\) maps from multivariate functions to multivariable functions."
  },
  {
    "objectID": "appendixa_gradients.html#ingredient-4-gradients",
    "href": "appendixa_gradients.html#ingredient-4-gradients",
    "title": "Appendix A ‚Äî Appendix A: Gradients Review",
    "section": "A.6 4. Ingredient 4: Gradients",
    "text": "A.6 4. Ingredient 4: Gradients\n\n\nThis is the easy part: a gradient is just a box holding all the \\(d\\) partial derivatives (assuming you have a function of \\(d\\) variables). For example, when \\(d=3\\):\n\n\\[\\nabla f(x,y,z)=\\begin{bmatrix}\\frac{\\partial f}{\\partial x}(x,y,z)\\\\ \\frac{\\partial f}{\\partial y}(x,y,z) \\\\\\frac{\\partial f}{\\partial z}(x,y,z)\\end{bmatrix}\\]\n\nOr, more generally, if \\(x\\) is a vector then\n\n\\[\\nabla f(x)=\\begin{bmatrix}\\frac{\\partial f}{\\partial x_1}(x)\\\\ \\frac{\\partial f}{\\partial x_2}(x) \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_d}(x)\\end{bmatrix}\\]\n\n(optional) Thus, a partial derivative is a function that has the same mapping as the original, e.g.¬†\\(\\mathbb{R}^3\\rightarrow \\mathbb{R}\\) (‚ÄúR three to R‚Äù).\n(optional) a gradient is a function that maps from the original input space to the same space, e.g.¬†\\(\\mathbb{R}^3\\rightarrow \\mathbb{R}^3\\) (‚ÄúR three to R three‚Äù).\n\nNotation warning: we use the term ‚Äúderivative‚Äù or ‚Äúgradient‚Äù to mean three different things:\n\nOperator (written \\(\\frac{d}{dx}\\) or \\(\\nabla\\)), which maps functions to functions; ‚Äúnow we take the gradient‚Äù.\nFunction (written \\(\\frac{df}{dx}\\) or \\(\\nabla f\\)), which maps vectors to vectors; ‚Äúthe gradient is \\(2x+5\\)‚Äù\n\nThis is what you get after applying the operator to a function.\n\nValue (written as a number or vector), which is just a number or vector; ‚Äúthe gradient is \\(\\begin{bmatrix}-2.34\\\\6.43\\end{bmatrix}\\)‚Äù\n\nThis is what you get after applying the function to an input.\n\n\nThis is extremely confusing!\nHere‚Äôs a table summarizing the situation, assuming 3 variables (in general it could be any number)\n\n\n\n\n\n\n\n\n\n\nName\nOperator\nFunction\nMaps\nExample Value\n\n\n\n\nDerivative\n\\(\\frac{d}{dx}\\)\n\\(\\frac{df}{dx}(x)\\)\n\\(\\mathbb{R}\\rightarrow \\mathbb{R}\\)\n\\(2.5\\)\n\n\nPartial Derivative\n\\({\\frac{\\partial}{\\partial x}}\\)\n\\({\\frac{\\partial f}{\\partial x}}(x,y,z)\\)\n\\({\\mathbb{R}^3\\rightarrow \\mathbb{R}}\\)\n\\(2.5\\)\n\n\nGradient\n\\(\\nabla\\)\n\\(\\nabla f(x,y,z)\\)\n\\(\\mathbb{R}^3\\rightarrow \\mathbb{R}^3\\)\n\\(\\begin{bmatrix}2.5\\\\0\\\\-1\\end{bmatrix}\\)\n\n\n\n\nA.6.1 4.1 Gradients intuition\nSince a gradient is a vector, we can talk about its magnitude and direction. - The magnitude is \\(\\|\\nabla f\\|\\) and tells us how fast things are changing. - The direction is \\(\\frac{\\nabla f}{\\|\\nabla f \\|}\\) and tells us the direction of fastest change or the steepest direction.\n\n# gradient vector field\nf = lambda x, y: np.sin(x * y)\n\nx = np.linspace(0, 4, 1000)\ny = np.linspace(0, 4, 1000)\nxx, yy = np.meshgrid(x, y)\nzz = f(xx, yy)\n\nplt.imshow(zz, extent=(np.min(x), np.max(x), np.min(y), np.max(y)), origin=\"lower\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"$\\sin(xy)$\")\nplt.colorbar();\n\n\n\n\n\\[\\nabla \\sin(xy) = \\begin{bmatrix} y \\cos(xy)\\\\x\\cos(xy)\\end{bmatrix}\\]\n\ngradf = lambda x, y: (y * np.cos(x * y), x * np.cos(x * y))\n\nxsmall = np.linspace(0, 4, 15)\nysmall = np.linspace(0, 4, 15)\nxxsmall, yysmall = np.meshgrid(xsmall, ysmall)\ngradx, grady = gradf(xxsmall, yysmall)\n\n\nplt.quiver(xxsmall, yysmall, gradx, grady)\nplt.axis(\"square\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"‚àá$ \\sin(xy)$\");\n\n\n\n\n\nplt.figure(figsize=(8,8))\nplt.imshow(zz,extent=(np.min(x), np.max(x), np.min(y), np.max(y)), origin='lower')\nplt.colorbar();\nplt.quiver(xxsmall,yysmall,gradx,grady);\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"$\\sin(xy)$ and its gradient\");\n\n\n\n\n\n\nA.6.2 4.2 Why is it the direction of fastest increase?\nFor example if the gradient is:\n\\[\\begin{bmatrix} 5 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix}\\, ,\\]\nwhy go in the gradient direction instead of the \\(x_1\\) direction, since that first component has the biggest partial derivative. Doesn‚Äôt it seem wasteful to go partly in those other directions?\nFirst, a proof that the gradient is the best direction. Let‚Äôs say we are at position \\(x\\) and we move by an infinitesimal (i.e.¬†extremely tiny) \\(v\\), which is a vector having components \\(v_1, v_2, \\ldots v_d\\). The change in \\(f\\) from moving from \\(x\\) to \\(x+v\\) is \\(\\frac{\\partial f}{dx_1} v_1 + \\frac{\\partial f}{dx_2} v_2 + \\ldots \\frac{\\partial f}{dx_d} v_d\\), where all the partial derivatives are evaluated at \\(x\\) (I believe this is related to the ‚Äútotal derivative‚Äù). In other words, the change in \\(f\\) is the dot product \\(\\nabla f \\cdot v\\). So now the question is, what vector \\(v\\) of fixed length maximizes \\(\\nabla f \\cdot v\\). The answer is a vector that points in the same direction as \\(\\nabla f\\). (That‚Äôs a property of the dot product, and is evident by the definition: \\(a \\cdot b = \\| a \\| \\|b \\| \\cos(\\theta)\\). Since \\(\\| \\nabla f \\|\\) and \\(\\|v\\|\\) are fixed in our case, to maximize this we want to maximize \\(\\cos(\\theta)\\), which means we want \\(\\cos(\\theta)=1\\) meaning \\(\\theta=0\\), or the angle between the vectors is \\(0\\)).\nSecond, the intuition. I think the ‚Äúparadox‚Äù comes from over-privileging the coordinate axes. They are not special in any way! For example if you rotate the coordinate system by 45 degrees, the direction of steepest ascent should also rotate by 45 degrees. Under the suggested system, this would not happen. Why? Well, there is always going to be one element of the gradient that is largest. Does that mean the direction of steepest ascent is always one of the coordinate axis directions? No.¬†That doesn‚Äôt make sense and also fails the ‚Äúrotate by 45 degrees test‚Äù because the direction will have rotated by 0, 90, 180 or 270 degrees."
  },
  {
    "objectID": "appendixb_logistic-loss.html#imports",
    "href": "appendixb_logistic-loss.html#imports",
    "title": "Appendix B ‚Äî Appendix B: Logistic Loss",
    "section": "B.1 Imports",
    "text": "B.1 Imports\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport plotly.express as px"
  },
  {
    "objectID": "appendixb_logistic-loss.html#logistic-regression-refresher",
    "href": "appendixb_logistic-loss.html#logistic-regression-refresher",
    "title": "Appendix B ‚Äî Appendix B: Logistic Loss",
    "section": "B.2 1. Logistic Regression Refresher",
    "text": "B.2 1. Logistic Regression Refresher\n\nLogistic Regression is a classification model where we calculate the probability of an observation belonging to a class as:\n\\[z=w^Tx\\]\n\\[\\hat{y} = \\frac{1}{(1+\\exp(-z))}\\]\nAnd then assign that observation to a class based on some threshold (usually 0.5):\n\\[\\text{Class }\\hat{y}=\\left\\{\n\\begin{array}{ll}\n    0, & \\hat{y}\\le0.5 \\\\\n    1, & \\hat{y}&gt;0.5 \\\\\n\\end{array}\n\\right.\\]"
  },
  {
    "objectID": "appendixb_logistic-loss.html#motivating-the-loss-function",
    "href": "appendixb_logistic-loss.html#motivating-the-loss-function",
    "title": "Appendix B ‚Äî Appendix B: Logistic Loss",
    "section": "B.3 2. Motivating the Loss Function",
    "text": "B.3 2. Motivating the Loss Function\n\n\nBelow is the mean squared error as a loss function for optimizing linear regression:\n\n\\[f(w)=\\frac{1}{n}\\sum^{n}_{i=1}(\\hat{y}-y_i))^2\\]\n\nThat won‚Äôt work for logistic regression classification problems because it ends up being ‚Äúnon-convex‚Äù (which basically means there are multiple minima)\nInstead we use the following loss function:\n\n\\[f(w)=-\\frac{1}{n}\\sum_{i=1}^ny_i\\log\\left(\\frac{1}{1 + \\exp(-w^Tx_i)}\\right) + (1 - y_i)\\log\\left(1 - \\frac{1}{1 + \\exp(-w^Tx_i)}\\right)\\]\n\nThis function is called the ‚Äúlog loss‚Äù or ‚Äúbinary cross entropy‚Äù\nI want to visually show you the differences in these two functions, and then we‚Äôll discuss why that loss functions works\nRecall the Pokemon dataset from Chapter 1, I‚Äôm going to load that in again (and standardize the data while I‚Äôm at it):\n\n\ndf = pd.read_csv(\"data/pokemon.csv\", usecols=['name', 'defense', 'attack', 'speed', 'capture_rt', 'legendary'])\nx = StandardScaler().fit_transform(df.drop(columns=[\"name\", \"legendary\"]))\nX = np.hstack((np.ones((len(x), 1)), x))\ny = df['legendary'].to_numpy()\ndf.head()\n\n\n\n\n\n\n\n\nname\nattack\ndefense\nspeed\ncapture_rt\nlegendary\n\n\n\n\n0\nBulbasaur\n49\n49\n45\n45\n0\n\n\n1\nIvysaur\n62\n63\n60\n45\n0\n\n\n2\nVenusaur\n100\n123\n80\n45\n0\n\n\n3\nCharmander\n52\n43\n65\n45\n0\n\n\n4\nCharmeleon\n64\n58\n80\n45\n0\n\n\n\n\n\n\n\n\nThe goal here is to use the features (but not ‚Äúname‚Äù, that‚Äôs just there for illustration purposes) to predict the target ‚Äúlegendary‚Äù (which takes values of 0/No and 1/Yes).\nSo we have 4 features meaning that our logistic regression model will have 5 parameters that need to be estimated (4 feature coefficients and 1 intercept)\nAt this point let‚Äôs define our loss functions:\n\n\ndef sigmoid(w, x):\n    \"\"\"Sigmoid function (i.e., logistic regression predictions).\"\"\"\n    return 1 / (1 + np.exp(-x @ w))\n\n\ndef mse(w, x, y):\n    \"\"\"Mean squared error.\"\"\"\n    return np.mean((sigmoid(w, x) - y) ** 2)\n\n\ndef logistic_loss(w, x, y):\n    \"\"\"Logistic loss.\"\"\"\n    return -np.mean(y * np.log(sigmoid(w, x)) + (1 - y) * np.log(1 - sigmoid(w, x)))\n\n\nFor a moment, let‚Äôs assume a value for all the parameters execpt for \\(w_1\\)\nWe will then calculate the mean squared error for different values of \\(w_1\\) as in the code below\n\n\nw1_arr = np.arange(-3, 6.1, 0.1)\nlosses = pd.DataFrame({\"w1\": w1_arr,\n                       \"mse\": [mse([0.5, w1, -0.5, 0.5, -2], X, y) for w1 in w1_arr],\n                       \"log\": [logistic_loss([0.5, w1, -0.5, 0.5, -2], X, y) for w1 in w1_arr]})\nlosses.head()\n\n\n\n\n\n\n\n\nw1\nmse\nlog\n\n\n\n\n0\n-3.0\n0.451184\n1.604272\n\n\n1\n-2.9\n0.446996\n1.571701\n\n\n2\n-2.8\n0.442773\n1.539928\n\n\n3\n-2.7\n0.438537\n1.508997\n\n\n4\n-2.6\n0.434309\n1.478955\n\n\n\n\n\n\n\n\nfig = px.line(losses.melt(id_vars=\"w1\", var_name=\"loss\"), x=\"w1\", y=\"value\", color=\"loss\", facet_col=\"loss\", facet_col_spacing=0.1)\nfig.update_yaxes(matches=None, showticklabels=True, col=2)\nfig.update_xaxes(matches=None, showticklabels=True, col=2)\nfig.update_layout(width=800, height=400)\n\n\n\n        \n        \n            \n            \n        \n\n\n\nThis is a pretty simple dataset but you can already see the ‚Äúnon-convexity‚Äù of the MSE loss function.\nIf you want a more mathematical description of the logistic loss function, check out Chapter 3 of Neural Networks and Deep Learning by Michael Nielsen or this Youtube video by Andrew Ng."
  },
  {
    "objectID": "appendixb_logistic-loss.html#breaking-down-the-log-loss-function",
    "href": "appendixb_logistic-loss.html#breaking-down-the-log-loss-function",
    "title": "Appendix B ‚Äî Appendix B: Logistic Loss",
    "section": "B.4 3. Breaking Down the Log Loss Function",
    "text": "B.4 3. Breaking Down the Log Loss Function\n\n\nSo we saw the log loss before:\n\n\\[f(w)=-\\frac{1}{n}\\sum_{i=1}^ny_i\\log\\left(\\frac{1}{1 + \\exp(-w^Tx_i)}\\right) + (1 - y_i)\\log\\left(1 - \\frac{1}{1 + \\exp(-w^Tx_i)}\\right)\\]\n\nIt looks complicated but it‚Äôs actually quite simple. Let‚Äôs break it down.\nRecall that we have a binary classification task here so \\(y_i\\) can only be 0 or 1.\n\n\nB.4.1 When y = 1\n\nWhen \\(y_i = 1\\) we are left with:\n\n\\[f(w)=-\\frac{1}{n}\\sum_{i=1}^n\\log\\left(\\frac{1}{1 + \\exp(-w^Tx_i)}\\right)\\]\n\nThat looks fine!\nWith \\(y_i = 1\\), if \\(\\hat{y_i} = \\frac{1}{1 + \\exp(-w^Tx_i)}\\) is also close to 1 we want the loss to be small, if it is close to 0 we want the loss to be large, that‚Äôs where the log() comes in:\n\n\ny = 1\ny_hat_small = 0.05\ny_hat_large = 0.95\n\n\n-np.log(y_hat_small)\n\n2.995732273553991\n\n\n\n-np.log(y_hat_large)\n\n0.05129329438755058\n\n\n\n\nB.4.2 When y = 0\n\nWhen \\(y_i = 1\\) we are left with:\n\n\\[f(w)=-\\frac{1}{n}\\sum_{i=1}^n\\log\\left(1 - \\frac{1}{1 + \\exp(-w^Tx_i)}\\right)\\]\n\nWith \\(y_i = 0\\), if \\(\\hat{y_i} = \\frac{1}{1 + \\exp(-w^Tx_i)}\\) is also close to 0 we want the loss to be small, if it is close to 1 we want the loss to be large, that‚Äôs where the log() comes in:\n\n\ny = 0\ny_hat_small = 0.05\ny_hat_large = 0.95\n\n\n-np.log(1 - y_hat_small)\n\n0.05129329438755058\n\n\n\n-np.log(1 - y_hat_large)\n\n2.99573227355399\n\n\n\n\nB.4.3 Plot Log Loss\n\nWe know that our predictions from logistic regression \\(\\hat{y}\\) are limited between 0 and 1 thanks to the sigmoid function\nSo let‚Äôs plot the losses because it‚Äôs interesting to see how the worse our predictions are, the worse the loss is (i.e., if \\(y=1\\) and our model predicts \\(\\hat{y}=0.05\\), the penalty is exponentially bigger than if the prediction was \\(\\hat{y}=0.90\\))\n\n\ny_hat = np.arange(0.01, 1.00, 0.01)\nlog_loss = pd.DataFrame({\"y_hat\": y_hat,\n                         \"y=0\": -np.log(1 - y_hat),\n                         \"y=1\": -np.log(y_hat)}).melt(id_vars=\"y_hat\", var_name=\"y\", value_name=\"loss\")\nfig = px.line(log_loss, x=\"y_hat\", y=\"loss\", color=\"y\")\nfig.update_layout(width=500, height=400)"
  },
  {
    "objectID": "appendixb_logistic-loss.html#log-loss-gradient",
    "href": "appendixb_logistic-loss.html#log-loss-gradient",
    "title": "Appendix B ‚Äî Appendix B: Logistic Loss",
    "section": "B.5 4. Log Loss Gradient",
    "text": "B.5 4. Log Loss Gradient\n\n\nIn Chapter 1 we used the gradient of the log loss to implement gradient descent\nHere‚Äôs the log loss and it‚Äôs gradient:\n\n\\[f(w)=-\\frac{1}{n}\\sum_{i=1}^ny_i\\log\\left(\\frac{1}{1 + \\exp(-w^Tx_i)}\\right) + (1 - y_i)\\log\\left(1 - \\frac{1}{1 + \\exp(-w^Tx_i)}\\right)\\]\n\\[\\frac{\\partial f(w)}{\\partial w}=\\frac{1}{n}\\sum_{i=1}^nx_i\\left(\\frac{1}{1 + \\exp(-w^Tx_i)} - y_i)\\right)\\]\n\nLet‚Äôs derive that now.\nWe‚Äôll denote:\n\n\\[z = -w^Tx_i\\]\n\\[\\sigma(z) = \\frac{1}{1 + \\exp(z)}\\]\n\nSuch that:\n\n\\[f(w)=-\\frac{1}{n}\\sum_{i=1}^ny_i\\log\\sigma(z) + (1 - y_i)\\log(1 - \\sigma(z))\\]\n\nOkay let‚Äôs do it:\n\n\\[\n\\begin{equation}\n\\begin{split}\n\\frac{\\partial f(w)}{\\partial w} & =-\\frac{1}{n}\\sum_{i=1}^ny_i \\times \\frac{1}{\\sigma(z)} \\times \\frac{\\partial \\sigma(z)}{\\partial w} + (1 - y_i) \\times \\frac{1}{1 - \\sigma(z)} \\times -\\frac{\\partial \\sigma(z)}{\\partial w} \\\\\n& =-\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i}{\\sigma(z)} - \\frac{1 - y_i}{1 - \\sigma(z)}\\right)\\frac{\\partial \\sigma(z)}{\\partial w} \\\\\n& =\\frac{1}{n}\\sum_{i=1}^n \\frac{\\sigma(z)-y_i}{\\sigma(z)(1 - \\sigma(z))}\\frac{\\partial \\sigma(z)}{\\partial w}\n\\end{split}\n\\end{equation}\n\\]\n\nNow we just need to work out \\(\\frac{\\partial \\sigma(z)}{\\partial w}\\), I‚Äôll mostly skip this part but there‚Äôs an intuitive derivation here, it‚Äôs just about using the chain rule:\n\n\\[\n\\begin{equation}\n\\begin{split}\n\\frac{\\partial \\sigma(z)}{\\partial w} & = \\frac{\\partial \\sigma(z)}{\\partial z} \\times \\frac{\\partial z}{\\partial w}\\\\\n& = \\sigma(z)(1-\\sigma(z))x_i \\\\\n\\end{split}\n\\end{equation}\n\\]\n\nSo finally:\n\n\\[\n\\begin{equation}\n\\begin{split}\n\\frac{\\partial f(w)}{\\partial w} & =\\frac{1}{n}\\sum_{i=1}^n \\frac{\\sigma(z)-y_i}{\\sigma(z)(1 - \\sigma(z))} \\times \\sigma(z)(1-\\sigma(z))x_i \\\\\n& = \\frac{1}{n}\\sum_{i=1}^nx_i(\\sigma(z)-y_i) \\\\\n& = \\frac{1}{n}\\sum_{i=1}^nx_i\\left(\\frac{1}{1 + \\exp(-w^Tx_i)} - y_i)\\right)\n\\end{split}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "appendixc_computing-derivatives.html#gradients-introduction",
    "href": "appendixc_computing-derivatives.html#gradients-introduction",
    "title": "Appendix C ‚Äî Appendix C: Computing Derivatives",
    "section": "C.1 1. Gradients Introduction",
    "text": "C.1 1. Gradients Introduction\n\nWhat if we are optimizing a function and we don‚Äôt know its derivative/gradient. This could happen because: - You‚Äôre optimizing the parameters of a computer simulation - a robot - weather / atmospheric science - financial simulation - etc. - You‚Äôre optimizing the hyperparameters of a machine learning algorithm - The derivative is just too much effort to compute and it‚Äôs slowing down your prototyping efforts\nWhat to do? Well, there are 3 main approaches to computing derivatives with a computer:\n\nSymbolic differentiation\nNumerical differentiation\nAutomatic differentiation\n\nWe‚Äôll explore these different options in this short appendix. Note that this material has been modified after material originally created by Mike Gelbart.\nAs a running example, consider \\(f(x)=\\sin(x)\\). We know that \\(f'(x)\\equiv\\frac{df(x)}{dx}=\\cos(x)\\) and thus \\(f'(1)=\\cos(1)\\)"
  },
  {
    "objectID": "appendixc_computing-derivatives.html#symbolic-differentiation",
    "href": "appendixc_computing-derivatives.html#symbolic-differentiation",
    "title": "Appendix C ‚Äî Appendix C: Computing Derivatives",
    "section": "C.2 2. Symbolic Differentiation",
    "text": "C.2 2. Symbolic Differentiation\n\nSymbolic differentiation is basically what you learned in calculus. We interpret the symbols in an equation and apply the rules of differentiation. Wolfram Alpha is a good tool for this, e.g., https://www.wolframalpha.com/input/?i=derivative+of+sin(x).\nSymbolic differentiation is useful but less so when dealing with data. We will not say more about this now. You should be aware of its existence."
  },
  {
    "objectID": "appendixc_computing-derivatives.html#numerical-differentiation-finite-differences",
    "href": "appendixc_computing-derivatives.html#numerical-differentiation-finite-differences",
    "title": "Appendix C ‚Äî Appendix C: Computing Derivatives",
    "section": "C.3 3. Numerical Differentiation (Finite Differences)",
    "text": "C.3 3. Numerical Differentiation (Finite Differences)\n\nA derivative is defined as:\n\\[\\frac{df(x)}{dx} \\equiv \\lim_{h\\rightarrow 0} \\frac{f(x+h)-f(x)}{h} \\]\nNumerical differentiation simply approximates the above using a very small \\(h\\):\n\\[\\frac{df(x)}{dx} \\approx \\frac{f(x+h)-f(x)}{h}\\]\nfor small \\(h\\).\nThis approach is called ‚Äúfinite differences‚Äù. I like to think of it as an estimate of as the ‚Äúrise over run‚Äù estimate of slope. Let‚Äôs give it a try!\n\nh = 0.001  # small h\nf = np.sin\nx0 = 1\ndfdx = (f(x0 + h) - f(x0)) / h\ndfdx\n\n0.5398814803603269\n\n\n\nnp.cos(x0)\n\n0.5403023058681398\n\n\nNot bad!\n\nC.3.1 3.1. Error of Derivative Estimates\nNumerical error arises from the discretization inherent finite difference approximations. Let‚Äôs explore this a bit. What do you think will happen to the absolute error of the estimate of the derivative if I change \\(h\\)?\n\nf = lambda x: np.sin(x)\ndfdx_estimate = lambda h: (f(x0 + h) - f(x0)) / h\ndfdx_true = np.cos(x0)\nabs_error = lambda h: np.abs(dfdx_estimate(h) - dfdx_true)\n\nh = 10 ** (np.linspace(-8, 0))\nplt.loglog(h, abs_error(h))\nplt.xlabel(\"h\")\nplt.ylabel(\"Error of derivative estimate\");\n\n\n\n\nThe above plot says that the error between our estimate and the true derivative grows as \\(h\\) gets bigger. Makes sense, right? But the plot thickens when we look at very small values of \\(h\\)‚Ä¶\n\nh = 10 ** (np.linspace(-16, 0))\nplt.loglog(h, abs_error(h))\nplt.xlabel(\"h\")\nplt.ylabel(\"Error of derivative estimate\");\n\n\n\n\nWait‚Ä¶ what?\nWhen \\(h\\ll 1\\) we have that \\(\\left| f(x+h)-f(x) \\right| \\ll 1\\). This leads to roundoff errors. Is it underflow?\n\nh = 10 ** (np.linspace(-25, 0))\nplt.loglog(h, abs_error(h));\n\n\n\n\nWell that flat part is underflow (estimate of the derivative is exactly zero). But what‚Äôs up with the part in between, from \\(h=10^{-16}\\) to \\(10^{-7}\\)?\nThe problem is regular old roundoff error related to floating point number precision. Roughly speaking, the magnitude of these roundoff errors is independent of \\(h\\) (e.g.¬†when computing \\(f(x)\\) it has nothing to do with \\(h\\)), and so the \\(h\\) in the denominator causes \\(\\frac{1}{h}\\)-like behavior. We can test this with a line, using our friend \\(10^{-16}\\).\n\nh = 10 ** (np.linspace(-16, 0))\nplt.loglog(h, abs_error(h))\nplt.loglog(h, 1e-16 * dfdx_true / h);\n\n\n\n\nWe‚Äôve seen a bunch of plots, but what have we actually learned? Well:\n\nthe numerical approximation to the derivative initially increases as \\(h\\) gets smaller (this effect is linear in \\(h\\))\nhowever, when \\(h\\) is too small then roundoff error kicks in and the error starts to increase (this effect is linear in \\(1/h\\))\n\nWhat you really need to know: picking \\(h\\) is tricky: it can‚Äôt be too big or too small.\n\n\nC.3.2 3.2. Gradients\nSo, what about gradients?\n\\[\\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_d} \\end{bmatrix}\\]\nWell, we can approximate each partial derivative as:\n\\[\\frac{\\partial f(x)}{\\partial x_1} \\approx \\frac{f(x+h e_1)-f(x)}{h}\\]\nwhere \\(e_1 = \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\).\nIn general:\n\\[\\frac{\\partial f(x)}{\\partial x_j} \\approx \\frac{f(x+h e_j)-f(x)}{h}\\]\nwhere \\(e_j\\) is \\(1\\) at position \\(j\\) and zero elsewhere.\nPut another way (if this helps),\n\\[\\frac{\\partial f(x)}{\\partial x_1} \\approx \\frac{f\\left(\\begin{bmatrix}x_1 + h \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_d \\end{bmatrix} \\right)-f(x)}{h}\\]\n\\[\\frac{\\partial f(x)}{\\partial x_2} \\approx \\frac{f\\left(\\begin{bmatrix}x_1 \\\\ x_2 + h \\\\ x_3 \\\\ \\vdots \\\\ x_d \\end{bmatrix} \\right)-f(x)}{h}\\]\nAnd so on and so forth.\n\n\nC.3.3 3.3. Cost of Estimating a Gradient\nAs you can see above, estimating a gradient requires \\(d+1\\) function evaluations. That can be expensive - perhaps \\(d\\) is \\(1000\\) or \\(10000\\). If you can compute the gradient symbolically, that is much better.\n\n\nC.3.4 3.4. (Optional) Other Finite Difference Formulas\nThere are better finite differences formulas than the one we are using, such as the centered difference formula:\n\\[\\frac{df(x)}{dx} \\approx \\frac{f(x+h)-f(x-h)}{2h}\\]\nIn this case the error goes down as \\(h^2\\), which is great, but most of the benefits come in applications that don‚Äôt concern us here (e.g., solving differential equations). However, in \\(d\\) dimensions, this requires \\(2d\\) evaluations of the function instead of \\(d+1\\), because with the forward difference formula (the original one above) we reuse the point \\(f(x)\\) for all the partial derivatives. Really, which formula you use depends on what you‚Äôre doing.\nIf you‚Äôre really bored, you can also check out the complex step method.\n\n\nC.3.5 3.5. Summary\nIf we can‚Äôt figure out the derivative of a function, but have code that computes the function, we can approximate the derivative using finite differences. This works pretty well but there are some pitfalls. In particular you need to choose \\(h\\) carefully and it costs \\(O(d)\\) function evaluations to estimate a gradient. It seems better to actually know the derivative if possible, because the computations will be faster and more accurate."
  },
  {
    "objectID": "appendixc_computing-derivatives.html#automatic-differentiation",
    "href": "appendixc_computing-derivatives.html#automatic-differentiation",
    "title": "Appendix C ‚Äî Appendix C: Computing Derivatives",
    "section": "C.4 4. Automatic Differentiation",
    "text": "C.4 4. Automatic Differentiation\n\nAutomatic differentiation (AD) is another way to take the derivative given some code that computes \\(f\\). It is advertised as the ‚Äúbest of both worlds‚Äù between symbolic and numerical differentiation: - Pro: takes the code as input, not math. - Pro: does not have the problem of choosing \\(h\\), or the extra \\(O(d)\\) cost. - Con: some AD software packages have limitations, hurt code readability. - Con: large space complexity.\nThere are two main types of AD, forward mode and reverse mode AD. We‚Äôll be discussing reverse mode here. Basically, AD keeps track of the gradient of primitive operations and uses the chain rule to link them together. We‚Äôll use a package called AutoGrad to demo AD.\nDemonstration 1: \\(\\sin(x)\\)\n\nanp.cos(1.0)\n\n0.5403023058681398\n\n\n\ngrad(anp.sin)(1.0)\n\n0.5403023058681398\n\n\nWhat happened above is that AutoGrad‚Äôs grad takes in a Python function, and returns another Python function that computes the gradient.\nSome more examples:\n\nx = anp.random.rand(4)\n\ndef foo(x):\n    return anp.sum(x)\n\ndef foo_grad(x):\n    return anp.ones(len(x)) ## SOLUTION\n\nfoo_grad(x)\n\narray([1., 1., 1., 1.])\n\n\n\nfoo_grad_AD = grad(foo)\nfoo_grad_AD(x)\n\narray([1., 1., 1., 1.])\n\n\n\ndef pin(x):\n    return anp.sin(x[1])\n\ndef pin_grad(x):\n    grad = anp.zeros(len(x)) ## SOLUTION\n    grad[1] = anp.cos(x[1])\n    return grad\n\npin_grad(x)\n\narray([0.       , 0.9779244, 0.       , 0.       ])\n\n\n\npin_grad_AD = grad(pin)\npin_grad_AD(x)\n\narray([0.       , 0.9779244, 0.       , 0.       ])\n\n\nAs you can see above, Autograd knows how to deal with things like np.sum, np.abs, subtraction, addition, etc. But let‚Äôs make sure it wasn‚Äôt cheating and just doing numerical differentiation!\n\ndef pin(x):\n    print(\"Evaluated function!\") # let us know what it's doing\n    return anp.sin(x[1])\n\ndef pin_grad(x):\n    grad = anp.zeros(len(x)) ## SOLUTION\n    grad[1] = anp.cos(x[1])\n    return grad\n\nIf it were doing numerical differentiation, it would need to evaluation the function \\(d+1\\) times (5 times in this case), like this:\n\nscipy.optimize.approx_fprime(x, pin, 1e-6) # do numerical differentiation\n\nEvaluated function!\nEvaluated function!\nEvaluated function!\nEvaluated function!\nEvaluated function!\n\n\narray([0.       , 0.9779243, 0.       , 0.       ])\n\n\n\npin_grad_AD = grad(pin) # automatic differentiation\npin_grad_AD(x)\n\nEvaluated function!\n\n\narray([0.       , 0.9779244, 0.       , 0.       ])\n\n\nLooks like it‚Äôs not doing numerical differentiation! Bam!\n\ndef baz(x):\n    result = 0\n    for i in range(len(x)):\n        result = result + x[i]**i\n    return result\n\ndef baz_grad(x):\n    result = [0] ## SOLUTION\n    for i in range(1,len(x)):\n        result.append(i*x[i]**(i-1))\n    return result\n\nbaz_grad(x)\n\n[0, 1.0, 0.8561805793442914, 0.008334506453204469]\n\n\n\nbaz_grad_AD = grad(baz) # differentiate through `for` loops!\nbaz_grad_AD(x)\n\narray([0.        , 1.        , 0.85618058, 0.00833451])\n\n\n\ndef bar(x):\n    if anp.abs(x[1]) &gt; 2:\n        return 0\n    else:\n        return -(x[0] * x[0] + 1) * anp.cos(x[1] - 1)\n\n\ndef bar_grad(x):\n    if anp.abs(x[1]) &gt; 2:  ## SOLUTION\n        return anp.zeros(len(x))\n    else:\n        result = 0 * x\n        result[0] = -2 * x[0] * anp.cos(x[1] - 1)\n        result[1] = (x[0] * x[0] + 1) * anp.sin(x[1] - 1)\n        return result\n\n\nbar_grad(x)\n\narray([-1.09518376, -1.13930008,  0.        ,  0.        ])\n\n\n\nbar_grad_AD = grad(bar) # differentiate through `if` statements!\nbar_grad_AD(x)\n\narray([-1.09518376, -1.13930008,  0.        ,  0.        ])\n\n\nAs an additional demo, let‚Äôs try robust regression with the Huber loss (whose derivative I don‚Äôt really feel like dealing with):\n\nd = 10\nn = 1000\n\n# generate random data\nX = anp.random.randn(n, d)\nw_true = anp.random.randn(d)\ny = X @ w_true\n# add random outliers\nNoutliers = 50\ny[:Noutliers] += 100 * anp.random.randn(Noutliers)\nw_true\n\narray([ 0.58231171, -0.07432767, -0.44828741, -0.16104079, -1.9348398 ,\n        1.1971574 ,  1.54178319, -0.07784519,  1.86594288, -0.56552392])\n\n\n\nfrom sklearn.linear_model import HuberRegressor\n\nhr = HuberRegressor(fit_intercept=False, alpha=0)\nhr.fit(X, y)\nhr.coef_\n\narray([ 0.58231172, -0.07432767, -0.44828741, -0.16104082, -1.93483981,\n        1.19715741,  1.5417832 , -0.07784521,  1.86594288, -0.56552394])\n\n\n\nhuber = lambda z: 0.5 * z ** 2 * (anp.abs(z) &lt;= 1) + (anp.abs(z) - 0.5) * (anp.abs(z) &gt; 1)\nf = lambda w: anp.sum(huber(X @ w - y))\n\nOkay here we go:\n\ndf_dw = grad(f) # differentiate through matrix multiplications, etc.\n\n\nw = np.zeros(d)\n\nalpha = 0.001\nwhile anp.linalg.norm(df_dw(w)) &gt; 0.0001:\n    w -= alpha * df_dw(w)\n\nw\n\narray([ 0.58501101, -0.07144062, -0.4559222 , -0.15739645, -1.94802414,\n        1.18996671,  1.52951647, -0.08104441,  1.8759754 , -0.56918479])\n\n\nNice! (They are not exactly the same because of epsilon hyperparameter in HuberRegressor.)\n\nC.4.1 4.1. Do I Never Need to Take a Derivative Again?\nHopefully not? There are production-grade automatic differentiation systems, like those used in TensorFlow or PyTorch so you may never have to worry about taking a derivative ever again."
  },
  {
    "objectID": "appendixd_bitmoji-cnn.html#introduction",
    "href": "appendixd_bitmoji-cnn.html#introduction",
    "title": "Appendix D ‚Äî Appendix D: Creating a CNN to Predict Bitmojis",
    "section": "D.1 1. Introduction",
    "text": "D.1 1. Introduction\n\nThis code-based appendix contains the code needed to develop and save the Bitmoji CNN‚Äôs I use in Chapter 6."
  },
  {
    "objectID": "appendixd_bitmoji-cnn.html#cnn-from-scratch",
    "href": "appendixd_bitmoji-cnn.html#cnn-from-scratch",
    "title": "Appendix D ‚Äî Appendix D: Creating a CNN to Predict Bitmojis",
    "section": "D.2 2. CNN from Scratch",
    "text": "D.2 2. CNN from Scratch\n\n\nTRAIN_DIR = \"data/bitmoji_rgb/train/\"\nVALID_DIR = \"data/bitmoji_rgb/valid/\"\nIMAGE_SIZE = 64\nBATCH_SIZE = 64\n\n# Transforms\ndata_transforms = transforms.Compose([transforms.Resize(IMAGE_SIZE), transforms.ToTensor()])\n# Load data and create dataloaders\ntrain_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=data_transforms)\nvalidloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Plot samples\nsample_batch = next(iter(trainloader))\nplt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\nplt.imshow(np.transpose(utils.make_grid(sample_batch[0], padding=1, normalize=True),(1,2,0)));\n\n\n\n\n\nclass bitmoji_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 8, (5, 5)),\n            nn.ReLU(),\n            nn.MaxPool2d((2, 2)),\n            nn.Conv2d(8, 4, (3, 3)),\n            nn.ReLU(),\n            nn.MaxPool2d((3, 3)),\n            nn.Flatten(),\n            nn.Linear(324, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n        \n    def forward(self, x):\n        out = self.main(x)\n        return out\n\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True):\n    \"\"\"Training wrapper for PyTorch network.\"\"\"\n    \n    train_loss, valid_loss, valid_accuracy = [], [], []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        valid_batch_loss = 0\n        valid_batch_acc = 0\n        \n        # Training\n        model.train()\n        for X, y in trainloader:\n            optimizer.zero_grad()\n            y_hat = model(X).flatten()\n            loss = criterion(y_hat, y.type(torch.float32))\n            loss.backward()\n            optimizer.step()\n            train_batch_loss += loss.item()\n        train_loss.append(train_batch_loss / len(trainloader))\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n            for X, y in validloader:\n                y_hat = model(X).flatten()\n                y_hat_labels = torch.sigmoid(y_hat) &gt; 0.5\n                loss = criterion(y_hat, y.type(torch.float32))\n                valid_batch_loss += loss.item()\n                valid_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()\n        valid_loss.append(valid_batch_loss / len(validloader))\n        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n        \n        # Print progress\n        if verbose:\n            print(f\"Epoch {epoch + 1}:\",\n                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n    \n    results = {\"train_loss\": train_loss,\n               \"valid_loss\": valid_loss,\n               \"valid_accuracy\": valid_accuracy}\n    return results\n\n\n# Define and train model\nmodel = bitmoji_CNN()\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters())\nresults = trainer(model, criterion, optimizer, trainloader, validloader, epochs=20)\n\nEpoch 1: Train Loss: 0.692. Valid Loss: 0.685. Valid Accuracy: 0.64.\nEpoch 2: Train Loss: 0.670. Valid Loss: 0.666. Valid Accuracy: 0.56.\nEpoch 3: Train Loss: 0.646. Valid Loss: 0.639. Valid Accuracy: 0.63.\nEpoch 4: Train Loss: 0.623. Valid Loss: 0.620. Valid Accuracy: 0.67.\nEpoch 5: Train Loss: 0.614. Valid Loss: 0.612. Valid Accuracy: 0.63.\nEpoch 6: Train Loss: 0.589. Valid Loss: 0.550. Valid Accuracy: 0.75.\nEpoch 7: Train Loss: 0.570. Valid Loss: 0.520. Valid Accuracy: 0.77.\nEpoch 8: Train Loss: 0.555. Valid Loss: 0.510. Valid Accuracy: 0.76.\nEpoch 9: Train Loss: 0.537. Valid Loss: 0.545. Valid Accuracy: 0.72.\nEpoch 10: Train Loss: 0.527. Valid Loss: 0.530. Valid Accuracy: 0.74.\nEpoch 11: Train Loss: 0.503. Valid Loss: 0.483. Valid Accuracy: 0.78.\nEpoch 12: Train Loss: 0.479. Valid Loss: 0.434. Valid Accuracy: 0.81.\nEpoch 13: Train Loss: 0.458. Valid Loss: 0.447. Valid Accuracy: 0.80.\nEpoch 14: Train Loss: 0.438. Valid Loss: 0.435. Valid Accuracy: 0.80.\nEpoch 15: Train Loss: 0.421. Valid Loss: 0.385. Valid Accuracy: 0.84.\nEpoch 16: Train Loss: 0.408. Valid Loss: 0.389. Valid Accuracy: 0.83.\nEpoch 17: Train Loss: 0.391. Valid Loss: 0.381. Valid Accuracy: 0.83.\nEpoch 18: Train Loss: 0.368. Valid Loss: 0.346. Valid Accuracy: 0.87.\nEpoch 19: Train Loss: 0.377. Valid Loss: 0.385. Valid Accuracy: 0.84.\nEpoch 20: Train Loss: 0.350. Valid Loss: 0.315. Valid Accuracy: 0.88.\n\n\n\n# Save model\nPATH = \"models/bitmoji_cnn.pt\"\ntorch.save(model, PATH)"
  },
  {
    "objectID": "appendixd_bitmoji-cnn.html#cnn-from-scratch-with-data-augmentation",
    "href": "appendixd_bitmoji-cnn.html#cnn-from-scratch-with-data-augmentation",
    "title": "Appendix D ‚Äî Appendix D: Creating a CNN to Predict Bitmojis",
    "section": "D.3 2. CNN from Scratch with Data Augmentation",
    "text": "D.3 2. CNN from Scratch with Data Augmentation\n\nConsider the ‚ÄúTom‚Äù Bitmoji below:\n\nimage = Image.open('img/tom-bitmoji.png')\nimage\n\n\n\n\nYou can tell it‚Äôs a me (class ‚ÄúTom‚Äù), can my CNN?\n\nimage_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0)\nprediction = int(torch.sigmoid(model(image_tensor)) &gt; 0.5)\nprint(f\"Prediction: {train_dataset.classes[prediction]}\")\n\nPrediction: tom\n\n\nGreat! But what happens if I flip my image like this:\n\nimage = image.rotate(180)\nimage\n\n\n\n\nYou can still tell that it‚Äôs me, but can my CNN?\n\nimage_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0)\nprediction = int(torch.sigmoid(model(image_tensor)) &gt; 0.5)\nprint(f\"Prediction: {train_dataset.classes[prediction]}\")\n\nPrediction: tom\n\n\nLooks like our CNN is not very robust to rotational changes in our input image. We could try and fix that using some data augmentation, let‚Äôs do that now:\n\n# Transforms\ndata_transforms = transforms.Compose([transforms.Resize(IMAGE_SIZE),\n                                      transforms.RandomVerticalFlip(p=0.5),\n                                      transforms.RandomHorizontalFlip(p=0.5),\n                                      transforms.ToTensor()])\n\n# Load data and re-create training loader\ntrain_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Plot samples\nsample_batch = next(iter(trainloader))\nplt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\nplt.imshow(np.transpose(utils.make_grid(sample_batch[0], padding=1, normalize=True),(1,2,0)));\n\n\n\n\nOkay, let‚Äôs train again with our new augmented dataset:\n\n# Define and train model\nmodel = bitmoji_CNN()\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters())\nresults = trainer(model, criterion, optimizer, trainloader, validloader, epochs=40)\n\nEpoch 1: Train Loss: 0.694. Valid Loss: 0.693. Valid Accuracy: 0.50.\nEpoch 2: Train Loss: 0.692. Valid Loss: 0.691. Valid Accuracy: 0.56.\nEpoch 3: Train Loss: 0.689. Valid Loss: 0.677. Valid Accuracy: 0.62.\nEpoch 4: Train Loss: 0.674. Valid Loss: 0.674. Valid Accuracy: 0.53.\nEpoch 5: Train Loss: 0.656. Valid Loss: 0.637. Valid Accuracy: 0.65.\nEpoch 6: Train Loss: 0.643. Valid Loss: 0.636. Valid Accuracy: 0.60.\nEpoch 7: Train Loss: 0.632. Valid Loss: 0.599. Valid Accuracy: 0.67.\nEpoch 8: Train Loss: 0.612. Valid Loss: 0.611. Valid Accuracy: 0.66.\nEpoch 9: Train Loss: 0.606. Valid Loss: 0.584. Valid Accuracy: 0.67.\nEpoch 10: Train Loss: 0.591. Valid Loss: 0.555. Valid Accuracy: 0.74.\nEpoch 11: Train Loss: 0.580. Valid Loss: 0.614. Valid Accuracy: 0.63.\nEpoch 12: Train Loss: 0.575. Valid Loss: 0.546. Valid Accuracy: 0.74.\nEpoch 13: Train Loss: 0.562. Valid Loss: 0.532. Valid Accuracy: 0.77.\nEpoch 14: Train Loss: 0.552. Valid Loss: 0.531. Valid Accuracy: 0.75.\nEpoch 15: Train Loss: 0.550. Valid Loss: 0.511. Valid Accuracy: 0.77.\nEpoch 16: Train Loss: 0.533. Valid Loss: 0.518. Valid Accuracy: 0.77.\nEpoch 17: Train Loss: 0.522. Valid Loss: 0.505. Valid Accuracy: 0.77.\nEpoch 18: Train Loss: 0.511. Valid Loss: 0.507. Valid Accuracy: 0.75.\nEpoch 19: Train Loss: 0.520. Valid Loss: 0.467. Valid Accuracy: 0.79.\nEpoch 20: Train Loss: 0.509. Valid Loss: 0.462. Valid Accuracy: 0.80.\nEpoch 21: Train Loss: 0.494. Valid Loss: 0.504. Valid Accuracy: 0.75.\nEpoch 22: Train Loss: 0.479. Valid Loss: 0.439. Valid Accuracy: 0.81.\nEpoch 23: Train Loss: 0.463. Valid Loss: 0.455. Valid Accuracy: 0.81.\nEpoch 24: Train Loss: 0.455. Valid Loss: 0.420. Valid Accuracy: 0.81.\nEpoch 25: Train Loss: 0.444. Valid Loss: 0.412. Valid Accuracy: 0.81.\nEpoch 26: Train Loss: 0.430. Valid Loss: 0.453. Valid Accuracy: 0.77.\nEpoch 27: Train Loss: 0.432. Valid Loss: 0.402. Valid Accuracy: 0.82.\nEpoch 28: Train Loss: 0.426. Valid Loss: 0.402. Valid Accuracy: 0.82.\nEpoch 29: Train Loss: 0.393. Valid Loss: 0.374. Valid Accuracy: 0.81.\nEpoch 30: Train Loss: 0.389. Valid Loss: 0.350. Valid Accuracy: 0.85.\nEpoch 31: Train Loss: 0.375. Valid Loss: 0.354. Valid Accuracy: 0.85.\nEpoch 32: Train Loss: 0.365. Valid Loss: 0.315. Valid Accuracy: 0.87.\nEpoch 33: Train Loss: 0.354. Valid Loss: 0.342. Valid Accuracy: 0.85.\nEpoch 34: Train Loss: 0.350. Valid Loss: 0.320. Valid Accuracy: 0.88.\nEpoch 35: Train Loss: 0.333. Valid Loss: 0.326. Valid Accuracy: 0.87.\nEpoch 36: Train Loss: 0.335. Valid Loss: 0.286. Valid Accuracy: 0.88.\nEpoch 37: Train Loss: 0.315. Valid Loss: 0.299. Valid Accuracy: 0.88.\nEpoch 38: Train Loss: 0.310. Valid Loss: 0.284. Valid Accuracy: 0.88.\nEpoch 39: Train Loss: 0.288. Valid Loss: 0.285. Valid Accuracy: 0.88.\nEpoch 40: Train Loss: 0.290. Valid Loss: 0.288. Valid Accuracy: 0.89.\n\n\n\n# Save model\nPATH = \"models/bitmoji_cnn_augmented.pt\"\ntorch.save(model, PATH)\n\nLet‚Äôs try predict this one again:\n\nimage\n\n\n\n\n\nimage_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0)\nprediction = int(torch.sigmoid(model(image_tensor)) &gt; 0.5)\nprint(f\"Prediction: {train_dataset.classes[prediction]}\")\n\nPrediction: tom\n\n\nGot it now!"
  }
]